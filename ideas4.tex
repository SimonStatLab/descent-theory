%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsmath}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\section{Holdout}


\subsection*{The problem}

Let $\theta^{*}$ be the true model parameters and suppose we observe
\[
y=g_{\theta^{*}}+\epsilon
\]


where $\epsilon$ are independent uniformly sub-gaussian random errors:

\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|W_{i}|^{2}/K^{2})\right]-1\right)\le\sigma^{2}
\]


Let $D$ be the full dataset and suppose it is randomly partitioned
into training set $T$ and validation set $V$. (We assume that the
size of $T$ and $V$ are fixed proportions of the total dataset.
e.g. $|T|=tn,|V|=vn$) Let the total number of observations be $n$.

Let the loss function for our regression problem be least squares.
We denote it as $\|h\|_{T}^{2}=\frac{1}{|T|}\sum_{i=1}^{|T|}h(x_{i})$
and similarly for $\|h\|_{V}^{2}$. 

Consider the joint optimization problem on a training/validation split
to find the best regularization parameter $\lambda$ in $\Lambda$:

\[
\hat{\lambda}=\arg\min_{\lambda\in\Lambda}\frac{1}{2}\|y-g_{\hat{\theta}(\lambda)}\|_{V}^{2}
\]
\[
\hat{\theta}(\lambda)=\arg\min_{\theta}\frac{1}{2}\|y-g_{\theta}\|_{T}^{2}+\lambda\left(P(\theta)+\frac{w}{2}\|\theta\|_{2}^{2}\right)
\]


Here $w>0$ is some fixed parameter. (One should choose this to be
on the order of 1e-15 or smaller. In practice, we can probably just
drop it entirely.)

Let the range of $\Lambda$ be from $[\lambda_{min},\lambda_{max}]$.
Both limits can grow and shrink at any polynomial rate, e.g. $\mbox{\ensuremath{\lambda}}_{max}=O_{P}(n^{\tau_{max}})$
and $\mbox{\ensuremath{\lambda}}_{min}=O_{P}(n^{-\tau_{min}})$. 

Suppose there is an optimal $\tilde{\lambda}$ s.t. $\|g_{\hat{\theta}(\lambda)}-g^{*}\|_{V}$
attains some optimal rate.

We show that $\hat{\lambda}$ will converge to $\tilde{\lambda}$
at an almost-parametric rate or just the optimal rate.

\[
\|g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}=O_{P}(1)\left(C\left(\frac{1-\log w+\kappa\log n}{|V|}\right)^{1/2}\vee\|g^{*}-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}\right)
\]


where $\kappa$ is a constant that depends on known constants, including
$\tau_{max}$ and $\tau_{\min}$ and $C$ depends on the Lipschitz
constant $M$ (see assumptions below) and the sub-gaussian parameters.


\subsubsection*{Other Assumptions}
\begin{itemize}
\item Suppose that penalty $P(\theta)$ is smooth and convex wrt $\theta$
-- its 1st and 2nd derivatives wrt $\theta$ are defined. Suppose
$\|\nabla_{\theta}P(\theta)|_{\hat{\theta}(\lambda)}\|\le O_{p}(n^{k_{1}})$.
\item Suppose $g_{\theta}$ is a smooth convex functions wrt $\theta$ --
its 1st and 2nd derivatives wrt $\theta$ are defined.
\item Suppose $g_{\theta}$ is Lipschitz with constant $M$ s.t. for any
subset $D_{0}\subseteq D$ 
\[
\|g_{\theta_{1}}-g_{\theta_{2}}\|_{D_{0}}\le M\left\Vert \theta_{1}-\theta_{2}\right\Vert _{2}
\]

\end{itemize}

\subsection*{Proof}


\subsubsection*{Step 1: Find the entropy of the model class $\mathcal{G}_{\lambda}$}

Consider any subset $D_{0}\subseteq D$. We show that the entropy
$H(u,\mathcal{G}_{\lambda},\|\cdot\|_{D_{0}})$ of the class 
\[
\mathcal{G}_{\lambda}=\left\{ g_{\hat{\theta}(\lambda)}:\lambda\in\Lambda\right\} 
\]
 

is bounded at a near-parametric rate:
\[
H\left(u,\mathcal{G}_{\lambda},\|\cdot\|_{D_{0}}\right)\le\log\left(\frac{M}{uw}\right)+\kappa\log n
\]


By the mean value theorem, for any $\delta>0$, there is some $\alpha\in[0,1]$
s.t. 
\[
\|\hat{\theta}(\ell)-\hat{\theta}(\ell+\delta)\|=\delta\left\Vert \nabla_{\lambda}\hat{\theta}(\lambda)|_{\lambda=\ell+\alpha\delta}\right\Vert 
\]


Since the problem is smooth, we can apply implicit differentiation
to get the derivative

\[
\nabla_{\lambda}\hat{\theta}(\lambda)=-H(\lambda)^{-1}\left(\nabla_{\theta}P(\theta)+w\theta\right)
\]


where the Hessian matrix is 
\[
H(\lambda)=\nabla_{\theta}^{2}\left(\|y-g_{\theta}\|_{T}^{2}+\lambda P(\theta)\right)+\lambda wI
\]


Under the assumption that $g_{\theta}$ and $P(\theta)$ are both
smooth and convex wrt $\theta$ which means the minimum eigenvalue
of $H(\lambda)$ is at least $\lambda w=O_{p}(n^{-\tau_{min}})w$.

From the asumptions, we have that

\begin{eqnarray*}
\left\Vert \nabla_{\lambda}\hat{\theta}(\lambda)\right\Vert  & \le & \left\Vert H(\lambda)^{-1}\left(\nabla_{\theta}P(\theta)+w\hat{\theta}(\lambda)\right)\right\Vert \\
 & \le & O_{p}(n^{\tau_{min}})w^{-1}\left(O_{p}(n^{k_{1}})+w\|\hat{\theta}(\lambda)\|\right)
\end{eqnarray*}


Note that $\|\hat{\theta}(\lambda)\|$ is easily bounded by the solution
to the ridge regression problem:

\[
\hat{\theta}_{r}(\lambda)=\arg\min\frac{1}{2}\|y-g_{\theta}\|_{T}^{2}+\lambda w\|\theta\|_{2}^{2}
\]


The solution $\hat{\theta}_{r}(\lambda)$ has norm at most $w^{-1}O_{P}(n^{\tau_{\min}})$.
It is straightforward to show that

\[
\|\hat{\theta}(\lambda)\|\le\|\hat{\theta}_{r}(\lambda)\|
\]


Therefore

\[
\left\Vert \nabla_{\lambda}\hat{\theta}(\lambda)\right\Vert =w^{-1}O_{p}(n^{\max(2\tau_{min},\tau_{min}+k_{1})})
\]


Now we can know bound the distance between $g_{\hat{\theta}(\ell)}$
and $g_{\hat{\theta}(\ell+\delta)}$

\begin{eqnarray*}
\|g_{\hat{\theta}(\ell)}-g_{\hat{\theta}(\ell+\delta)}\|_{D_{0}} & \le & M\left\Vert \hat{\theta}(\ell)-\hat{\theta}(\ell+\delta)\right\Vert _{2}\\
 & = & M\delta w^{-1}O_{p}(n^{\max(2\tau_{min},\tau_{min}+k_{1})})
\end{eqnarray*}


Then the covering number is on the order of $n^{\kappa}$ where $\kappa$
grows linearly in $\tau_{min},\tau_{max}$ and $k_{1}$. 
\[
N\left(u,\mathcal{G}_{\lambda},\|\cdot\|_{D_{0}}\right)\le\frac{M}{uw}O_{p}(n^{\kappa})\implies H\left(u,\mathcal{G}_{\lambda},\|\cdot\|_{D_{0}}\right)\le\log\left(\frac{M}{uw}\right)+\kappa\log n
\]



\subsubsection*{Step 2: Find the basic inequality}

By definition, 
\[
\|y-g_{\hat{\theta}(\hat{\lambda})}\|_{V}^{2}\le\|y-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}^{2}
\]


Rearranging, we get the basic inequality

\[
\|g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}^{2}\le2\left|\left(\epsilon,g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\right)\right|_{V}+\left|\left(g^{*}-g_{\hat{\theta}(\tilde{\lambda})},g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\right)\right|_{V}
\]


Clearly if the second term dominates, Cauchy Schwarz (and a symmetrization
argument) give us that $\|g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\|$
converges that the same rate as $\|g^{*}-g_{\hat{\theta}(\tilde{\lambda})}\|$.

In the next step we bound the empirical process term.


\subsubsection*{Step 3: Bound empirical process with a chaining argument}

If $\|\epsilon\|_{V}\le2\sigma$, the basic inequality implies that
$\|g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}\le2\sigma$.
Therefore we can bound the empirical process term by a standard chaining
argument (basically a copy of Thrm 9.1 in Vandegeer).

First apply Lemma 1 to the entropy bound derived in Step 1. We get
that there exists some $C$ s.t. for all $\delta$ s.t. $R\ge\delta/\sigma$
and

\[
\delta\ge CR\left(\frac{1-\log(M/w)+\kappa\log n}{|V|}\right)^{1/2}
\]


we have

\[
Pr\left(\sup_{\lambda_{1},\lambda_{2}:\|g_{\hat{\theta}(\lambda_{1})}-g_{\hat{\theta}(\lambda_{2})}\|_{2}\le R}\left|\left(\epsilon,g_{\hat{\theta}(\lambda_{1})}-g_{\hat{\theta}(\lambda_{2})}\right)\right|_{n}\ge\delta\wedge\|\epsilon\|_{n}\le\sigma\right)\le C\exp\left(-n\frac{\delta^{2}}{C^{2}R^{2}}\right)
\]


Next we chain. Let $S=\min\{s\in\{0,1,...\}:2^{s}>2\sigma\}$. For
\[
\delta\ge16C\left(\frac{1-\log w+\kappa\log n}{|V|}\right)^{1/2}
\]


we have

\begin{eqnarray*}
 &  & Pr\left(\left|\left(\epsilon,g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\right)\right|_{V}\ge\delta^{2}\wedge\|\epsilon\|_{V}\le2\sigma\right)\\
 & \le & \sum_{s=0}^{S}Pr\left(\sup_{\lambda_{1},\lambda_{2}:\|g_{\hat{\theta}(\lambda_{1})}-g_{\hat{\theta}(\lambda_{2})}\|_{2}\le2^{s+1}\delta}\left|\left(\epsilon,g_{\hat{\theta}(\lambda_{1})}-g_{\hat{\theta}(\lambda_{2})}\right)\right|_{V}\ge2^{2s-1}\delta^{2}\wedge\|\epsilon\|_{V}\le2\sigma\right)\\
 & \le & \sum_{s=0}^{S}C\exp\left(-|V|\frac{2^{4s-2}\delta^{4}}{4C^{2}2^{2s+2}\delta^{2}}\right)\\
 & \le & C\exp\left(-|V|\frac{\delta^{2}}{c^{2}}\right)
\end{eqnarray*}
Note that Lemma 1 can be applied since for $s=0,...,S$, 
\[
\sqrt{|V|}2^{2s+2}\delta^{2}\ge16C2^{s+1}\delta\left(1-\log(M/w)+\kappa\log n\right)^{1/2}
\]



\subsubsection*{Step 4: Win}

We can now bound $\|g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}$
with high probability. Let

\[
\delta\ge16C\left(\frac{1-\log(M/w)+\kappa\log n}{|V|}\right)^{1/2}
\]


Then

\begin{eqnarray*}
 &  & Pr\left(\|g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}\ge\delta\right)\\
 & \le & Pr\left(\|g^{*}-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}\ge\delta\right)+Pr\left(\left|\left(\epsilon,g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\right)\right|_{V}\ge\delta^{2}\right)\\
 & \le & Pr\left(\left|\left(\epsilon,g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\right)\right|_{V}\ge\delta^{2}\wedge\|\epsilon\|_{V}\le2\sigma\right)+Pr\left(\|\epsilon\|_{V}>2\sigma\right)+Pr\left(\|g^{*}-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}\ge\delta\right)\\
 & \le & C\exp\left(-|V|\frac{\delta^{2}}{c^{2}}\right)+Pr\left(\|\epsilon\|_{V}>2\sigma\right)+Pr\left(\|g^{*}-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}\ge\delta\right)
\end{eqnarray*}


Since $\epsilon$ are sub-gaussian, Bernstein's inequality gives us
\[
Pr\left(\|\epsilon\|_{V}\ge2\sigma\right)\le\exp\left(-\frac{|V|\sigma^{2}}{12K^{2}}\right)
\]


Therefore we either recover a near-parametric rate or the optimal
convergence rate. 
\[
\|g_{\hat{\theta}(\hat{\lambda})}-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}=O_{P}(1)\left(C\left(\frac{1-\log(M/w)+\kappa\log n}{|V|}\right)^{1/2}\vee\|g^{*}-g_{\hat{\theta}(\tilde{\lambda})}\|_{V}\right)
\]



\section{Lemmas}


\subsubsection*{Lemma 1}

Suppose function class $\mathcal{F}$ has 
\[
H\left(u,\mathcal{F},\|\cdot\|_{n}\right)\le J\left(\log\left(\frac{M}{uw}\right)+\kappa\log n\right)
\]


for positive constants $J,M,w,\kappa$.

Suppose $\sup_{f\in\mathcal{F}}\|f\|_{n}\le R$. 

Suppose $\epsilon$ are independent sub-gaussian RV with constants
$K$ and $\sigma$.

Then there exists some $C$ s.t. for all $\delta$ s.t. $R\ge\delta/\sigma$
and

\[
\delta\ge CRJ^{1/2}\left(\frac{1-\log(M/w)+\kappa\log n}{n}\right)^{1/2}
\]


we have

\[
Pr\left(\sup_{f_{1},f_{2}\in\mathcal{F}}\left|\left(\epsilon,f_{1}-f_{2}\right)\right|_{n}\ge\delta\wedge\|\epsilon\|_{n}\le\sigma\right)\le C\exp\left(-|n|\frac{\delta^{2}}{C^{2}R^{2}}\right)
\]



\subsubsection*{Proof}

We apply Corollary 8.3 in Vandegeer to determine the value $\delta$
s.t. $\delta$ bounds the empirical process term with high probability.

Let 
\[
\bar{\mathcal{F}}=\left\{ f_{1}-f_{2}:f_{1},f_{2}\in\mathcal{F}\right\} 
\]


Consequently, for $R\ge\delta/\sqrt{2}\sigma$ ,

\begin{eqnarray*}
\int_{0}^{R}H^{1/2}\left(u,\bar{\mathcal{F}},\|\cdot\|_{n}\right)du & = & \sqrt{2J}\int_{0}^{R}\left(\log\frac{1}{u}+\log\left(\frac{M}{w}\right)+\kappa\log n\right)^{1/2}du\\
 & = & O_{P}(1)R\sqrt{J}\left(\int_{0}^{1}\log\left(\frac{1}{u}\right)+\log\left(\frac{M}{w}\right)+\kappa\log ndu\right)^{1/2}\\
 & \le & O_{P}(1)R\sqrt{J}\left(1-\log(M/w)+\kappa\log n\right)^{1/2}
\end{eqnarray*}


where $C$ depends on the sub-gaussian parameters for $\epsilon$
($\sigma$ and $K$) and $c$ depends on $M$.

Then apply Corollary 8.3 for all 
\[
\delta\ge CR\sqrt{J}\left(\frac{1-\log(M/w)+\kappa\log n}{n}\right)^{1/2}
\]



\section{Corollaries}


\subsection{Convergence Rate Equivalence between the original regression problem
and the perturbed ridge problem}

Suppose the original regression problem is 
\[
\hat{\beta}(\lambda)=\arg\min_{\beta\in\Theta}\frac{1}{2}\|y-g_{\beta}\|_{T}^{2}+\lambda P(\beta)
\]


and the new perturbed ridge problem is 
\[
\hat{\theta}(\lambda)=\arg\min_{\theta\in\Theta}\frac{1}{2}\|y-g_{\theta}\|_{T}^{2}+\lambda\left(P(\theta)+\frac{w}{2}\|\theta\|_{2}^{2}\right)
\]


Let $\theta^{*}$ be the true model parameters. Suppose there are
constants $K_{0},K_{1}>0$ s.t. 
\[
\frac{w}{2}\|\theta^{*}\|_{2}^{2}\le K_{0}P(\theta^{*})+K_{1}
\]


Then the rate of convergence of $\left\Vert g_{\hat{\beta}(\lambda)}-g^{*}\right\Vert _{T}$
is the same as $\left\Vert g_{\hat{\theta}(\lambda)}-g^{*}\right\Vert _{T}$
modulo some constant.


\subsubsection*{Proof}

For ease of notation, we will write $\hat{\theta}=\hat{\theta}(\lambda)$
and $\hat{\beta}=\hat{\beta}(\lambda)$.

By definition, 
\begin{eqnarray*}
\frac{1}{2}\|y-g_{\hat{\theta}}\|_{T}^{2}+\tilde{\lambda}\left(P(\hat{\theta})+\frac{w}{2}\|\hat{\theta}\|^{2}\right) & \le & \frac{1}{2}\|y-g_{\theta^{*}}\|_{T}^{2}+\tilde{\lambda}\left(P(\theta^{*})+\frac{w}{2}\|\theta^{*}\|^{2}\right)\\
 & \le & \frac{1}{2}\|y-g_{\theta^{*}}\|_{T}^{2}+\tilde{\lambda}(1+K_{0})P(\theta^{*})+\tilde{\lambda}K_{1}
\end{eqnarray*}


Therefore
\[
\frac{1}{2}\|y-g_{\hat{\theta}}\|_{T}^{2}+\tilde{\lambda}P(\hat{\theta})\le\frac{1}{2}\|y-g_{\theta^{*}}\|_{T}^{2}+\tilde{\lambda}(1+K_{0})P(\theta^{*})+\tilde{\lambda}K_{1}
\]


Notice that this inequality is very similar to the inequality from
the original regression problem

\[
\frac{1}{2}\|y-g_{\hat{\beta}}\|_{T}^{2}+\tilde{\lambda}P(\hat{\beta})\le\frac{1}{2}\|y-g_{\theta^{*}}\|_{T}^{2}+\tilde{\lambda}P(\theta^{*})
\]


Therefore the same arguments to prove the convergence rate of $\left\Vert g_{\hat{\beta}(\lambda)}-g^{*}\right\Vert _{T}$
give the same convergence rate for $\left\Vert g_{\hat{\theta}(\lambda)}-g^{*}\right\Vert _{T}$.
(Example: refer to Thrm 10.2 in Vandegeer)


\subsection{Regression problems with smooth-almost-everywhere penalty functions}

If the regularization functions contain smooth-almost-everywhere penalty
functions, we still have the same convergence rate. This requires
the additional assumption that the local optimality space is the same
as the differentiable space (refer to condition 1 in the hillclimbing
paper).

A small modification to Step 1 of the proof shows that the entropy
of the class \textbf{$\mathcal{G}_{\lambda}$ }is still the same.
Hence the rest of the proof remains unchanged.


\subsubsection*{Proof}

\textbf{Step 1 for Smooth-almost-everywhere functions: Find the entropy
of the model class $\mathcal{G}_{\lambda}$}

Let $S$ be the set of knots at which $\nabla_{\lambda}\theta(\lambda)|_{\lambda=s}$
does not exist. Since the regression problem is smooth almost everywhere,
$S$ should have measure zero.

First we apply the mean value theorem to two points $\ell$ and $\ell+\delta$
that have no knots in between. That is, for any $\delta>0$, there
is some $\alpha\in[0,1]$ s.t. 
\[
\|\hat{\theta}(\ell)-\hat{\theta}(\ell+\delta)\|=\delta\left\Vert \nabla_{\lambda}\hat{\theta}(\lambda)|_{\lambda=\ell+\alpha\delta}\right\Vert 
\]


Apply the same assumptions from the hillclimbing paper regarding the
differentiable space and the local optimality space. We can then reformulate
the solutions in terms of the differentiable space. Suppose $U$ forms
an orthonormal basis for the differentiable space. Hence we can rewrite
$\theta$ in terms of $\beta$ s.t. $\theta=U\beta$. The derivative
of $\hat{\beta}(\lambda)$ can be calculated since the locally equivalent
regression problem is now smooth:

\[
\nabla_{\lambda}\hat{\beta}(\lambda)=H(\lambda)^{-1}\left(_{U}\nabla P(U\hat{\beta})+wU\hat{\beta}\right)
\]


where the Hessian matrix is 
\[
H(\lambda)=_{U}\nabla^{2}\left(\|y-g_{U\hat{\beta}}\|_{T}^{2}+\lambda P(U\hat{\beta})\right)+\lambda wI
\]


Under the assumption that $g_{\theta}$ and $P(\theta)$ are both
smooth and convex wrt $\theta$, the minimum eigenvalue of $H(\lambda)$
is at least $\lambda w=O_{p}(n^{-\tau_{min}})w$. Hence

\begin{eqnarray*}
\left\Vert \nabla_{\lambda}\hat{\beta}(\lambda)\right\Vert  & \le & O_{p}(n^{\tau_{min}})w^{-1}\left(O_{p}(n^{k_{1}})+w\|\hat{\beta}(\lambda)\|\right)
\end{eqnarray*}


By the same argument as above, $\|\hat{\beta}(\lambda)\|$ is easily
bounded by the solution to the analogous ridge regression problem:

\[
\hat{\beta}_{r}(\lambda)=\arg\min\frac{1}{2}\|y-g_{U\beta}\|_{T}^{2}+\lambda w\|U\beta\|_{2}^{2}
\]


Therefore

\[
\left\Vert \nabla_{\lambda}\hat{\beta}(\lambda)\right\Vert =w^{-1}O_{p}(n^{\max(2\tau_{min},\tau_{min}+k_{1})})
\]


Now we can bound the distance between $g_{\hat{\theta}(\ell)}$ and
$g_{\hat{\theta}(\ell+\delta)}$

\begin{eqnarray*}
\|g_{\hat{\theta}(\ell)}-g_{\hat{\theta}(\ell+\delta)}\|_{V} & \le & M\left\Vert U\hat{\beta}(\ell)-U\hat{\beta}(\ell+\delta)\right\Vert _{2}\\
 & = & M\delta w^{-1}O_{p}(n^{\max(2\tau_{min},\tau_{min}+k_{1})})
\end{eqnarray*}


Next consider if there are knots between points $\ell$ and $\ell+\delta$.
We can recover the same upper bound by chaining. That is, suppose
there are some countable number of knots $\lambda_{i}$ for $i=1,...,s$
(where $s$ can equal $\infty$) between $\ell$ and $\ell+\delta$,
with distances 
\[
\delta_{0}=\lambda_{1}-\ell,\delta_{i}=\lambda_{i+1}-\lambda_{i},\delta_{s}=\ell+\delta-\lambda_{s}
\]


Then

\begin{eqnarray*}
\|g_{\hat{\theta}(\ell)}-g_{\hat{\theta}(\ell+\delta)}\|_{V} & \le & \|g_{\hat{\theta}(\ell)}-g_{\hat{\theta}(\lambda_{1})}\|_{V}+\|g_{\hat{\theta}(\lambda_{s})}-g_{\hat{\theta}(\ell+\delta)}\|_{V}+\sum_{i=1}^{s}\|g_{\hat{\theta}(\lambda_{i})}-g_{\hat{\theta}(\lambda_{i+1})}\|_{V}\\
 & = & Mw^{-1}O_{p}(n^{\max(2\tau_{min},\tau_{min}+k_{1})})\sum_{i=0}^{s}\delta_{i}\\
 & = & M\delta w^{-1}O_{p}(n^{\max(2\tau_{min},\tau_{min}+k_{1})})
\end{eqnarray*}


Then the covering number is on the order of $n^{\kappa}$ where $\kappa$
grows linearly in $\tau_{min},\tau_{max}$ and $k_{1}$ and is independent
of the number of knots. 
\[
N\left(u,\mathcal{G}_{\lambda},\|\cdot\|_{V}\right)\le\frac{L}{uw}O_{p}(n^{\kappa})\implies H\left(u,\mathcal{G}_{\lambda},\|\cdot\|_{V}\right)\le\log\left(\frac{M}{uw}\right)+\kappa\log n
\]



\subsection{K-fold Cross-Validation}

The same convergence rate holds for $K$-fold cross-validation. For
$k=1,...,K$, let $D_{k}$ represent the $k$th fold and $D_{-k}$
denote all the folds minus the $k$th fold. For a given $\lambda$,
train over $D_{-k}$ and then validate over $D_{k}$. 

Let $\|h\|_{D_{k}}^{2}=\frac{1}{|D_{k}|}\sum_{i\in D_{k}}h(x_{i})^{2}$
and similarly for $\|h\|_{D_{-k}}^{2}$ for the set $D_{-k}$ and
$\|h\|_{D}^{2}$ for the set $D$. Let $\left(h,g\right)_{k}=\frac{1}{|D_{k}|}\sum_{i\in D_{k}}h(x_{i})g(x_{i})$
and $(h,g)_{-k}$ for the set $D_{-k}$ and $(h,g)_{D}$ for the set
$D$.

The joint optimization problem for k-fold cross-validation is 
\[
\hat{\lambda}=\arg\min_{\lambda\in\Lambda}\frac{1}{2}\sum_{k=1}^{K}\|y-g_{\hat{\theta}_{D_{-k}}(\lambda)}\|_{k}^{2}
\]


\[
\hat{\theta}_{D_{-k}}(\lambda)=\arg\min_{\theta\in\Theta}\frac{1}{2}\|y-g_{\theta}\|_{D_{-k}}^{2}+\lambda\left(P(\theta)+\frac{w}{2}\|\theta\|_{2}^{2}\right)
\]


and the final model produced is 
\[
\hat{\theta}_{D}(\hat{\lambda})=\arg\min_{\theta\in\Theta}\frac{1}{2}\|y-g_{\theta}\|_{D}^{2}+\lambda P(\theta)
\]


First, we show that the convergence rate of $\sum_{k=1}^{K}\|g_{\hat{\theta}_{k}(\tilde{\lambda})}-g_{\hat{\theta}_{k}(\hat{\lambda})}\|_{k}^{2}$
is either nearly-parametric or is the same as the optimal convergence
rate:

\[
\sqrt{\sum_{k=1}^{K}\|g_{\hat{\theta}_{D_{-k}}(\tilde{\lambda})}-g_{\hat{\theta}_{D_{-k}}(\hat{\lambda})}\|_{D_{k}}^{2}}=O_{P}(1)\left(\left(\frac{1-2\log w+\kappa\log n}{\min_{k\in1:K}|D_{k}|}\right)^{1/2}\vee\sqrt{\sum_{k=1}^{K}\|g_{\hat{\theta}_{D_{-k}}(\tilde{\lambda})}-g_{\theta^{*}}\|_{D_{k}}^{2}}\right)
\]


Then we show that the final model has a prediction error that is also
at the optimal rate:
\[
\|g_{\hat{\theta}_{D}(\tilde{\lambda})}-g_{\hat{\theta}_{D}(\hat{\lambda})}\|_{D}=O_{P}(1)\left(\left(\frac{1-2\log w+\kappa\log n}{\min_{k\in1:K}|D_{k}|}\right)^{1/2}\vee\|g_{\hat{\theta}_{D}(\tilde{\lambda})}-g_{\theta^{*}}\|_{D}\right)
\]



\subsubsection*{Proof}

\textbf{Step 1: Basic inequality crunching to bound the error for
each model $g_{\hat{\theta}_{D_{-k}}(\hat{\lambda})}$}

For ease of notation, we will denote $g_{\hat{\theta}_{D_{-k}}(\hat{\lambda})}=g_{\hat{\theta}_{k}(\hat{\lambda})}$.
By definition,

\[
\sum_{k=1}^{K}\|y-g_{\hat{\theta}_{k}(\hat{\lambda})}\|_{k}^{2}\le\sum_{k=1}^{K}\|y-g_{\hat{\theta}_{k}(\tilde{\lambda})}\|_{k}^{2}
\]


Rearranging as usual, we get 
\begin{eqnarray*}
\sum_{k=1}^{K}\|g_{\hat{\theta}_{k}(\tilde{\lambda})}-g_{\hat{\theta}_{k}(\hat{\lambda})}\|_{k}^{2} & \le & 2\sum_{k=1}^{K}\left(y-g_{\hat{\theta}_{k}(\tilde{\lambda})},g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}\\
 & \le & 2\left|\sum_{k=1}^{K}\left(\epsilon,g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}\right|+2\left|\sum_{k=1}^{K}\left(g^{*}-g_{\hat{\theta}_{k}(\tilde{\lambda})},g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}\right|
\end{eqnarray*}


\textbf{Case 1:} Suppose $\left|\sum_{k=1}^{K}\left(g^{*}-g_{\hat{\theta}_{k}(\tilde{\lambda})},g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}\right|$
is bigger

\begin{eqnarray*}
\sum_{k=1}^{K}\|g_{\hat{\theta}_{k}(\tilde{\lambda})}-g_{\hat{\theta}_{k}(\hat{\lambda})}\|_{k}^{2} & \le & 4\left|\sum_{k=1}^{K}\left(g^{*}-g_{\hat{\theta}_{k}(\tilde{\lambda})},g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}\right|\\
 & \le & 4\sqrt{\left(\sum_{k=1}^{K}\left\Vert g^{*}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right\Vert _{k}^{2}\right)\left(\sum_{k=1}^{K}\left\Vert g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right\Vert _{k}^{2}\right)}
\end{eqnarray*}


where the second inequality follows from Cauchy-Schwarz. Then

\[
\sum_{k=1}^{K}\left\Vert g^{*}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right\Vert _{k}^{2}\le O_{p}(1)\sum_{k=1}^{K}\left\Vert g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right\Vert _{k}^{2}
\]


Hence in this case, we recover whatever the convergence rate is for
the optimal $\tilde{\lambda}$.

\textbf{Case 2:} Suppose $\left|\sum_{k=1}^{K}\left(\epsilon,g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}\right|$
is bigger

\begin{eqnarray*}
\sum_{k=1}^{K}\|g_{\hat{\theta}_{k}(\tilde{\lambda})}-g_{\hat{\theta}_{k}(\hat{\lambda})}\|_{k}^{2} & \le & 4\left|\sum_{k=1}^{K}\left(\epsilon,g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}\right|
\end{eqnarray*}


Like Step 3, suppose $\|\epsilon\|_{n}\le2\sigma$. Then the basic
inequality allows us to bound 
\begin{eqnarray*}
\sum_{k=1}^{K}\|g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\|_{k}^{2} & \le & O_{p}(1)\left|\sum_{k=1}^{K}\left(\epsilon,g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}\right|\\
 & \le & O_{p}(1)\sqrt{\sum_{k=1}^{K}\|\epsilon\|_{k}^{2}}\sqrt{\sum_{k=1}^{K}\|g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\|_{k}^{2}}
\end{eqnarray*}


Hence we can suppose 
\[
\sqrt{\sum_{k=1}^{K}\|g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\|_{k}^{2}}\le2\sigma
\]


Now we can bound each of the $k$ empirical process terms $\left(\epsilon,g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}$
by a chaining argument. Following the same argument as Step 3 in the
main proof above, we get for $k=1:K$, 
\[
Pr\left(\left|\left(\epsilon,g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}\right|\ge\delta^{2}\wedge\|\epsilon\|_{n}\le2\sigma\right)\le C\exp\left(-|D_{k}|\frac{\delta^{2}}{c^{2}}\right)
\]


Putting Case 1 and 2 together, we get for all $\delta$ s.t.

\[
\delta\ge O_{p}(1)\left(\frac{1-\log(M/w)+\kappa\log n}{\min_{k\in1:K}|D_{k}|}\right)^{1/2}
\]


we have

\begin{eqnarray*}
 &  & Pr\left(\sum_{k=1}^{K}\|g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\|_{k}^{2}\ge\delta^{2}\right)\\
 & \le & Pr\left(\left|\sum_{k=1}^{K}\left(\epsilon,g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}\right|\ge\delta^{2}\wedge\|\epsilon\|_{n}\le2\sigma\right)+Pr\left(\|\epsilon\|_{n}\le2\sigma\right)+Pr\left(\sum_{k=1}^{K}\left\Vert g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right\Vert _{k}^{2}\ge\delta^{2}\right)\\
 & \le & \sum_{k=1}^{K}Pr\left(\left|\left(\epsilon,g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right)_{k}\right|\ge\delta^{2}/K\wedge\|\epsilon\|_{n}\le2\sigma\right)+Pr\left(\|\epsilon\|_{n}\le2\sigma\right)+Pr\left(\sum_{k=1}^{K}\left\Vert g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right\Vert _{k}^{2}\ge\delta^{2}\right)\\
 & \le & C\sum_{k=1}^{K}\exp\left(-|D_{k}|\frac{\delta^{2}}{c^{2}K}\right)+\exp\left(-\frac{n\sigma^{2}}{12K^{2}}\right)+Pr\left(\sum_{k=1}^{K}\left\Vert g_{\hat{\theta}_{k}(\hat{\lambda})}-g_{\hat{\theta}_{k}(\tilde{\lambda})}\right\Vert _{k}^{2}\ge\delta^{2}\right)
\end{eqnarray*}



\subsubsection*{Step 2: Bound the error of the final retrained model}

By definition of the norm,

\begin{eqnarray*}
\|g_{\hat{\theta}_{D}(\tilde{\lambda})}-g_{\hat{\theta}_{D}(\hat{\lambda})}\|_{D} & = & \sqrt{\sum_{k=1}^{K}\frac{|D_{k}|}{|D|}\|g_{\hat{\theta}_{D}(\tilde{\lambda})}-g_{\hat{\theta}_{D}(\hat{\lambda})}\|_{D_{k}}^{2}}
\end{eqnarray*}


Using the triangle inequality, we have

\[
\|g_{\hat{\theta}_{D}(\tilde{\lambda})}-g_{\hat{\theta}_{D}(\hat{\lambda})}\|_{D_{k}}\le\|g_{\hat{\theta}_{D}(\tilde{\lambda})}-g_{\hat{\theta}_{D_{-k}}(\tilde{\lambda})}\|_{D_{k}}+\|+\|g_{\hat{\theta}_{D_{-k}}(\tilde{\lambda})}-g_{\hat{\theta}_{D_{-k}}(\hat{\lambda})}\|_{D_{k}}+\|g_{\hat{\theta}_{D_{-k}}(\hat{\lambda})}-g_{\hat{\theta}_{D}(\hat{\lambda})}\|_{D_{k}}
\]


We have already bounded $\|g_{\hat{\theta}_{D_{-k}}(\tilde{\lambda})}-g_{\hat{\theta}_{D_{-k}}(\hat{\lambda})}\|_{D_{k}}$
in Step 1 of this proof. We now bound the two other terms. Consider
any $\lambda$. 

\[
\|y-g_{\hat{\theta}_{D}(\lambda)}\|_{D}^{2}+\lambda^{2}P(\hat{\theta}_{D}(\lambda))\le\|y-g_{\hat{\theta}_{D_{-k}}(\lambda)}\|_{D}^{2}+\lambda^{2}P(\hat{\theta}_{D_{-k}}(\lambda))
\]


\[
\|y-g_{\hat{\theta}_{D_{-k}}(\lambda)}\|_{D_{-k}}^{2}+\lambda^{2}P(\hat{\theta}_{D_{-k}}(\lambda))\le\|y-g_{\hat{\theta}_{D}(\lambda)}\|_{D_{-k}}^{2}+\lambda^{2}P(\hat{\theta}_{D}(\lambda))
\]


Adding the two inequalities (with appropriate scaling term), we have

\[
\|y-g_{\hat{\theta}_{D}(\lambda)}\|_{D}^{2}+\frac{|D_{-k}|}{|D|}\|y-g_{\hat{\theta}_{D_{-k}}(\lambda)}\|_{D_{-k}}^{2}\le\|y-g_{\hat{\theta}_{D_{-k}}(\lambda)}\|_{D}^{2}+\frac{|D_{-k}|}{|D|}\|y-g_{\hat{\theta}_{D}(\lambda)}\|_{D_{-k}}^{2}
\]


This can be simplified into

\[
\|y-g_{\hat{\theta}_{D}(\lambda)}\|_{D_{k}}^{2}\le\|y-g_{\hat{\theta}_{D_{-k}}(\lambda)}\|_{D_{k}}^{2}
\]


The basic inequality therefore tells us
\[
\|g_{\hat{\theta}_{D_{-k}}(\lambda)}-g_{\hat{\theta}_{D}(\lambda)}\|_{D_{k}}^{2}\le2\left|\left(\epsilon,g_{\hat{\theta}_{D_{-k}}(\lambda)}-g_{\hat{\theta}_{D}(\lambda)}\right)\right|_{D_{k}}+2\left|\left(g^{*}-g_{\hat{\theta}_{D_{-k}}(\lambda)},g_{\hat{\theta}_{D_{-k}}(\lambda)}-g_{\hat{\theta}_{D}(\lambda)}\right)\right|_{D_{k}}
\]


\textbf{Case 1: The first term is bigger.}

We apply the entropy arguments again. By Step 1 in the main proof

\[
H\left(u,\left\{ g_{\hat{\theta}_{D_{-k}}(\lambda)}:\lambda\in\Lambda\right\} ,\|\cdot\|_{D_{k}}\right)\le\log\left(\frac{M}{uw}\right)+\kappa\log n
\]


and 
\[
H\left(u,\left\{ g_{\hat{\theta}_{D}(\lambda)}:\lambda\in\Lambda\right\} ,\|\cdot\|_{D_{k}}\right)\le\log\left(\frac{M}{uw}\right)+\kappa\log n
\]


Hence by Step 3 of the main proof, we apply a chaining argument again
to get
\[
\left|\left(\epsilon,g_{\hat{\theta}_{D_{-k}}(\lambda)}-g_{\hat{\theta}_{D}(\lambda)}\right)\right|_{D_{k}}=O_{P}(1)\left(\frac{1-\log(M/w)+\kappa\log n}{|D_{k}|}\right)^{1/2}
\]


Therefore this first term in the basic inequality is a near-parametric
rate.

\textbf{Case 2: The second term is bigger}

In this case, we have that

\textbf{
\begin{eqnarray*}
\|g_{\hat{\theta}_{D_{-k}}(\lambda)}-g_{\hat{\theta}_{D}(\lambda)}\|_{D_{k}} & \le & O_{p}(1)\|g^{*}-g_{\hat{\theta}_{D_{-k}}(\lambda)}\|_{D_{k}}\\
 & \le & O_{p}(1)\left(\|g^{*}-g_{\hat{\theta}_{D_{-k}}(\tilde{\lambda})}\|_{D_{k}}+\|g_{\hat{\theta}_{D_{-k}}(\tilde{\lambda})}-g_{\hat{\theta}_{D_{-k}}(\lambda)}\|_{D_{k}}\right)
\end{eqnarray*}
}

Therefore if $\lambda=\tilde{\lambda}$, the second term is zero so
retraining the model does not change the convergence rate. If $\lambda=\hat{\lambda}$,
we recall that the second term was bounded in Step 1. Therefore retraining
the model with $\hat{\lambda}$ over the whole dataset preserves the
convergence rate.


\subsection{Multiple Regularization Parameters}

Suppose we are fitting model parameters $\theta$ but we'd like to
apply a separate penalty to $J$ partitions of the model parameters:
$P_{j}(\theta_{j})$ for $j=1,...,J$. Let $\boldsymbol{\lambda}=(\lambda_{1},...,\lambda_{J})$
be their corresponding penalty parameters. We suppose that $\Lambda$
is the box $[\lambda_{min},\lambda_{max}]^{J}$ where $\lambda_{min}=O_{p}(n^{-\tau_{min}})$
and $\lambda_{max}=O_{p}(n^{\tau_{max}})$.

For simplicity, suppose the problem is smooth and that we tune $\boldsymbol{\lambda}$
over a training/validation split. (One can probably use the same arguments
as above to extend this to the case when the problem is smooth almost
everywhere and $K$-fold cross validation.)

Consider the joint optimization problem

\[
\hat{\boldsymbol{\lambda}}=\arg\min_{\boldsymbol{\lambda}\in\Lambda}\frac{1}{2}\|y-g_{\hat{\theta}(\boldsymbol{\lambda})}\|_{V}^{2}
\]
\[
\hat{\theta}(\boldsymbol{\lambda})=\arg\min_{\theta}\frac{1}{2}\|y-g_{\theta}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\theta_{j})+\frac{w}{2}\|\theta_{j}\|_{2}^{2}\right)
\]


The convergence rate of $\|g_{\hat{\theta}(\hat{\boldsymbol{\lambda}})}-g_{\hat{\theta}(\tilde{\boldsymbol{\lambda}})}\|_{V}$
is either nearly-parametric or the optimal convergence rate 
\begin{eqnarray*}
\|g_{\hat{\theta}(\hat{\boldsymbol{\lambda}})}-g_{\hat{\theta}(\tilde{\boldsymbol{\lambda}})}\|_{V} & = & O_{P}(1)\left(\left(\frac{J(1-\log w+\kappa\log n)}{|V|}\right)^{1/2}\vee\|g_{\theta^{*}}-g_{\hat{\theta}(\tilde{\boldsymbol{\lambda}})}\|_{V}\right)
\end{eqnarray*}


Note that we do indeed have to pay a price for tuning over $J$ dimensions.


\subsubsection*{Proof}


\subsubsection*{Step 0: Find $\nabla_{\lambda_{j}}\hat{\theta}(\boldsymbol{\lambda})$
via implicit differentiation}

We can again use implicit differentiation to get $\nabla_{\lambda_{j}}\hat{\theta}(\boldsymbol{\lambda})$.
The equations get bulky, but the logic is straightforward.

Since $\hat{\theta}(\boldsymbol{\lambda})$ is a local minima, 
\[
\nabla_{\theta}\left(\|y-g_{\hat{\theta}}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P(\hat{\theta}_{j})+\frac{w}{2}\|\hat{\theta}_{j}\|_{2}^{2}\right)\right)=0
\]


which simplifies to

\[
\nabla_{\theta}\|y-g_{\hat{\theta}}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(\nabla_{\theta}P(\hat{\theta}_{j})+w\hat{\theta}_{j}\right)=0
\]


Implicit differentiation wrt $\lambda_{\ell}$ (for $\ell=1,...,J$)
gives us

\[
\nabla_{\lambda_{\ell}}\hat{\theta}(\boldsymbol{\lambda})=\left(\nabla_{\theta}^{2}\|y-g_{\hat{\theta}(\boldsymbol{\lambda})}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\nabla_{\theta}^{2}P(\theta_{j})+diag\left(\lambda_{j}wI_{j}\right)\right)^{-1}\left(\nabla_{\theta}P(\theta_{\ell})+w\left[\begin{array}{c}
0\\
\theta_{\ell}\\
0
\end{array}\right]\right)
\]


where $I_{j}$ are appropriately-sized identity matrices.


\subsubsection*{Step 1: Find the entropy of the model class $\mathcal{G}_{\boldsymbol{\lambda}}$}

First we show that the entropy $H(u,\mathcal{G}_{\boldsymbol{\lambda}},\|\cdot\|_{V})$
of the class 
\[
\mathcal{G}_{\boldsymbol{\lambda}}=\left\{ \theta_{\boldsymbol{\lambda}}:\boldsymbol{\lambda}\in\Lambda\right\} 
\]
 

is bounded at a near-parametric rate:
\[
H\left(u,\mathcal{G}_{\lambda},\|\cdot\|_{V}\right)\le J\left(\log\left(\frac{M}{uw}\right)+\kappa\log n\right)
\]


By the mean value theorem, for any vector $\boldsymbol{\delta}$ there
is some $\alpha\in[0,1]$ s.t. 
\[
\|\hat{\theta}(\boldsymbol{\ell})-\hat{\theta}(\boldsymbol{\ell}+\boldsymbol{\delta})\|=\|\boldsymbol{\delta}\|\left\Vert \nabla_{\boldsymbol{\lambda}}\hat{\theta}(\boldsymbol{\lambda})|_{\boldsymbol{\lambda}=\boldsymbol{\ell}+\alpha\boldsymbol{\delta}}\right\Vert 
\]


Since the problem is smooth, we can apply implicit differentiation
to get the derivative

\[
\nabla_{\boldsymbol{\lambda}}\hat{\theta}(\boldsymbol{\lambda})=-H(\boldsymbol{\lambda})^{-1}\left(\nabla_{\theta}P(\theta)+w\theta\right)
\]


where the Hessian matrix was derived above 
\[
H(\boldsymbol{\lambda})=\nabla_{\theta}^{2}\|y-g_{\hat{\theta}(\boldsymbol{\lambda})}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\nabla_{\theta}^{2}P(\theta_{j})+diag\left(\lambda_{j}wI_{j}\right)
\]


The minimum eigenvalue of $H(\boldsymbol{\lambda})$ is at least $wO_{p}(n^{-\tau_{min}})$.

From the asumptions, we have that for every $\ell=1,...,J$

\begin{eqnarray*}
\left\Vert \nabla_{\lambda_{\ell}}\hat{\theta}(\boldsymbol{\lambda})\right\Vert  & \le & \left\Vert H(\boldsymbol{\lambda})^{-1}\left(\nabla_{\theta}P(\theta)+w\hat{\theta}(\boldsymbol{\lambda})\right)\right\Vert \\
 & \le & O_{p}(n^{\tau_{min}})w^{-1}\left(O_{p}(n^{k_{1}})+w\|\hat{\theta}(\boldsymbol{\lambda})\|\right)
\end{eqnarray*}


Again we can bound $\|\hat{\theta}(\boldsymbol{\lambda})\|$ by the
solution to the ridge regression problem to get that
\[
\|\hat{\theta}(\boldsymbol{\lambda})\|\le w^{-1}O_{P}(n^{\tau_{\min}})
\]


Therefore

\[
\left\Vert \nabla_{\lambda_{j}}\hat{\theta}(\boldsymbol{\lambda})\right\Vert =w^{-1}O_{p}(n^{\max(2\tau_{min},\tau_{min}+k_{1})})
\]


Now we can bound the distance between $g_{\hat{\theta}(\boldsymbol{\ell})}$
and $g_{\hat{\theta}(\boldsymbol{\ell}+\boldsymbol{\delta})}$

\begin{eqnarray*}
\|g_{\hat{\theta}(\boldsymbol{\ell})}-g_{\hat{\theta}(\boldsymbol{\ell}+\boldsymbol{\delta})}\|_{V} & \le & M\left\Vert \hat{\theta}(\boldsymbol{\ell})-\hat{\theta}(\boldsymbol{\ell}+\boldsymbol{\delta})\right\Vert _{2}\\
 & = & M\left\Vert \left.\sum_{j=1}^{J}\delta_{j}\nabla_{\lambda_{j}}\hat{\theta}(\boldsymbol{\lambda})\right|_{\boldsymbol{\lambda}=\boldsymbol{\ell}+\alpha\boldsymbol{\delta}}\right\Vert \\
 & \le & M\sum_{j=1}^{J}|\delta_{j}|\left\Vert \left.\nabla_{\lambda_{j}}\hat{\theta}(\boldsymbol{\lambda})\right|_{\boldsymbol{\lambda}=\boldsymbol{\ell}+\alpha\boldsymbol{\delta}}\right\Vert \\
 & \le & M\|\boldsymbol{\delta}\|w^{-1}O_{p}(n^{\max(2\tau_{min},\tau_{min}+k_{1})})
\end{eqnarray*}


Hence we can form a $u$-covering set for $\mathcal{G}_{\boldsymbol{\lambda}}=\left\{ g_{\hat{\theta}(\boldsymbol{\lambda})}:\boldsymbol{\lambda}\in\Lambda\right\} $
by covering the box $\Lambda=[\lambda_{min},\lambda_{max}]^{J}$ with
balls of radius $d=uwO_{p}(n^{-\max(2\tau_{min},\tau_{min}+k_{1})})/M$.

So the total number of balls needed is on the order of

\[
\left(\frac{M}{uw}O_{p}(n^{\kappa})\right)^{J}
\]


where $\kappa$ grows linearly in $\tau_{min},\tau_{max}$ and $k_{1}$.
\[
N\left(u,\mathcal{G}_{\boldsymbol{\lambda}},\|\cdot\|_{V}\right)\le\left(\frac{M}{uw}O_{p}(n^{\kappa})\right)^{J}\implies H\left(u,\mathcal{G}_{\boldsymbol{\lambda}},\|\cdot\|_{V}\right)\le J\left(\log\left(\frac{M}{uw}\right)+\kappa\log n\right)
\]



\subsubsection*{Step 2: Exactly the same}


\subsubsection*{Step 3: Exactly the same with additional term $J^{1/2}$}


\subsubsection*{Step 4: Same with an additional term $J^{1/2}$}


\section{Examples}


\subsubsection*{Ridge Regression}

\[
\hat{\theta}(\lambda)=\arg\min_{\theta}\frac{1}{2}\|y-X\theta\|_{T}^{2}+\frac{\lambda}{2}\|\theta\|^{2}
\]


The Hessian is 
\[
X^{T}X+\lambda I
\]
so its minimum eigenvalue is $\lambda$.

The derivative of the penalty wrt $\theta$ is 
\[
\nabla_{\theta}P(\theta)=\theta
\]


Since $\theta=(X^{T}X+\lambda I)^{-1}X^{T}y$, the derivative has
bounded norm $\|\nabla_{\theta}P(\theta)\|=O_{P}(n^{\tau_{min}})$.

All other assumptions are obviously satisfied. Note that in this problem,
we can drop the tiny additional ridge penalty entirely.


\subsubsection*{Smoothing Splines with a Sobolev Penalty}

The optimization problem can be formulated as

\[
\hat{\theta}(\lambda)=\arg\min_{\theta}\frac{1}{2}\|y-\theta\|_{T}^{2}+\frac{\lambda}{2}\theta^{T}K\theta
\]


where $K=N^{-T}\Omega N$, $N$ is the normalized B-splines evaluated
at the input points, and $\Omega$ has entries $\Omega_{ij}=\int b_{i}"(x)b_{j}"(x)dx$.

Assume the input points $i/n$ for $i=1:n$ and we fit $y$ with cubic
B-splines. Then $K=DC^{-1}Dn^{3}$ where $D$ is the (universal, non-data-dependent)
second-order discrete diference operator and $C$ is a tridiagonal
matrix with diagonal elements equal to 2/3 and off-diagonal elements
equal to 1/6.

The Hessian is 
\[
I+\lambda K
\]
so its minimum eigenvalue is 1.

The derivative of the penalty wrt $\theta$ is 
\[
\nabla_{\theta}P(\theta)=K\theta
\]


The maximum eigenvalue of $K$ is on the order of $O_{p}(n^{-3})$.
Clearly$\|\theta\|$ can be bounded by $\|y\|$ modulo a constant.
So $\|\nabla_{\theta}P(\theta)\|=O_{P}(n^{3})$. Also, $\nabla_{\theta}^{2}P(\theta)=K$
so its norm is shrinking at a rate of $O_{P}(n^{-3})$.

All other assumptions are obviously satisfied. Note that in this problem,
we can drop the tiny additional ridge penalty entirely.

\textbf{Lasso (with a tiny ridge)}

\[
\hat{\theta}(\lambda)=\arg\min_{\theta}\frac{1}{2}\|y-X\theta\|_{T}^{2}+\lambda\left(\|\theta\|_{1}+\frac{w}{2}\|\theta\|_{2}^{2}\right)
\]


With modifications to the proof above, we can probably show that the
proof carries through for penalties that are smooth almost everywhere.
The main tricky part is dealing with the fact that the Hessian with
respect to the differentiable space changes in size for different
values of $\lambda$.

Anyhow, by a hand-wavy argument, we have that the Hessian matrix with
respect to the differentiable space $S_{\lambda}$ is 
\[
X_{S_{\lambda}}^{T}X_{S_{\lambda}}+\lambda wI_{S_{\lambda}}
\]
 so its minimum eigenvalue is $w\lambda$.

The lasso penalty clearly satisfies our assumptions since its derivative
$\nabla_{\theta}\|\theta\|_{1}=sgn(\theta)$ has bounded norm $\|\nabla_{\theta}\|\theta\|_{1}\|\le p$.

All other assumptions are satisfied.
\end{document}
