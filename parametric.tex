%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage[landscape]{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\title{Proofs for Smoothness of Parametric Regression Models}

\maketitle

\section*{Intro}

In this document, we consider parametric regression models $g(\cdot|\boldsymbol{\theta})$
where $\boldsymbol{\theta}\in\mathbb{R}^{p}$. Throughout, we will
suppose $\boldsymbol{\theta}^{*}$ is the model such that 
\[
\boldsymbol{\theta}^{*}=\arg\min_{\theta\in\Theta}E_{x,y}\left[\left(y-g(x|\boldsymbol{\theta})\right)^{2}\right]
\]


Technically, all the proofs require is that $\boldsymbol{\theta}^{*}\in\Theta$
is fixed. In the convergence rate proofs, we will need $\boldsymbol{\theta}^{*}$
to satisfy $E[y|x]=g(x|\boldsymbol{\theta}^{*})$.

We are interested in establishing inequalities of the form 
\[
\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}-\boldsymbol{\hat{\theta}_{\lambda^{(2)}}}\|_{2}\le C\|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}
\]


If the functions are Lipschitz in their parameterization, we will
also be able to bound the distance between the actual functions. That
is, if there are constants $L>0$ and $r\in\mathbb{R}$, such that
for all $\boldsymbol{\theta_{1},\theta_{2}}$ 
\[
\|g(\cdot|\boldsymbol{\theta}_{1})-g(\cdot|\boldsymbol{\theta}_{2})\|_{\infty}\le Lp^{r}\|\boldsymbol{\theta}_{1}-\boldsymbol{\theta}_{2}\|_{2}
\]


Then
\[
\|g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(2)}}})\|_{\infty}\le Lp^{r}C\|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}
\]



\subsubsection*{Document Outline}

First, we consider smooth training criteria and prove smoothness for
two parametric regression examples:
\begin{enumerate}
\item Multiple penalties for a single model
\[
\hat{\boldsymbol{\theta}}_{\lambda}=\arg\min_{\theta\in\mathbb{R}^{p}}\frac{1}{2}\|y-g(\cdot|\boldsymbol{\theta})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta})+\frac{w}{2}\|\boldsymbol{\theta}\|_{2}^{2}\right)
\]

\item Additive model
\[
\hat{\boldsymbol{\theta}}_{\lambda}=\arg\min_{\theta\in\mathbb{R}^{p}}\frac{1}{2}\|y-\sum_{j=1}^{J}g_{j}(\cdot|\boldsymbol{\theta}_{j})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta}_{j})+\frac{w}{2}\|\boldsymbol{\theta}_{j}\|_{2}^{2}\right)
\]

\end{enumerate}
Then we will extend these results to non-smooth penalty functions.

Finally we will consider examples of parametric penalty functions.
This includes a deep dive into the Sobolev penalty.


\section{Multiple smooth penalties for a single model}

The function class of interest are the minimizers of the penalized
least squares criterion: 
\[
\mathcal{G}(T)=\left\{ \hat{\boldsymbol{\theta}}_{\lambda}=\arg\min_{\theta\in\mathbb{R}^{p}}\frac{1}{2}\|y-g(\cdot|\boldsymbol{\theta})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta})+\frac{w}{2}\|\boldsymbol{\theta}\|_{2}^{2}\right):\boldsymbol{\lambda}\in\Lambda\right\} 
\]


where $\Lambda=[\lambda_{min},\lambda_{max}]^{J}$ and $w>0$ is a
fixed constant.

Suppose that the penalties and the function $g(x|\boldsymbol{\theta})$
are twice-differentiable and convex wrt $\boldsymbol{\theta}$: 
\begin{itemize}
\item Suppose that $\nabla_{\theta}^{2}P_{j}(\boldsymbol{\theta})$ are
PSD matrices for all $j=1,...,J$. 
\item Suppose that $\nabla_{\theta}^{2}\|y-g(x|\boldsymbol{\theta})\|_{T}^{2}$
is a PSD matrix.
\end{itemize}
Suppose there is some $K>0$ such that for all $j=1,...,J$ and any
$\boldsymbol{\theta,\beta},m'$, we have

\[
\left|\left.\frac{\partial}{\partial m}P_{j}\left(\boldsymbol{\theta}+m\boldsymbol{\beta}\right)\right|_{m=m'}\right|\le K\|\boldsymbol{\beta}\|_{2}
\]


Then for any $\boldsymbol{\lambda^{(1)},\lambda^{(2)}}\in\Lambda$
we have
\[
\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}-\boldsymbol{\hat{\theta}_{\lambda^{(2)}}}\|_{2}\le\|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}\left(w\sqrt{J}\lambda_{min}\right)^{-1}\left(K+w\sqrt{\frac{2}{J\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)C}\right)
\]


where 
\[
C=\frac{1}{2}\|y-g(\cdot|\boldsymbol{\theta}^{*})\|_{T}^{2}+\lambda_{max}\sum_{j=1}^{J}\left(P_{j}(\boldsymbol{\theta}^{*})+\frac{w}{2}\|\boldsymbol{\theta}^{*}\|_{2}^{2}\right)
\]



\subsubsection*{Proof}

Consider any $\boldsymbol{\lambda^{(1)},\lambda^{(2)}}\in\Lambda$.
Let $\boldsymbol{\beta}=\hat{\boldsymbol{\theta}}_{\lambda^{(2)}}-\hat{\boldsymbol{\theta}}_{\lambda^{(1)}}$.

Define 
\[
\hat{m}_{\boldsymbol{\beta}}(\boldsymbol{\lambda})=\arg\min_{m\in\mathbb{R}}\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})+\frac{w}{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta}\|_{2}^{2}\right)
\]


By definition, we know that $\hat{m}_{\beta}(\boldsymbol{\lambda^{(2)}})=1$
and $\hat{m}_{\beta}(\boldsymbol{\lambda^{(1)}})=0$.

\textbf{1. We calculate $\nabla_{\lambda}\hat{m}_{\boldsymbol{\beta}}(\boldsymbol{\lambda})$
using the implicit differentiation trick.}

By the KKT conditions, we have 
\[
\left.\frac{\partial}{\partial m}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right)+\sum_{j=1}^{J}\lambda_{j}w\langle\boldsymbol{\beta},\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta}\rangle\right|_{m=\hat{m}_{\beta}(\boldsymbol{\lambda})}=0
\]


Now we implicitly differentiate with respect to $\lambda_{\ell}$
for $\ell=1,2,...,J$

\[
\frac{\partial}{\partial\lambda_{\ell}}\left\{ \left.\left[\frac{\partial}{\partial m}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right)+\sum_{j=1}^{J}\lambda_{j}w\langle\boldsymbol{\beta},\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta}\rangle\right]\right|_{m=\hat{m}_{\beta}(\boldsymbol{\lambda})}\right\} =0
\]


By the product rule and chain rule, we have 

\[
\left.\left\{ \left[\frac{\partial^{2}}{\partial m^{2}}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right)+\sum_{j=1}^{J}\lambda_{j}w\|\boldsymbol{\beta}\|_{2}^{2}\right]\frac{\partial}{\partial\lambda_{\ell}}\hat{m}_{\beta}(\boldsymbol{\lambda})+\frac{\partial}{\partial m}P_{\ell}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})+w\langle\boldsymbol{\beta},\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta}\rangle\right\} \right|_{m=\hat{m}_{\beta}(\boldsymbol{\lambda})}=0
\]


Rearranging, for every $\ell=1,...,J$, we get

\[
\frac{\partial}{\partial\lambda_{\ell}}\hat{m}_{\boldsymbol{\beta}}(\boldsymbol{\lambda})=\left.-\left[\frac{\partial^{2}}{\partial m^{2}}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right)+\sum_{j=1}^{J}\lambda_{j}w\|\boldsymbol{\beta}\|_{2}^{2}\right]^{-1}\left[\frac{\partial}{\partial m}P_{\ell}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})+w\langle\boldsymbol{\beta},\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta}\rangle\right]\right|_{m=\hat{m}_{\beta}(\boldsymbol{\lambda})}
\]


In vector notation, we have 
\[
\nabla_{\lambda}\hat{m}_{\boldsymbol{\beta}}(\boldsymbol{\lambda})=\left.-\left[\frac{\partial^{2}}{\partial m^{2}}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right)+\sum_{j=1}^{J}\lambda_{j}w\|\boldsymbol{\beta}\|_{2}^{2}\right]^{-1}\left[\nabla_{m}P(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})+w\langle\boldsymbol{\beta},\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta}\rangle\boldsymbol{1}\right]\right|_{m=\hat{m}_{\beta}(\boldsymbol{\lambda})}
\]


where $\nabla_{m}P(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})$
is the $J$-dimensional vector

\[
\nabla_{m}P(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})=\left[\begin{array}{c}
\frac{\partial}{\partial m}P_{1}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\\
...\\
\frac{\partial}{\partial m}P_{J}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})
\end{array}\right]
\]


\textbf{2. Bound $\|\nabla_{\lambda}\hat{m}_{\boldsymbol{\beta}}(\boldsymbol{\lambda})\|$}

\textbf{Bounding the first multiplicand:}

The first multiplicand is bounded by 
\[
\left|\frac{\partial^{2}}{\partial m^{2}}\left(\frac{1}{2}\left\Vert y-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right)+\sum_{j=1}^{J}\lambda_{j}w\|\boldsymbol{\beta}\|_{2}^{2}\right|^{-1}\le\left(wJ\lambda_{min}\|\boldsymbol{\beta}\|_{2}^{2}\right)^{-1}
\]


since the mean squared error and the penalty functions are convex.

\textbf{Bounding the second multiplicand:}

The first summand in the second multiplicand is bounded by assumption
\[
\left|\frac{\partial}{\partial m}P_{\ell}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+m\boldsymbol{\beta})\right|\le K\|\boldsymbol{\beta}\|_{2}
\]


The second summand in the second multiplicand is bounded by
\begin{eqnarray}
\left|w\langle\boldsymbol{\beta},\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+\hat{m}_{\beta}(\boldsymbol{\lambda})\boldsymbol{\beta}\rangle\right| & \le & w\|\boldsymbol{\beta}\|_{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+\hat{m}_{\beta}(\boldsymbol{\lambda})\boldsymbol{\beta}\|_{2}
\end{eqnarray}


We need to bound $\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+\hat{m}_{\beta}(\boldsymbol{\lambda})\boldsymbol{\beta}\|_{2}$.
By definition of $\hat{m}_{\beta}(\boldsymbol{\lambda})$ , 
\begin{eqnarray*}
\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+\hat{m}_{\beta}(\boldsymbol{\lambda})\boldsymbol{\beta}\|_{2}^{2} & \le & \frac{1}{2}\|y-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})+\frac{w}{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}\|_{2}^{2}\right)\\
 & = & \frac{1}{2}\|y-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})+\frac{w}{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}\|_{2}^{2}\right)+\sum_{j=1}^{J}\left(\lambda_{j}-\lambda_{j}^{(1)}\right)\left(P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})+\frac{w}{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}\|_{2}^{2}\right)
\end{eqnarray*}


To bound the first part of the right hand side, use the definition
of $\hat{\boldsymbol{\theta}}_{\lambda^{(1)}}$: 
\begin{eqnarray*}
\frac{1}{2}\|y-g(\cdot|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})+\frac{w}{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}\|_{2}^{2}\right) & \le & \frac{1}{2}\|y-g(\cdot|\boldsymbol{\theta}^{*})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}(\boldsymbol{\theta}^{*})+\frac{w}{2}\|\boldsymbol{\theta}^{*}\|_{2}^{2}\right)\\
 & \le & \frac{1}{2}\|y-g(\cdot|\boldsymbol{\theta}^{*})\|_{T}^{2}+\lambda_{max}\sum_{j=1}^{J}\left(P_{j}(\boldsymbol{\theta}^{*})+\frac{w}{2}\|\boldsymbol{\theta}^{*}\|_{2}^{2}\right)\\
 & = & C
\end{eqnarray*}


To bound the second part of the right hand side, note that 
\begin{eqnarray*}
\sum_{j=1}^{J}\left(\lambda_{j}-\lambda_{j}^{(1)}\right)\left(P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})+\frac{w}{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}\|_{2}^{2}\right) & \le & \sum_{j=1}^{J}\left(\lambda_{j}-\lambda_{j}^{(1)}\right)\left[\max_{k=1:J}P_{k}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)}}\|_{2}^{2}\right]\\
 & \le & J\lambda_{max}\left[\max_{k=1:J}P_{k}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)}}\|_{2}^{2}\right]
\end{eqnarray*}


Combining the above three inequalities, we get
\begin{equation}
\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+\hat{m}_{\beta}(\boldsymbol{\lambda})\boldsymbol{\beta}\|_{2}^{2}\le C+J\lambda_{max}\left[\max_{k=1:J}P_{k}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)}}\|_{2}^{2}\right]
\end{equation}


To bound $\max_{k=1:J}P_{k}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)}}\|_{2}^{2}$,
we note that by the definition of $\hat{\boldsymbol{\theta}}_{\lambda^{(1)}}$,
we have 

\begin{eqnarray*}
\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)}}\|_{2}^{2}\right) & \le & \frac{1}{2}\|y-g(\cdot|\boldsymbol{\theta}^{*})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}(\boldsymbol{\theta}^{*})+\frac{w}{2}\|\boldsymbol{\theta}^{*}\|_{2}^{2}\right)\\
 & \le & C
\end{eqnarray*}


Therefore
\begin{equation}
\max_{k=1:J}P_{k}(\boldsymbol{\hat{\theta}_{\lambda^{(1)}}})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)}}\|_{2}^{2}\le\frac{C}{\lambda_{min}}
\end{equation}


Plugging (3) into (2) above, we get 
\begin{eqnarray}
\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+\hat{m}_{\beta}(\boldsymbol{\lambda})\boldsymbol{\beta}\|_{2}^{2} & \le & \left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)C
\end{eqnarray}


We can combine (4) with the fact that 
\[
J\lambda_{min}\frac{w}{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+\hat{m}_{\beta}(\boldsymbol{\lambda})\boldsymbol{\beta}\|_{2}^{2}\le\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+\hat{m}_{\beta}(\boldsymbol{\lambda})\boldsymbol{\beta}\|_{2}^{2}
\]


to get
\[
\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+\hat{m}_{\beta}(\boldsymbol{\lambda})\boldsymbol{\beta}\|_{2}\le\sqrt{\frac{2}{J\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)C}
\]


Plug the inequality above into (1) to get 
\[
w\langle\boldsymbol{\beta},\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}+\hat{m}_{\beta}(\boldsymbol{\lambda})\boldsymbol{\beta}\rangle\le w\|\boldsymbol{\beta}\|_{2}\sqrt{\frac{2}{J\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)C}
\]


Finally we have bounded the derivative of $\frac{\partial}{\partial\lambda_{\ell}}\hat{m}_{\beta}(\boldsymbol{\lambda})$.
For every $\ell=1,...,J$, we have 
\begin{eqnarray*}
\left|\frac{\partial}{\partial\lambda_{\ell}}\hat{m}_{\beta}(\boldsymbol{\lambda})\right| & \le & \left(wJ\lambda_{min}\|\boldsymbol{\beta}\|_{2}^{2}\right)^{-1}\left(K\|\boldsymbol{\beta}\|_{2}+w\|\boldsymbol{\beta}\|_{2}\sqrt{\frac{2}{J\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)C}\right)\\
 & = & \left(wJ\lambda_{min}\|\boldsymbol{\beta}\|_{2}\right)^{-1}\left(K+w\sqrt{\frac{2}{J\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)C}\right)
\end{eqnarray*}


We can sum up these bounds to bound the norm of the gradient $\nabla_{\lambda}\hat{m}_{\beta}(\boldsymbol{\lambda})$:
\begin{eqnarray*}
\left\Vert \nabla_{\lambda}\hat{m}_{\beta}(\boldsymbol{\lambda})\right\Vert  & = & \sqrt{\sum_{\ell=1}^{J}\left(\frac{\partial}{\partial\lambda_{\ell}}\hat{m}_{\beta}(\boldsymbol{\lambda})\right)^{2}}\\
 & \le & \left(w\lambda_{min}\sqrt{J}\|\boldsymbol{\beta}\|_{2}\right)^{-1}\left(K+w\sqrt{\frac{2}{J\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)C}\right)
\end{eqnarray*}


\textbf{3. Apply Mean Value Theorem}

Since the training criterion is smooth, then $\hat{m}_{\boldsymbol{\beta}}(\boldsymbol{\lambda})$
is continuous and differentiable over the line segment $\left\{ \alpha\boldsymbol{\lambda^{(1)}}+(1-\alpha)\boldsymbol{\lambda^{(2)}}:\alpha\in[0,1]\right\} $.
Therefore by MVT, there is some $\alpha\in(0,1)$ such that 
\begin{eqnarray*}
\left|\hat{m}_{\beta}(\boldsymbol{\lambda^{(2)}})-\hat{m}_{\beta}(\boldsymbol{\lambda^{(1)}})\right| & = & \left|\left.\left\langle \boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}},\nabla_{\lambda}\hat{m}_{\beta}(\boldsymbol{\lambda})\right\rangle \right|_{\boldsymbol{\lambda}=\alpha\boldsymbol{\lambda^{(1)}}+(1-\alpha)\boldsymbol{\lambda^{(2)}}}\right|\\
 & \le & \|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}\left\Vert \left.\nabla_{\lambda}\hat{m}_{\beta}(\boldsymbol{\lambda})\right|_{\boldsymbol{\lambda}=\alpha\boldsymbol{\lambda^{(1)}}+(1-\alpha)\boldsymbol{\lambda^{(2)}}}\right\Vert \\
 & \le & \|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}\left(w\sqrt{J}\lambda_{min}\|\boldsymbol{\beta}\|_{2}\right)^{-1}\left(K+w\sqrt{\frac{2}{J\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)C}\right)
\end{eqnarray*}


Recall that $\hat{m}_{\beta}(\boldsymbol{\lambda^{(2)}})-\hat{m}_{\beta}(\boldsymbol{\lambda^{(1)}})=1$.
Rearranging, we get
\[
\|\boldsymbol{\beta}\|_{2}=\|\boldsymbol{\hat{\theta}_{\lambda^{(1)}}}-\boldsymbol{\hat{\theta}_{\lambda^{(2)}}}\|_{2}\le\|\boldsymbol{\lambda^{(2)}}-\boldsymbol{\lambda^{(1)}}\|_{2}\left(w\sqrt{J}\lambda_{min}\right)^{-1}\left(K+w\sqrt{\frac{2}{J\lambda_{min}w}\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)C}\right)
\]



\section{Additive Model}

The function class of interest are the minimizers of the penalized
least squares criterion: 
\[
\mathcal{G}(T)=\left\{ \hat{\boldsymbol{\theta}}_{\lambda}=\arg\min_{\theta\in\mathbb{R}^{p}}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\boldsymbol{\theta}_{j})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta}_{j})+\frac{w}{2}\|\boldsymbol{\theta}_{j}\|_{2}^{2}\right):\boldsymbol{\lambda}\in\Lambda\right\} 
\]


where $\Lambda=[\lambda_{min},\lambda_{max}]^{J}$.

Suppose that the penalties and the mean squared error $\|y-\sum_{j=1}^{J}g_{j}(x|\boldsymbol{\theta}_{j})\|_{T}^{2}$
are twice-differentiable and convex wrt $\boldsymbol{\theta}$
\begin{itemize}
\item $\nabla_{\boldsymbol{\theta}_{j}}^{2}P_{j}(\boldsymbol{\theta}_{j})$
are PSD matrices for all $j=1,...,J$ 
\item $\nabla_{\boldsymbol{\theta}}^{2}\|y-\sum_{j=1}^{J}g_{j}(x|\boldsymbol{\theta}_{j})\|_{T}^{2}$
is a PSD matrix.
\end{itemize}
Suppose for each $j=1,...,J$, there is a constant $K_{j}\ge0$ such
that for all $\boldsymbol{\beta},\boldsymbol{\theta},m'$, we either
have

\begin{equation}
\left|\frac{\partial}{\partial m}P_{j}(\boldsymbol{\theta}+m\boldsymbol{\beta})\right|_{m=m'}\le K_{j}\|\boldsymbol{\beta}\|_{2}
\end{equation}


or 
\begin{equation}
\left\Vert \left.\frac{\partial}{\partial m}g_{j}(X_{T,j}|\boldsymbol{\theta}+m\boldsymbol{\beta})\right|_{m=m'}\right\Vert =\sqrt{\sum_{i=1}^{n}\left(\left.\frac{\partial}{\partial m}g_{j}(x_{T,j}|\boldsymbol{\theta}+m\boldsymbol{\beta})\right|_{m=m'}\right)^{2}}\le K_{j}\|\boldsymbol{\beta}\|_{2}
\end{equation}


(These conditions bound the spectrum of the penalty function or the
function itself.)

For $j=1,..,J$, let 

\[
d_{j}=\begin{cases}
\left(K_{j}+w\sqrt{\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\frac{2C}{\lambda_{min}w}}\right) & \mbox{if assumption (5) holds for }P_{j}\\
\frac{1}{\lambda_{min}}K_{j}\sqrt{\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)2C} & \mbox{if assumption (6) holds for }g_{j}
\end{cases}
\]


Let 
\[
C=\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\boldsymbol{\theta}_{j}^{*})\right\Vert _{T}^{2}+\lambda_{max}\sum_{j=1}^{J}\left(P_{j}(\boldsymbol{\theta}_{j}^{*})+\frac{w}{2}\|\boldsymbol{\theta}_{j}^{*}\|_{2}^{2}\right)
\]


Then for any $\boldsymbol{\lambda^{(1)},\lambda^{(2)}}\in\Lambda$
we have for all $j=1,...,J$
\[
\|\boldsymbol{\theta}_{\lambda^{(1)},j}-\boldsymbol{\theta}_{\lambda^{(2)},j}\|\le\left\Vert \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\right\Vert \lambda_{min}^{-1}w^{-1}\left(\max_{j=1,..,J}d_{j}\right)
\]



\subsubsection*{Proof}

Consider any $\boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\in\Lambda$.
Let $\boldsymbol{\beta}_{j}=\hat{\boldsymbol{\theta}}_{\lambda^{(2)},j}-\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}$
for all $j=1,...,J$.

Define 
\[
\boldsymbol{\hat{m}}(\boldsymbol{\lambda})=\arg\min_{\boldsymbol{m}}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j}\|_{2}^{2}\right)
\]


By definition, we know that $\boldsymbol{\hat{m}}(\boldsymbol{\lambda}^{(2)})=\boldsymbol{1}$
and $\boldsymbol{\hat{m}}(\boldsymbol{\lambda^{(1)}})=\boldsymbol{0}$.

\textbf{1. We calculate $\nabla_{\lambda}\boldsymbol{\hat{m}_{k}(\lambda)}$
using the implicit differentiation trick.}

By the KKT conditions, we have for all $j=1:J$ 
\begin{equation}
\left.\frac{\partial}{\partial m_{j}}\left(\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j})\right\Vert _{T}^{2}+\lambda_{j}P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j})\right)+\lambda_{j}w\langle\boldsymbol{\beta}_{j},\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j}\rangle\right|_{\boldsymbol{m}=\boldsymbol{\hat{m}}(\boldsymbol{\lambda})}=0
\end{equation}


Now we implicitly differentiate with respect to $\lambda_{\ell}$
for $\ell=1,2,...,J$

\[
\frac{\partial}{\partial\lambda_{\ell}}\left\{ \left.\left[\frac{\partial}{\partial m_{j}}\left(\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j})\right\Vert _{T}^{2}+\lambda_{j}P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j})\right)+\lambda_{j}w\langle\boldsymbol{\beta}_{j},\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j}\rangle\right]\right|_{\boldsymbol{m}=\hat{\boldsymbol{m}}(\boldsymbol{\lambda})}\right\} =0
\]


By the product rule and chain rule, we have 

\begin{eqnarray*}
\left.\left\{ \sum_{k=1}^{J}\left[\frac{\partial^{2}}{\partial m_{k}\partial m_{j}}\left(\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j})\right\Vert _{T}^{2}+1[k=j]\lambda_{j}P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j})\right)+1[k=j]\lambda_{j}w\|\boldsymbol{\beta}_{j}\|_{2}^{2}\right]\frac{\partial}{\partial\lambda_{\ell}}\hat{m}_{k}(\boldsymbol{\lambda})\right\} \right|_{\boldsymbol{m}=\hat{\boldsymbol{m}}(\boldsymbol{\lambda})}\\
+1[j=\ell]\left.\left\{ \frac{\partial}{\partial m}_{\ell}P_{\ell}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},\ell}+m_{\ell}\boldsymbol{\beta}_{\ell})+w\langle\boldsymbol{\beta}_{\ell},\hat{\boldsymbol{\theta}}_{\lambda^{(1)},\ell}+m_{\ell}\boldsymbol{\beta}_{\ell}\rangle\right\} \right|_{\boldsymbol{m}=\hat{\boldsymbol{m}}(\boldsymbol{\lambda})} & = & 0
\end{eqnarray*}


Define the following matrices 
\begin{eqnarray*}
S:S_{jk} & = & \left.\frac{\partial^{2}}{\partial m_{k}\partial m_{j}}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j})\right\Vert _{T}^{2}\right|_{\boldsymbol{m}=\hat{\boldsymbol{m}}(\boldsymbol{\lambda})}
\end{eqnarray*}


\[
D_{1}=\left.diag\left(\frac{\partial^{2}}{\partial m_{j}^{2}}\lambda_{j}P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j})\right)\right|_{\boldsymbol{m}=\hat{\boldsymbol{m}}(\boldsymbol{\lambda})}
\]


\[
D_{2}=diag\left(\lambda_{j}w\|\boldsymbol{\beta}_{j}\|_{2}^{2}\right)
\]


\[
D_{3}=\left.diag\left(\frac{\partial}{\partial m}_{\ell}P_{\ell}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},\ell}+m_{\ell}\boldsymbol{\beta}_{\ell})+w\langle\boldsymbol{\beta}_{\ell},\hat{\boldsymbol{\theta}}_{\lambda^{(1)},\ell}+m_{\ell}\boldsymbol{\beta}_{\ell}\rangle\right)\right|_{\boldsymbol{m}=\hat{\boldsymbol{m}}(\boldsymbol{\lambda})}
\]


\[
M=\left(\begin{array}{cccc}
\nabla_{\lambda}\hat{m}_{1}(\lambda) & \nabla_{\lambda}\hat{m}_{2}(\lambda) & ... & \nabla_{\lambda}\hat{m}_{J}(\lambda)\end{array}\right)
\]


We can then combine all the equations into the following system of
equations:
\[
M=-D_{3}\left(S+D_{1}+D_{2}\right)^{-1}
\]


$S$ is a PSD matrix since the composition of a convex function with
an affine function is convex.

$D_{1}$ is a PSD matrix since the penalty functions are convex.

\textbf{2. We bound every diagonal element in $D_{3}$}:

By Cauchy-Schwarz, 
\[
\left|w\langle\boldsymbol{\beta}_{k},\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+\hat{m}_{k}(\boldsymbol{\lambda})\boldsymbol{\beta}_{k}\rangle\right|\le w\|\boldsymbol{\beta}_{k}\|\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+\hat{m}_{k}(\boldsymbol{\lambda})\boldsymbol{\beta}_{k}\|
\]


To bound $\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+\hat{m}_{k}(\boldsymbol{\lambda})\boldsymbol{\beta}_{k}\|$,
we use the definition of $\hat{m}_{k}(\boldsymbol{\lambda})$:
\begin{eqnarray*}
 &  & \left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+\hat{m}_{j}(\boldsymbol{\lambda})\boldsymbol{\beta}_{j})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}\left(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+\hat{m}_{k}(\boldsymbol{\lambda})\boldsymbol{\beta}_{k}\right)+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+\hat{m}_{k}(\boldsymbol{\lambda})\boldsymbol{\beta}_{k}\|^{2}\right)\\
 & \le & \frac{1}{2}\|y-\sum_{j=1}^{J}g(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}\|_{2}^{2}\right)\\
 & = & \frac{1}{2}\|y-\sum_{j=1}^{J}g(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}\|_{2}^{2}\right)+\sum_{j=1}^{J}(\lambda_{j}-\lambda_{j}^{(1)})\left(P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}\|_{2}^{2}\right)\\
 & \le & C+J\lambda_{max}\max_{j=1:J}\left(P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}\|_{2}^{2}\right)
\end{eqnarray*}


To bound the term $\max_{j=1:J}\left(P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}\|_{2}^{2}\right)$,
we use the basic inequality for $\hat{\boldsymbol{\theta}}_{\lambda^{(1)}}$:
\begin{eqnarray*}
\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}\|_{2}^{2}\right) & \le & \frac{1}{2}\|y-\sum_{j=1}^{J}g(\cdot|\hat{\boldsymbol{\theta}}_{j}^{*})\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}(\hat{\boldsymbol{\theta}}_{j}^{*})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{j}^{*}\|_{2}^{2}\right)\\
 & \le & C
\end{eqnarray*}


Since 
\[
\lambda_{min}\left(\max_{j=1:J}P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}\|_{2}^{2}\right)\le\sum_{j=1}^{J}\lambda_{j}^{(1)}\left(P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}\|_{2}^{2}\right)
\]


then we have that
\[
\max_{j=1:J}P_{j}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j})+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}\|_{2}^{2}\le\frac{C}{\lambda_{min}}
\]


Therefore
\[
\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+\hat{m}_{j}(\boldsymbol{\lambda})\boldsymbol{\beta}_{j})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}\left(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+\hat{m}_{k}(\boldsymbol{\lambda})\boldsymbol{\beta}_{k}\right)+\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+\hat{m}_{k}(\boldsymbol{\lambda})\boldsymbol{\beta}_{k}\|^{2}\right)\le\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)C
\]


This implies that
\begin{equation}
\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+m_{\ell}\boldsymbol{\beta}_{k}\|\le\sqrt{\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\frac{2C}{\lambda_{min}w}}
\end{equation}


and
\begin{equation}
\left\Vert y-\sum_{j=1}^{J}g_{j}(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+\hat{m}_{j}(\boldsymbol{\lambda})\boldsymbol{\beta}_{j})\right\Vert _{T}\le\sqrt{\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)2C}
\end{equation}


If combine the assumption (5) with (8), we get 
\[
\left|\frac{\partial}{\partial m}_{k}P_{k}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+m_{k}\boldsymbol{\beta}_{k})+w\langle\boldsymbol{\beta}_{k},\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+m_{k}\boldsymbol{\beta}_{k}\rangle\right|_{\boldsymbol{m}=\hat{\boldsymbol{m}}(\boldsymbol{\lambda})}\le\|\boldsymbol{\beta}_{k}\|\left(K_{k}+w\sqrt{\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\frac{2C}{\lambda_{min}w}}\right)
\]


On the other hand, suppose the other assumption (6) is satisfied.
Then we will need to use the implicit differentiation equation (7).
Rearranging, we get

\begin{eqnarray*}
\left.\frac{\partial}{\partial m_{k}}\left(P_{k}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+m_{k}\boldsymbol{\beta}_{k})\right)\right|_{\boldsymbol{m}=\boldsymbol{\hat{m}}(\boldsymbol{\lambda})}+w\langle\boldsymbol{\beta}_{k},\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+m_{k}\boldsymbol{\beta}_{k}\rangle & = & \frac{1}{\lambda_{k}}\left\langle \left.\frac{\partial}{\partial m}g_{k}\left(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+m_{k}\boldsymbol{\beta}_{k}\right)\right|_{m=\hat{m}(\lambda)},y-\sum_{j=1}^{J}g_{j}(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+m_{j}\boldsymbol{\beta}_{j})\right\rangle _{T}\\
 & \le & \frac{1}{\lambda_{min}}K_{k}\|\boldsymbol{\beta}_{k}\|\left\Vert y-\sum_{j=1}^{J}g_{j}\left(\cdot|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}+\hat{m}_{j}(\boldsymbol{\lambda})\boldsymbol{\beta}_{j}\right)\right\Vert _{T}
\end{eqnarray*}


Plugging in (9), we get 
\[
\left|\frac{\partial}{\partial m}_{k}P_{k}(\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+m_{k}\boldsymbol{\beta}_{k})+w\langle\boldsymbol{\beta}_{k},\hat{\boldsymbol{\theta}}_{\lambda^{(1)},k}+m_{k}\boldsymbol{\beta}_{k}\rangle\right|_{\boldsymbol{m}=\hat{\boldsymbol{m}}(\boldsymbol{\lambda})}\le\|\boldsymbol{\beta}_{k}\|\frac{1}{\lambda_{min}}K_{k}\sqrt{\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)2C}
\]


Using these upper bounds, we can bound $D_{3}$ by the diagonal matrix
\[
\left\{ \max_{k=1,...,J}d_{k}\right\} diag\left(\left\{ \|\beta_{k}\|\right\} _{k=1}^{J}\right)\succeq D_{3}
\]
 

where for $k=1,...,J$
\[
d_{k}=\begin{cases}
\left(K_{k}+w\sqrt{\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\frac{2C}{\lambda_{min}w}}\right) & \mbox{if assumption (5) holds for }k\\
\frac{1}{\lambda_{min}}K_{k}\sqrt{\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)2C} & \mbox{if assumption (6) holds for }k
\end{cases}
\]


\textbf{3. We bound the norm of $\nabla_{\lambda}\hat{m}_{k}(\lambda)$
for all $k=1,...,J$.}

For every $k=1,...,J$, we have
\begin{eqnarray}
\|\nabla_{\lambda}\hat{m}_{k}(\lambda)\| & = & \|Me_{k}\|\nonumber \\
 & = & \|D_{3}\left(S+D_{1}+D_{2}\right)^{-1}e_{k}\|\nonumber \\
 & \le & \left\{ \max_{k=1,...,J}d_{k}\right\} \left\Vert diag\left(\left\{ \|\beta\|_{k}\right\} _{k=1}^{J}\right)\left(S+D_{1}+D_{2}\right)^{-1}e_{k}\right\Vert \nonumber \\
 & \le & \left\{ \max_{k=1,...,J}d_{k}\right\} \max_{\ell}\|\boldsymbol{\beta}_{\ell}\|\left\Vert \left(S+D_{1}+D_{2}\right)^{-1}e_{k}\right\Vert \nonumber \\
 & \le & \left\{ \max_{k=1,...,J}d_{k}\right\} \max_{\ell}\|\boldsymbol{\beta}_{\ell}\|\left\Vert D_{2}^{-1}e_{k}\right\Vert 
\end{eqnarray}


The last line follows from the matrix inverse lemma: Since $S+D_{1}$
is a PSD matrix, then 
\[
\left\Vert \left(S+D_{1}+D_{2}\right)^{-1}e_{k}\right\Vert \le\left\Vert D_{2}^{-1}e_{k}\right\Vert 
\]


Now consider (8) for 
\[
k\coloneqq\ell_{max}=\arg\max_{\ell}\|\boldsymbol{\beta}_{\ell}\|
\]


(Notice we can choose any $k$ in $1,...,J$. The inequality holds
for all k so we just choose the $k$ that is most interesting for
our problem.)

We have 
\begin{eqnarray*}
\|\nabla_{\lambda}\hat{m}_{\ell_{max}}(\lambda)\| & \le & \left\{ \max_{k=1,...,J}d_{k}\right\} \|\boldsymbol{\beta}_{\ell_{max}}\|\left\Vert D_{2}^{-1}e_{\ell_{max}}\right\Vert \\
 & = & \left\{ \max_{k=1,...,J}d_{k}\right\} \|\boldsymbol{\beta}_{\ell_{max}}\|\lambda_{\ell_{max}}^{-1}w^{-1}\|\boldsymbol{\beta}_{\ell_{max}}\|_{2}^{-2}\\
 & \le & \left\{ \max_{k=1,...,J}d_{k}\right\} \|\boldsymbol{\beta}_{\ell_{max}}\|^{-1}\lambda_{min}^{-1}w^{-1}
\end{eqnarray*}


\textbf{4. Apply the Mean Value Theorem}

Since the training criterion is smooth, then $\hat{m}_{\ell_{max}}(\lambda)$
is a continuous, differentiable function.

By the MVT, we have that there exists an $\alpha\in(0,1)$ such that
\begin{eqnarray*}
\left|\hat{m}_{\ell_{max}}(\boldsymbol{\lambda}^{(2)})-\hat{m}_{\ell_{max}}(\boldsymbol{\lambda}^{(1)})\right| & = & \left|\left\langle \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)},\nabla_{\lambda}\hat{m}_{\ell_{max}}(\boldsymbol{\lambda})\right\rangle _{\boldsymbol{\lambda}=\alpha\boldsymbol{\lambda}^{(1)}+(1-\alpha)\boldsymbol{\lambda}^{(2)}}\right|\\
 & \le & \left\Vert \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\right\Vert \left\{ \max_{k=1,...,J}d_{k}\right\} \lambda_{min}^{-1}w^{-1}\|\boldsymbol{\beta}_{\ell_{max}}\|^{-1}
\end{eqnarray*}


We know that $\hat{m}_{k}(\boldsymbol{\lambda}^{(2)})-\hat{m}_{k}(\boldsymbol{\lambda}^{(1)})=\boldsymbol{1}$
for all $k=1,..,J$. Rearranging the inequality above, we get
\[
\max_{k}\|\boldsymbol{\theta}_{\lambda^{(1)},k}-\boldsymbol{\theta}_{\lambda^{(2)},k}\|=\|\boldsymbol{\beta}_{\ell_{max}}\|\le\left\Vert \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\right\Vert \left\{ \max_{k=1,...,J}d_{k}\right\} \lambda_{min}^{-1}w^{-1}
\]



\section{Nonsmooth Penalties}

Suppose we are dealing with parametric regression problems from Section
1 or 2. We keep all the same assumptions, except those that concern
the smoothness of the penalties. 

Recall that $\Lambda\subseteq\mathbb{R}^{J}$. Consider the measure
space over $\Lambda$ with respect to the Lebesgue measure $\mu$.
We suppose that for a given dataset $\left(X,y\right)$, suppose the
following three assumptions hold:

\textbf{Assumption (1):} Let the penalized training criterion be denoted
$L_{T}(\boldsymbol{\theta},\boldsymbol{\lambda})$. Denote the differentiable
space of $L_{T}(\cdot,\boldsymbol{\lambda})$ at any point $\boldsymbol{\theta}$
as 
\[
\Omega^{L_{T}(\cdot,\boldsymbol{\lambda})}\left(\boldsymbol{\theta}\right)=\left\{ \boldsymbol{\eta}|\lim_{\epsilon\rightarrow0}\frac{L_{T}(\boldsymbol{\theta}+\epsilon\boldsymbol{\eta})-L_{T}(\boldsymbol{\theta})}{\epsilon}\mbox{ exists}\right\} 
\]


Suppose there is a set $\Lambda_{smooth}\subseteq\Lambda$ such that
$\mu\left(\Lambda_{smooth}^{C}\right)=0$ and for every $\boldsymbol{\lambda}\in\Lambda_{smooth}$,
there exists a ball with nonzero radius centered at $\boldsymbol{\lambda}$,
denoted $B(\boldsymbol{\lambda})$, such that the following conditions
hold:

\textbf{Cond 1:} For all $\boldsymbol{\lambda}'\in B(\boldsymbol{\lambda})$,
the training criterion $L_{T}(\cdot,\cdot)$ is twice differentiable
along directions in $\Omega^{L_{T}(\cdot,\cdot)}\left(\hat{\boldsymbol{\theta}}_{\lambda}\right)$.
(So technically the twice-differentiable space is constant)

\textbf{Cond 2:} $\Omega^{L_{T}(\cdot,\boldsymbol{\lambda})}\left(\hat{\boldsymbol{\theta}}_{\lambda}\right)$
is a local optimality space of $B(\boldsymbol{\lambda})$:
\[
\arg\min_{\theta\in\Theta}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}'\right)=\arg\min_{\theta\in\Omega^{L_{T}(\cdot,\boldsymbol{\lambda})}\left(\hat{\boldsymbol{\theta}}_{\lambda}\right)}L_{T}\left(\boldsymbol{\theta},\boldsymbol{\lambda}'\right)\mbox{ }\forall\boldsymbol{\lambda}'\in B(\boldsymbol{\lambda})
\]


\textbf{Cond 3: }(Not necessary if we keep the ridge penalty) There
is an orthonormal basis $U_{\lambda}$ of $\Omega^{L_{T}(\cdot,\boldsymbol{\lambda})}\left(\hat{\boldsymbol{\theta}}_{\lambda}\right)$
such that the Hessian of the training criterion taken along directions
$U_{\lambda}$ is invertible.

\textbf{Assumption (2):} For every $\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\in\Lambda_{smooth}$,
let the line segment between the two points be denoted 
\[
\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})=\left\{ \alpha\boldsymbol{\lambda^{(1)}}+(1-\alpha)\boldsymbol{\lambda^{(2)}}:\alpha\in[0,1]\right\} 
\]


Suppose the intersection $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\cap\Lambda_{smooth}^{C}$
is countable.

\textbf{Assumption} \textbf{(3):} All the conditions specified in
Section 1 and 2 that bound the spectrum of $P_{j}$ or $g_{j}$ only
need to apply when the directional derivatives exist. That is, the
condition on the spectrum of the penalty derivative is now

\[
\left|\frac{\partial}{\partial m}P_{j}\left(\boldsymbol{\theta}+m\boldsymbol{\beta}\right)\right|\le K\|\boldsymbol{\beta}\|_{2}\mbox{ if }\frac{\partial}{\partial m}P_{j}\left(\boldsymbol{\theta}+m\boldsymbol{\beta}\right)\mbox{exists}
\]


Similarly, we would change the condition on the spectrum of the function
derivative to
\[
\left|\frac{\partial}{\partial m}g_{j}\left(\boldsymbol{\theta}+m\boldsymbol{\beta}\right)\right|\le K\|\boldsymbol{\beta}\|_{2}\mbox{ if }\frac{\partial}{\partial m}g_{j}\left(\boldsymbol{\theta}+m\boldsymbol{\beta}\right)\mbox{exists}
\]


Under these assumptions, the same Lipschitz conditions hold for dataset
$\left(X,y\right)$ and every $\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\in\Lambda_{smooth}$.


\subsubsection*{Proof}

Consider any $\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\in\Lambda_{smooth}$.
The length of $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
covered by set $A$ can be expressed as 
\[
\mu_{1}\left(A\cap\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\right)
\]


where $\mu_{1}$ is the Lebesgue measure over the line segment $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$.
(So if $A\cap\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
is just a line segment, it is the length $\|A\cap\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\|_{2}$)

By the Differentiability Cover Lemma below, there exists a countable
set of points $\cup_{i=1}^{\infty}\boldsymbol{\ell}^{(i)}\subset\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
such that the union of their ``balls of differentiabilities'' entirely
cover $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$:
\[
\max_{\{\boldsymbol{\ell}^{(i)}\}_{i=1}^{\infty}}\mu_{1}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}^{(i)})\cap\mathcal{L}\left(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\right)\right)=\left\Vert \mathcal{L}\left(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\right)\right\Vert _{2}
\]


Let 
\[
\left\{ \boldsymbol{\ell}_{max}^{(i)}\right\} _{i=1}^{\infty}=\left\{ \arg\max_{\left\{ \boldsymbol{\ell}^{(i)}\right\} }\mu_{1}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}^{(i)})\cap\mathcal{L}\left(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\right)\right)\right\} \cup\left\{ \boldsymbol{\lambda}^{(1)},\boldsymbol{\lambda}^{(2)}\right\} 
\]


Let $P$ be the intersections of the boundary of $B\left(\boldsymbol{\ell}_{max}^{(i)}\right)$
with the line segment $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$:
\[
P=\cup_{i=1}^{\infty}\mbox{Bd}B\left(\boldsymbol{\ell}_{max}^{(i)}\right)\cap\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})
\]


Every point $p\in P$ can be expressed as $\alpha_{p}\boldsymbol{\lambda^{(1)}}+(1-\alpha_{p})\boldsymbol{\lambda^{(2)}}$
for some $\alpha_{p}\in[0,1]$. This means we can order these points
$\{\boldsymbol{p}^{(i)}\}_{i=1}^{\infty}$ by increasing $\alpha_{p}$.
By our assumptions, the differentiable space of the training criterion
must be constant over the interior of line segment $\mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)$
(so there might be bad behavior at the endpoints). Let the differentiable
space over the interior of line segment $\mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)$
be denoted $\Omega_{i}$. 

By our assumptions, the differentiable space is also a local optimality
space. Let $U^{(i)}$ be an orthonormal basis of $\Omega_{i}$. For
each $i$, we can express $\hat{\boldsymbol{\theta}}_{\lambda}$ for
all $\boldsymbol{\lambda}\in\mbox{Int}\left\{ \mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)\right\} $
as
\[
\hat{\boldsymbol{\theta}}_{\lambda}=U^{(i)}\hat{\boldsymbol{\beta}}_{\lambda}
\]
\[
\hat{\boldsymbol{\beta}}_{\lambda}=\arg\min_{\beta}L_{T}(U^{(i)}\boldsymbol{\beta},\boldsymbol{\lambda})
\]


Now apply the result in Section 1 or 2 over every line segment $\mathcal{L}\left(\boldsymbol{p^{(i)},p^{(i+1)}}\right)$.
To do this, we must modify the proofs to take directional derivatives
along the columns of $U^{(i)}$. We can establish that there is a
constant $c>0$ independent of $i$ such that for all $i=1,2...$,
we have 
\[
\left\Vert \boldsymbol{\hat{\beta}}_{p^{(i)}}-\boldsymbol{\hat{\beta}}_{p^{(i+1)}}\right\Vert _{2}\le c\|\boldsymbol{p^{(i)}}-\boldsymbol{p^{(i+1)}}\|_{2}
\]


Finally, we can sum these inequalities. By the triangle inequality,
\begin{eqnarray*}
\left\Vert \boldsymbol{\hat{\theta}_{\lambda^{(1)}}}-\boldsymbol{\hat{\theta}_{\lambda^{(2)}}}\right\Vert _{2} & \le & \sum_{i=1}^{\infty}\|\boldsymbol{\hat{\theta}}_{p^{(i)}}-\boldsymbol{\hat{\theta}}_{p^{(i+1)}}\|_{2}\\
 & = & \sum_{i=1}^{\infty}\|U^{(i)}\boldsymbol{\hat{\beta}}_{p^{(i)}}-U^{(i)}\boldsymbol{\hat{\beta}}_{p^{(i+1)}}\|_{2}\\
 & = & \sum_{i=1}^{\infty}\|\boldsymbol{\hat{\beta}}_{p^{(i)}}-\boldsymbol{\hat{\beta}}_{p^{(i+1)}}\|_{2}\\
 & \le & \sum_{i=1}^{\infty}c\|\boldsymbol{p^{(i)}}-\boldsymbol{p^{(i+1)}}\|_{2}\\
 & = & c\|\boldsymbol{\lambda}^{(1)}-\boldsymbol{\lambda}^{(2)}\|_{2}
\end{eqnarray*}



\subsubsection*{Lemma - Differentiability Cover }

For any $\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}\in\Lambda_{smooth}$,
there exists a countable set of points $\cup_{i=1}^{\infty}\boldsymbol{\ell}^{(i)}\subset\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
such that the union of their ``balls of differentiabilities'' entirely
cover $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
\[
\max_{\{\boldsymbol{\ell}^{(i)}\}_{i=1}^{\infty}}d_{\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}^{(i)})\right)=\left\Vert \mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\right\Vert 
\]



\subsubsection*{Proof}

We prove this by contradiction. Let 
\[
\left\{ \boldsymbol{\ell}_{max}^{(i)}\right\} _{i=1}^{\infty}=\arg\max_{\{\boldsymbol{\ell}^{(i)}\}_{i=1}^{\infty}}d_{\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}^{(i)})\right)
\]


and for contradiction, suppose that the covered length is less than
the length of the line segment:
\[
d_{\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}_{max}^{(i)})\right)<\left\Vert \mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\right\Vert 
\]


By assumption (2), since $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\cap\Lambda_{smooth}^{C}$
is countable, there must exist a point $p\in\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})\backslash\left\{ \cup_{i=1}^{\infty}B(\boldsymbol{\ell}_{max}^{(i)})\right\} $
such that $p\notin\Lambda_{smooth}^{C}$. However if we consider the
set of points $\left\{ \boldsymbol{\ell}_{max}^{(i)}\right\} _{i=1}^{\infty}\cup\{p\}$,
then 
\[
d_{\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}_{max}^{(i)})\right)<d_{\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}}}\left(\cup_{i=1}^{\infty}B(\boldsymbol{\ell}_{max}^{(i)})\cup B(p)\right)
\]


This is a contradiction of the definition of $\{\boldsymbol{\ell}_{max}^{(i)}\}$.
Therefore we should always be able to cover $\mathcal{L}(\boldsymbol{\lambda^{(1)}},\boldsymbol{\lambda^{(2)}})$
with ``balls of differentiability.''


\section{Example}


\subsection{Penalties that satisfy the conditions}

We will show penalties that satisfy the condition
\[
\frac{\partial}{\partial m}P(\boldsymbol{\theta}+m\boldsymbol{\beta})\le K\|\boldsymbol{\beta}\|_{2}
\]


for some constant $K>0$.

\textbf{Ridge:}

The perturbation isn't necessary if there is already a ridge penalty
in the original penalized regression problem. Just set the penalties
$P_{j}(\boldsymbol{\theta})\equiv0$ and fix $w=2$.

\textbf{Lasso:}

\begin{eqnarray*}
\frac{\partial}{\partial m}\|\theta+m\beta\|_{1} & = & \langle sgn(\theta+m\beta),\beta\rangle\\
 & \le & \|sgn(\theta+m\beta)\|_{2}\|\beta\|_{2}\\
 & \le & p\|\beta\|_{2}
\end{eqnarray*}


so $K=p$ in this case.

\textbf{Generalized Lasso:} let $G$ be the maximum eigenvalue of
$D$.
\begin{eqnarray*}
\frac{\partial}{\partial m}\|D(\theta+m\beta)\|_{1} & = & \langle sgn(D(\theta+m\beta)),D\beta\rangle\\
 & \le & \|sgn(D(\theta+m\beta))\|_{2}\|D\beta\|_{2}\\
 & \le & pG\|\beta\|_{2}
\end{eqnarray*}


so $K=pG$ in this case.

\textbf{Group Lasso:}

If we have un-pooled penalty parameters as follows 
\[
\sum_{j=1}^{J}\lambda_{j}\|\boldsymbol{\theta}^{(j)}+m^{(j)}\boldsymbol{\beta}^{(j)}\|_{2}
\]


then we need the following bound for every $j=1,...,J$

\begin{eqnarray*}
\frac{\partial}{\partial m^{(j)}}\|\boldsymbol{\theta}^{(j)}+m^{(j)}\boldsymbol{\beta}^{(j)}\|_{2} & = & \left\langle \frac{\boldsymbol{\theta}^{(j)}+m^{(j)}\boldsymbol{\beta}^{(j)}}{\|\boldsymbol{\theta}^{(j)}+m^{(j)}\boldsymbol{\beta}^{(j)}\|_{2}},\boldsymbol{\beta}^{(j)}\right\rangle \\
 & \le & \|\boldsymbol{\beta}^{(j)}\|_{2}
\end{eqnarray*}


So $K=1$ in this case. 

If there is a single penalty parameter for the entire group laso penalty
as follows 
\[
\lambda\sum_{j=1}^{J}\|\boldsymbol{\theta}^{(j)}+m\boldsymbol{\beta}^{(j)}\|_{2}
\]


then 
\begin{eqnarray*}
\frac{\partial}{\partial m}\sum_{j=1}^{J}\|\boldsymbol{\theta}^{(j)}+m\boldsymbol{\beta}^{(j)}\|_{2} & = & \sum_{j=1}^{J}\left\langle \frac{\boldsymbol{\theta}^{(j)}+m\boldsymbol{\beta}^{(j)}}{\|\boldsymbol{\theta}^{(j)}+m\boldsymbol{\beta}^{(j)}\|_{2}},\boldsymbol{\beta}^{(j)}\right\rangle \\
 & \le & \sum_{j=1}^{J}\|\boldsymbol{\beta}^{(j)}\|_{2}\\
 & \le & \sqrt{J}\|\boldsymbol{\beta}\|_{2}
\end{eqnarray*}


and $K=\sqrt{J}$.


\subsection{Sobolev}

Given a function $h$, the Sobolev penalty for $h$ is

\[
P(h)=\int(h^{(r)}(x))^{2}dx
\]


The Sobolev penalty is used in nonparametric regression models, but
such nonparametric regression models can be re-expressed in parametric
form. We will use this to understand the smoothness of models fitted
in this manner.

Consider the class of smoothing splines 
\[
\left\{ \hat{g}(\cdot|\lambda)=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}g_{j}(x_{j})\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P(g_{j}):\lambda\in\Lambda\right\} 
\]


Each function $\hat{g}_{j}(\cdot|\lambda)$ is a spline that can be
expressed as the weighted sum of $B$ normalized B-splines of degree
$r+1$ for a given set of knots:
\[
\hat{g}_{j}(x|\lambda)=\sum_{i=1}^{B}\theta_{i}N_{j,i}(x)
\]


Note that the normalized B-splines have the property that they sum
up to one at all points within the boundary of the knots. Also recall
that B-splines are non-negative.

Therefore we can re-express the class of smoothing splines as a set
of function parameters
\[
\left\{ \hat{\boldsymbol{\theta}}_{\lambda}=\arg\min_{\theta}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}N_{T,j}\boldsymbol{\theta}_{j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\theta}_{j}):\lambda\in\Lambda\right\} 
\]
where $N_{T,j}$ is a matrix of the evaluations of the normalized
B-spline basis at $x_{j}$. $P_{j}(\boldsymbol{\theta_{j}})$ is the
Sobolev penalty and can be written as $\boldsymbol{\theta}_{j}^{T}V_{j}\boldsymbol{\theta}_{j}$
for an appropriate penalty matrix $V_{j}$. We will not need to express
anything in terms of $V_{j}$ so the penalty will be just written
as $P_{j}(\boldsymbol{\theta}_{j})$.

Instead of considering the original smoothing spline problem with
the roughness penalty, we will add a ridge penalty on the function
parameters
\[
\left\{ \hat{\boldsymbol{\theta}}_{\lambda}=\arg\min_{\theta}\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}N_{T,j}\boldsymbol{\theta}_{j}\right\Vert _{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(\boldsymbol{\theta}_{j})+\frac{w}{2}\|\boldsymbol{\theta}_{j}\|_{2}^{2}\right):\lambda\in\Lambda\right\} 
\]


Let 
\[
C=\frac{1}{2}\left\Vert y-\sum_{j=1}^{J}N_{T,j}\boldsymbol{\theta}_{j}^{*}\right\Vert _{T}^{2}+\lambda_{max}\sum_{j=1}^{J}\left(P_{j}(\boldsymbol{\theta}_{j}^{*})+\frac{w}{2}\|\boldsymbol{\theta}_{j}^{*}\|_{2}^{2}\right)
\]


Then for any $\boldsymbol{\lambda^{(1)},\lambda^{(2)}}\in\Lambda$
we have for all $j=1,...,J$
\[
\|\boldsymbol{\theta}_{\lambda^{(1)},j}-\boldsymbol{\theta}_{\lambda^{(2)},j}\|_{2}\le\left\Vert \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\right\Vert _{2}\lambda_{min}^{-1}w^{-1}\left(\frac{1}{\lambda_{min}}B\sqrt{\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\frac{2C}{\lambda_{min}w}}\right)
\]


Moreover,

\[
\left\Vert \sum_{j=1}^{J}\hat{g}_{j}(x_{j}|\boldsymbol{\lambda}^{(1)})-\hat{g}_{j}(x_{j}|\boldsymbol{\lambda}^{(2)})\right\Vert _{\infty}\le\left\Vert \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\right\Vert _{2}J\sqrt{B}\lambda_{min}^{-1}w^{-1}\left(\frac{1}{\lambda_{min}}B\sqrt{\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\frac{2C}{\lambda_{min}w}}\right)
\]



\subsubsection*{Proof}

To apply the result from Section 2, we just need to bound the spectral
norm 
\[
\left\Vert \nabla_{\theta}g_{j}(X_{T,j}|\boldsymbol{\theta})\right\Vert =\|N_{T,j}\|
\]
Note that the eigenvalue of $N_{T,j}$ is bounded by $B$ since the
maximum eigenvalue of a non-negative matrix is bounded by its maximum
row sum. In the case of $N_{T,j}$, since it is the values of normalized
B-splines, each row is at most the number of B-spline basis functions.
That is, we have for all $j=1,...,J$
\[
\left\Vert \nabla_{\theta}g_{j}(X_{T,j}|\boldsymbol{\theta})\right\Vert =\|N_{T,j}\|\le B
\]


Hence for all $\boldsymbol{\theta,\beta},m'$, we have
\[
\left\Vert \left.\frac{\partial}{\partial m}g_{j}(X_{T,j}|\boldsymbol{\theta}+m\boldsymbol{\beta})\right|_{m=m'}\right\Vert \le B\|\boldsymbol{\beta}\|
\]


Apply the result from Section 2 to get the result
\[
\left\Vert \boldsymbol{\theta}_{\lambda^{(1)},j}-\boldsymbol{\theta}_{\lambda^{(2)},j}\right\Vert _{2}\le\left\Vert \boldsymbol{\lambda}^{(2)}-\boldsymbol{\lambda}^{(1)}\right\Vert _{2}\lambda_{min}^{-1}w^{-1}\left(\frac{1}{\lambda_{min}}B\sqrt{\left(1+\frac{J\lambda_{max}}{\lambda_{min}}\right)\frac{2C}{\lambda_{min}w}}\right)
\]


The ``moreover'' statement follows from the fact that for any point
$\boldsymbol{x}$, we have 
\begin{eqnarray*}
\left|\sum_{j=1}^{J}\hat{g}_{j}(x_{j}|\boldsymbol{\lambda}^{(1)})-\hat{g}_{j}(x_{j}|\boldsymbol{\lambda}^{(2)})\right| & = & \left|\sum_{j=1}^{J}\sum_{i=1}^{B}\left(\hat{\theta}_{\lambda^{(1)},j,i}-\hat{\theta}_{\lambda^{(2)},j,i}\right)N_{j,i}(x_{j})\right|\\
 & \le & \sum_{j=1}^{J}\sum_{i=1}^{B}\left|\left(\hat{\theta}_{\lambda^{(1)},j,i}-\hat{\theta}_{\lambda^{(2)},j,i}\right)N_{j,i}(x_{j})\right|\\
 & \le & \sum_{j=1}^{J}\sum_{i=1}^{B}\left|\hat{\theta}_{\lambda^{(1)},j,i}-\hat{\theta}_{\lambda^{(2)},j,i}\right|\\
 & \le & \sum_{j=1}^{J}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}-\hat{\boldsymbol{\theta}}_{\lambda^{(2)},j}\|_{1}\\
 & \le & \sqrt{B}\sum_{j=1}^{J}\|\hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}-\hat{\boldsymbol{\theta}}_{\lambda^{(2)},j}\|_{2}
\end{eqnarray*}


where the second inequality uses the fact that normalized B-splines
have value at most 1. Therefore

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{j}(x_{j}|\lambda^{(1)})-\hat{g}_{j}(x_{j}|\lambda^{(2)})\right\Vert _{\infty} & \le & \sqrt{B}\sum_{j=1}^{J}\left\Vert \hat{\boldsymbol{\theta}}_{\lambda^{(1)},j}-\hat{\boldsymbol{\theta}}_{\lambda^{(2)},j}\right\Vert _{2}
\end{eqnarray*}

\end{document}
