%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsmath}

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{babel}

\makeatother

\usepackage{babel}
\begin{document}

\subsection*{Lemma: Additive Models and Additive Penalties}

Consider the problem 
\[
\frac{1}{2}\|y-\sum_{j=1}^{J}g_{j}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(g_{j})+\frac{w}{2}\|g_{j}\|_{D}^{2}\right)
\]


We suppose the penalty functions $P_{j}$ are convex and twice-differentiable.
(We do not need the semi-norm assumption.)

Suppose that $\sup_{g\in\mathcal{G}}\|g\|_{D}\le G$.

For all $d>0$, any $\lambda^{(1)},\lambda^{(2)}$ that satisfy 
\[
\|\lambda^{(1)}-\lambda^{(2)}\|\le\frac{dw}{2J}\left(\sqrt{\frac{n}{n_{T}}}n^{\tau_{min}}\left(2G+\|\epsilon\|_{T}\right)+2wG\right)^{-1}n^{-\tau_{\min}}
\]


we have 
\[
\|\hat{g}_{j}(\cdot|\lambda^{(1)})-\hat{g}_{j}(\cdot|\lambda^{(2)})\|_{D}\le d/J
\]


Hence 
\[
\|\sum_{j=1}^{J}\hat{g}_{j}(\cdot|\lambda^{(1)})-\hat{g}_{j}(\cdot|\lambda^{(2)})\|_{D}\le d
\]



\subsubsection*{Proof}

Let $h_{j}=\hat{g}_{j}(\cdot|\lambda^{(1)})-\hat{g}_{j}(\cdot|\lambda^{(2)})$.
Suppose for contradiction that for $\tilde{k}$, we have $\|h_{\tilde{k}}\|_{D}>d/J$.

Let 
\[
Z=\left\{ j:\|h_{j}\|>0\right\} 
\]
Consider the optimization problem

\[
\left\{ \hat{m}_{j}(\lambda)\right\} _{j\in Z}=\arg\min_{m}\frac{1}{2}\|y-\sum_{j=1}^{J}\left(g_{j}+m_{j}h_{j}\right)\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}(g_{j}+m_{j}h_{j})+\frac{w}{2}\|g_{j}+m_{j}h_{j}\|_{D}^{2}\right)
\]


Note that if $\|h_{j}\|=0$, then we just set $m_{j}=0$ as a constant.

Now by the KKT conditions, for all $\ell\in Z$, we have

\[
\langle y-\sum_{j=1}^{J}\left(g_{j}+m_{j}h_{j}\right),h_{\ell}\rangle_{T}+\lambda_{\ell}\frac{\partial}{\partial m_{\ell}}P_{\ell}(g_{\ell}+m_{\ell}h_{\ell})+\lambda_{\ell}w\langle h_{\ell},g_{\ell}+m_{\ell}h_{\ell}\rangle_{D}=0
\]


It's implicit derivative with respect to $\lambda_{k}$ is 
\begin{eqnarray*}
\langle\sum_{j=1}^{J}\frac{\partial\hat{m}_{j}(\lambda)}{\partial\lambda_{k}}h_{j},h_{\ell}\rangle_{T}+\lambda_{\ell}\frac{\partial^{2}}{\partial m_{\ell}^{2}}P_{\ell}(g_{\ell}+m_{\ell}h_{\ell})\frac{\partial\hat{m}_{\ell}(\lambda)}{\partial\lambda_{k}}+\lambda_{\ell}w\|h_{\ell}\|_{D}^{2}\frac{\partial\hat{m}_{\ell}(\lambda)}{\partial\lambda_{k}}\\
+1\left[\ell=k\right]\left(\frac{\partial}{\partial m_{\ell}}P_{\ell}(g_{\ell}+m_{\ell}h_{\ell})+w\langle h_{\ell},g_{\ell}+m_{\ell}h_{\ell}\rangle_{D}\right) & = & 0
\end{eqnarray*}


Define the following matrices 
\[
S:S_{ij}=\langle h_{j},h_{\ell}\rangle_{T}
\]
\[
D_{1}=diag\left(\lambda_{\ell}\frac{\partial^{2}}{\partial m_{\ell}^{2}}P_{\ell}(g_{\ell}+m_{\ell}h_{\ell})\right)
\]


\[
D_{2}=diag\left(\lambda_{\ell}w\|h_{\ell}\|_{D}^{2}\right)
\]


\[
D_{3}=diag\left(\frac{\partial}{\partial m_{\ell}}P_{\ell}(g_{\ell}+m_{\ell}h_{\ell})+w\langle h_{\ell},g_{\ell}+m_{\ell}h_{\ell}\rangle_{D}\right)
\]


\[
M=\left(\begin{array}{cccc}
\frac{\partial\hat{m}_{1}(\lambda)}{\partial\lambda} & \frac{\partial\hat{m}_{2}(\lambda)}{\partial\lambda} & ... & \frac{\partial\hat{m}_{J}(\lambda)}{\partial\lambda}\end{array}\right)
\]


(You will have to omit certain columns/rows of the matrices if $m_{j}=0$
is constant.)

From the implicit differentiation equations, we have the following
system of equations: 
\[
M=D_{3}\left(S+D_{1}+D_{2}\right)^{-1}
\]


We know that $S$ is a PSD matrix (since it can be written as $S=HH^{T}$
where $H_{j}=h_{j}$ evaluated at covariates $T$).

We are interested in bounding the gradient of $\hat{m}_{\tilde{k}}(\lambda)$
wrt $\lambda$, which is the $\tilde{k}$-th column of $M$ has norm.
By Lemma PSD\_Matrix\_Inverse, we know that 
\begin{eqnarray*}
\|\nabla_{\lambda}\hat{m}_{\tilde{k}}(\lambda)\| & = & \|Me_{\tilde{k}}\|\\
 & = & \|D_{3}\left(S+D_{1}+D_{2}\right)^{-1}e_{\tilde{k}}\|\\
 & \le & \|D_{3}\left(D_{1}+D_{2}\right)^{-1}e_{\tilde{k}}\|\\
 & \le & \left|\frac{\partial}{\partial m_{\tilde{k}}}P_{\tilde{k}}(g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}})+w\langle h_{\tilde{k}},g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}}\rangle_{D}\right|\lambda_{\tilde{k}}^{-1}w^{-1}\|h_{\tilde{k}}\|_{D}^{-2}
\end{eqnarray*}
where the last inequality is derived by plugging in the $\tilde{k}$th
entry in the diagonal matrices. 

Note that from the KKT conditions, we have that

\begin{eqnarray*}
\left|\frac{\partial}{\partial m_{\tilde{k}}}P_{\tilde{k}}(g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}})\right| & = & \left|\frac{1}{\lambda_{\tilde{k}}}\langle y-\sum_{j=1}^{J}\left(g_{j}+m_{j}h_{j}\right),h_{\tilde{k}}\rangle_{T}+w\langle h_{\tilde{k}},g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}}\rangle_{D}\right|\\
 & \le & n^{\tau_{min}}\|y-\sum_{j=1}^{J}\left(g_{j}+m_{j}h_{j}\right)\|_{T}\|h_{\tilde{k}}\|_{T}+w\|h_{\tilde{k}}\|_{D}\|g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}}\|_{D}\\
 & \le & \left(\sqrt{\frac{n}{n_{T}}}n^{\tau_{min}}\left(2G+\|\epsilon\|_{T}\right)+wG\right)\|h_{\tilde{k}}\|_{D}
\end{eqnarray*}


Also 
\[
w\langle h_{\tilde{k}},g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}}\rangle_{D}\le w\|h_{\tilde{k}}\|_{D}G
\]


Hence 
\[
\|\nabla_{\lambda}\hat{m}_{\tilde{k}}(\lambda)\|\le\left(\sqrt{\frac{n}{n_{T}}}n^{\tau_{min}}\left(2G+\|\epsilon\|_{T}\right)+2wG\right)n^{\tau_{\min}}w^{-1}\|h_{\tilde{k}}\|_{D}^{-1}
\]


By the MVT, there is some $\alpha\in[0,1]$ such that 
\begin{eqnarray*}
\left|\hat{m}_{\tilde{k}}(\lambda^{(2)})-\hat{m}_{\tilde{k}}(\lambda^{(1)})\right| & = & \left|\left\langle \lambda^{(2)}-\lambda^{(1)},\nabla_{\lambda}\hat{m}_{\tilde{k}}(\lambda)\right\rangle _{\lambda=\alpha\lambda^{(1)}+(1-\alpha)\lambda^{(2)}}\right|\\
 & \le & \|\lambda^{(2)}-\lambda^{(1)}\|\left(\sqrt{\frac{n}{n_{T}}}n^{\tau_{min}}\left(2G+\|\epsilon\|_{T}\right)+2wG\right)n^{\tau_{\min}}\frac{J}{dw}\\
 & = & 1/2
\end{eqnarray*}


But this is a contradiction since we know that $\hat{m}_{\tilde{k}}(\lambda^{(2)})=1$
and $\hat{m}_{\tilde{k}}(\lambda^{(1)})=0$.


\subsection*{Lemma: Additive Models and Additive Penalties, Nonsmooth}

Same assumptions as above, but we allow the penalties to be nonsmooth.

Suppose for almost every $\lambda$, the differentiable space $\Omega^{L_{T}(\cdot,\lambda)}(\hat{g}(\cdot|\lambda))$
is a local optimality space.

Suppose for almost every $\lambda$, the penalty function is twice
differentaible in the differentiable space.

The conclusions are the same as before.

For all $d>0$, any $\lambda^{(1)},\lambda^{(2)}$ that satisfy 
\[
\|\lambda^{(1)}-\lambda^{(2)}\|\le\frac{dw}{2J}\left(\frac{n}{n_{T}}n^{\tau_{min}}\left(2G+\|\epsilon\|_{T}\right)+wG+G\right)^{-1}n^{-\tau_{\min}}
\]


we have 
\[
\|\hat{g}_{j}(\cdot|\lambda^{(1)})-\hat{g}_{j}(\cdot|\lambda^{(2)})\|_{D}\le d/J
\]


Hence 
\[
\|\sum_{j=1}^{J}\hat{g}_{j}(\cdot|\lambda^{(1)})-\hat{g}_{j}(\cdot|\lambda^{(2)})\|_{D}\le d
\]



\subsubsection*{Proof}

Let $\lambda^{(1)},\lambda^{(2)}$ be the penalty parameters satisfying
the distance constraint above. Let $C$ be the constant defined in
the assumption 
\[
\|\lambda^{(1)}-\lambda^{(2)}\|\le dC
\]


Under the assumptions about the differentiable space and the local
optimality space, we know that for almost every pair $\lambda^{(1)},\lambda^{(2)}$,
there is a line 
\[
\mathcal{L}=\left\{ \alpha\lambda^{(1)}+(1-\alpha)\lambda^{(2)}:\alpha\in[0,1]\right\} 
\]


containing a finite set of points $\{\ell_{i}\}_{i=0}^{N+1}\subset\mathcal{L}$
where $\ell_{0}=\lambda^{(1)}$ and $\ell_{N+1}=\lambda^{(2)}$ such
that:

1. The differentiable spaces $\Omega^{L_{T}(\cdot,\ell_{i})}(\hat{g}(\cdot|\ell_{i}))$
satisfy the condition that the differentiable space is a local optimality
differentiable space conditions and

2. The union of the differentiable spaces contains the entire line
$\mathcal{L}$: 
\[
\mathcal{L}\subset\cup_{i=0}^{N+1}\Omega^{L_{T}(\cdot,\ell_{i})}(\hat{g}(\cdot|\ell_{i}))
\]


Now we partition $\mathcal{L}$ according to the differentiable spaces.
We will partition with the centers of each differentiable space and
points in the intersection of all the differentiable spaces. Let $\{\ell_{(i)}\}_{i=0}^{N}\subset\mathcal{L}$
be the points such that $\ell_{(i)}$ is in the differentiable space
$\Omega^{L_{T}(\cdot,\ell_{i})}(\hat{g}(\cdot|\ell_{i}))$ and $\Omega^{L_{T}(\cdot,\ell_{i+1})}(\hat{g}(\cdot|\ell_{i+1}))$.
That is, we choose 
\[
\ell_{(i)}\in\Omega^{L_{T}(\cdot,\ell_{i})}(\hat{g}(\cdot|\ell_{i}))\cap\Omega^{L_{T}(\cdot,\ell_{i+1})}(\hat{g}(\cdot|\ell_{i+1}))
\]


Hence the following points form a partition of $\mathcal{L}$ 
\[
\left(\ell_{0},\ell_{(0)}\right),\left(\ell_{(0)},\ell_{1}\right),...,\left(\ell_{N},\ell_{(N)}\right),\left(\ell_{(N)},\ell_{N+1}\right)
\]


Note that 
\[
\|\ell_{i}-\ell_{(i)}\|\le\frac{\|\ell_{i}-\ell_{(i)}\|}{\|\lambda^{(1)}-\lambda^{(2)}\|}dC
\]


Applying the smooth lemma to the pairs of points above, we have that

\[
\|g(\cdot|\ell_{i})-g(\cdot|\ell_{(i)})\|_{D}\le\frac{\|\ell_{i}-\ell_{(i)}\|}{\|\lambda^{(1)}-\lambda^{(2)}\|}d
\]


Similarly, 
\[
\|g(\cdot|\ell_{i+1})-g(\cdot|\ell_{(i)})\|_{D}\le\frac{\|\ell_{i+1}-\ell_{(i)}\|}{\|\lambda^{(1)}-\lambda^{(2)}\|}d
\]


Hence 
\begin{eqnarray*}
\|g(\cdot|\lambda^{(1)})-g(\cdot|\lambda^{(2)})\|_{D} & \le & \sum_{i=0}^{N}\|g(\cdot|\ell_{i})-g(\cdot|\ell_{(i)})\|_{D}+\|g(\cdot|\ell_{i+1})-g(\cdot|\ell_{(i)})\|_{D}\\
 & \le & d\left(\sum_{i=0}^{N}\frac{\|\ell_{i+1}-\ell_{(i)}\|}{\|\lambda^{(1)}-\lambda^{(2)}\|}+\frac{\|\ell_{i}-\ell_{(i)}\|}{\|\lambda^{(1)}-\lambda^{(2)}\|}\right)\\
 & = & d
\end{eqnarray*}



\subsection*{Lemma PSD\_Matrix\_Inverse}

Suppose $A$ is a PSD matrix and $D$ is a diagonal matrix with positive
entries. Then for any vector $x$, we have 
\[
\|D^{-1}x\|\ge\|(A+D)^{-1}x\|
\]


Moreover, for any diagonal matrix $D_{1}$ with positive entries,
we have 
\[
\|D_{1}D^{-1}x\|\ge\|D_{1}(A+D)^{-1}x\|
\]



\subsubsection*{Proof}

Notation: For matrix $B$, define $B^{2}=BB$.

It suffices to show that for all $x$, 
\[
x^{T}\left(D^{-2}-(A+D)^{-2}\right)x\ge0
\]


That is, we are interested in showing that $D^{-2}-(A+D)^{-2}$ is
PSD. This can be shown by noting that

\[
(A+D)^{2}\succeq D^{2}\implies D^{-2}\succeq(A+D)^{-2}
\]


To show the ``moreover'' part, note that it suffices to show that
$D_{1}^{2}\left(D^{-2}-(A+D)^{-2}\right)$ is PSD. Since $D^{-2}-(A+D)^{-2}$
is PSD, we have
\[
\|D_{1}D^{-1}x\|^{2}\ge\|D_{1}(A+D)^{-1}x\|^{2}
\]


(Note that if $D_{1}$ were a PSD matrix, this would also hold.)
\end{document}
