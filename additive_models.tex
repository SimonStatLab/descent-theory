%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{babel}
\begin{document}

\subsection*{Lemma: Additive Models and Additive Penalties}

Consider the problem 
\[
\frac{1}{2}\|y-\sum_{j=1}^{J}g_{j}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(g_{j})+\frac{w}{2}\|g_{j}\|_{D}^{2}
\]


We suppose the penalty functions $P_{j}$ are convex and twice-differentiable.
(We do not need the semi-norm assumption.)

Suppose that $\sup_{g\in\mathcal{G}}\|g\|_{D}\le G$.

For all $d>0$, any $\lambda^{(1)},\lambda^{(2)}$ that satisfy 
\[
\|\lambda^{(1)}-\lambda^{(2)}\|\le\frac{dw}{2J}\left(\frac{n}{n_{T}}n^{\tau_{min}}\left(2G+\|\epsilon\|_{T}\right)+wG+G\right)^{-1}n^{-\tau_{\min}}
\]


we have 
\[
\|\hat{g}_{j}(\cdot|\lambda^{(1)})-\hat{g}_{j}(\cdot|\lambda^{(2)})\|_{D}\le d/J
\]


Hence
\[
\|\sum_{j=1}^{J}\hat{g}_{j}(\cdot|\lambda^{(1)})-\hat{g}_{j}(\cdot|\lambda^{(2)})\|_{D}\le d
\]



\subsubsection*{Proof}

Let $h_{j}=\hat{g}_{j}(\cdot|\lambda^{(1)})-\hat{g}_{j}(\cdot|\lambda^{(2)})$.
Suppose for contradiction that for $\tilde{k}$, we have $\|h_{\tilde{k}}\|_{D}>d/J$. 

Let 
\[
Z=\left\{ j:\|h_{j}\|>0\right\} 
\]
Consider the optimization problem

\[
\left\{ \hat{m}_{j}(\lambda)\right\} _{j\in Z}=\arg\min_{m}\frac{1}{2}\|y-\sum_{j=1}^{J}\left(g_{j}+m_{j}h_{j}\right)\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(g_{j}+m_{j}h_{j})+\frac{w}{2}\|g_{j}+m_{j}h_{j}\|_{D}^{2}
\]


Note that if $\|h_{j}\|=0$, then we just set $m_{j}=0$ as a constant.

Now by the KKT conditions, for all $\ell\in Z$, we have

\[
\langle y-\sum_{j=1}^{J}\left(g_{j}+m_{j}h_{j}\right),h_{\ell}\rangle_{T}+\lambda_{\ell}\frac{\partial}{\partial m_{\ell}}P_{\ell}(g_{\ell}+m_{\ell}h_{\ell})+\lambda_{\ell}w\langle h_{\ell},g_{\ell}+m_{\ell}h_{\ell}\rangle_{D}=0
\]


It's implicit derivative with respect to $\lambda_{k}$ is
\begin{eqnarray*}
\langle\sum_{j=1}^{J}\frac{\partial\hat{m}_{j}(\lambda)}{\partial\lambda_{k}}h_{j},h_{\ell}\rangle_{T}+\lambda_{\ell}\frac{\partial^{2}}{\partial m_{\ell}^{2}}P_{\ell}(g_{\ell}+m_{\ell}h_{\ell})\frac{\partial\hat{m}_{\ell}(\lambda)}{\partial\lambda_{k}}+\lambda_{\ell}w\|h_{\ell}\|_{D}^{2}\frac{\partial\hat{m}_{\ell}(\lambda)}{\partial\lambda_{k}}\\
+1\left[\ell=k\right]\left(\frac{\partial}{\partial m_{\ell}}P_{\ell}(g_{\ell}+m_{\ell}h_{\ell})+w\langle h_{\ell},g_{\ell}+m_{\ell}h_{\ell}\rangle_{D}\right) & = & 0
\end{eqnarray*}


Define the following matrices 
\[
S:S_{ij}=\langle h_{j},h_{\ell}\rangle_{T}
\]
\[
D_{1}=diag\left(\lambda_{\ell}\frac{\partial^{2}}{\partial m_{\ell}^{2}}P_{\ell}(g_{\ell}+m_{\ell}h_{\ell})\right)
\]


\[
D_{2}=diag\left(\lambda_{\ell}w\|h_{\ell}\|_{D}^{2}\right)
\]


\[
D_{3}=diag\left(\frac{\partial}{\partial m_{\ell}}P_{\ell}(g_{\ell}+m_{\ell}h_{\ell})+w\langle h_{\ell},g_{\ell}+m_{\ell}h_{\ell}\rangle_{D}\right)
\]


\[
M=\left(\begin{array}{cccc}
\frac{\partial\hat{m}_{1}(\lambda)}{\partial\lambda} & \frac{\partial\hat{m}_{2}(\lambda)}{\partial\lambda} & ... & \frac{\partial\hat{m}_{J}(\lambda)}{\partial\lambda}\end{array}\right)
\]


(You will have to omit certain columns/rows of the matrices if $m_{j}=0$
is constant.)

From the implicit differentiation equations, we have the following
system of equations:
\[
M=D_{3}\left(S+D_{1}+D_{2}\right)^{-1}
\]


We know that $S$ is a PSD matrix (since it can be written as $S=HH^{T}$
where $H_{j}=h_{j}|_{T}$). Hence we can express $S=UD_{4}U^{T}$
(where $U$ are orthonormal)
\[
M=D_{3}\left(UD_{4}U^{T}+D_{1}+D_{2}\right)^{-1}
\]


Let $D_{12}=D_{1}+D_{2}$. By Woodbury's matrix identity, 
\[
M=D_{3}\left(D_{12}^{-1}-D_{12}^{-1}U\left(\Sigma^{-1}+D_{12}\right)^{-1}U^{T}D_{12}^{-1}\right)
\]


Now consider the $\tilde{k}$th column of $M$: 
\begin{eqnarray*}
\nabla_{\lambda}\hat{m}_{\tilde{k}}(\lambda) & = & D_{3}\left(D_{12}^{-1}-D_{12}^{-1}U\left(\Sigma^{-1}+D_{12}\right)^{-1}U^{T}D_{12}^{-1}\right)e_{\tilde{k}}
\end{eqnarray*}


Note that the $j$th elements for $j\ne\tilde{k}$ must be zero since
orthogonality is preserved by the orthonormal matrices $U$ . Also
note that $D_{12}^{-1}U\left(\Sigma^{-1}+D_{12}\right)^{-1}U^{T}D_{12}^{-1}$
is a positive definite matrix. Hence we have 
\begin{eqnarray*}
\left|\frac{\partial}{\partial\lambda_{\tilde{k}}}\hat{m}_{\tilde{k}}(\lambda)\right| & \le & \left|\frac{\partial}{\partial m_{\tilde{k}}}P_{\tilde{k}}(g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}})+w\langle h_{\tilde{k}},g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}}\rangle_{D}\right|\lambda_{\tilde{k}}^{-1}w^{-1}\|h_{\tilde{k}}\|_{D}^{-2}
\end{eqnarray*}


Note that from the KKT conditions, we have that
\begin{eqnarray*}
\left|\frac{\partial}{\partial m_{\tilde{k}}}P_{\tilde{k}}(g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}})\right| & \le & \left|\frac{1}{\lambda_{\tilde{k}}}\langle y-\sum_{j=1}^{J}\left(g_{j}+m_{j}h_{j}\right),h_{\tilde{k}}\rangle_{T}+w\langle h_{\tilde{k}},g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}}\rangle_{D}\right|\\
 & \le & n^{\tau_{min}}\|y-\sum_{j=1}^{J}\left(g_{j}+m_{j}h_{j}\right)\|_{T}\|h_{\tilde{k}}\|_{T}+w\|h_{\tilde{k}}\|_{D}\|g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}}\|_{D}\\
 & \le & \left(\frac{n}{n_{T}}n^{\tau_{min}}\left(2G+\|\epsilon\|_{T}\right)+wG\right)\|h_{\tilde{k}}\|_{D}
\end{eqnarray*}


Also 
\[
w\langle h_{\tilde{k}},g_{\tilde{k}}+m_{\tilde{k}}h_{\tilde{k}}\rangle_{D}\le w\|h_{\tilde{k}}\|_{D}G
\]


Hence
\[
\left|\frac{\partial}{\partial\lambda_{\tilde{k}}}\hat{m}_{\tilde{k}}(\lambda)\right|\le\left(\frac{n}{n_{T}}n^{\tau_{min}}\left(2G+\|\epsilon\|_{T}\right)+wG+G\right)n^{\tau_{\min}}w^{-1}\|h_{\tilde{k}}\|_{D}^{-1}
\]


By the MVT, there is some $\alpha\in[0,1]$ such that 
\begin{eqnarray*}
\left|\hat{m}_{\tilde{k}}(\lambda^{(2)})-\hat{m}_{\tilde{k}}(\lambda^{(1)})\right| & = & \left|\left\langle \lambda^{(2)}-\lambda^{(1)},\frac{\partial}{\partial\lambda_{\tilde{k}}}\hat{m}_{\tilde{k}}(\lambda)\right\rangle _{\lambda=\alpha\lambda^{(1)}+(1-\alpha)\lambda^{(2)}}\right|\\
 & \le & \|\lambda^{(2)}-\lambda^{(1)}\|\left(\frac{n}{n_{T}}n^{\tau_{min}}\left(2G+\|\epsilon\|_{T}\right)+wG+G\right)n^{\tau_{\min}}\frac{J}{dw}\\
 & = & 1/2
\end{eqnarray*}


But this is a contradiction since we know that $\hat{m}_{\tilde{k}}(\lambda^{(2)})=1$
and $\hat{m}_{\tilde{k}}(\lambda^{(1)})=0$.


\subsection*{Lemma: Additive Models and Additive Penalties, Nonsmooth}

Same assumptions as above, but we allow the penalties to be nonsmooth. 

Suppose for almost every $\lambda$, the differentiable space $\Omega^{L_{T}(\cdot,\lambda)}(\hat{g}(\cdot|\lambda))$
is a local optimality space.

Suppose for almost every $\lambda$, the penalty function is twice
differentaible in the differentiable space.

The conclusions are the same as before.

For all $d>0$, any $\lambda^{(1)},\lambda^{(2)}$ that satisfy 
\[
\|\lambda^{(1)}-\lambda^{(2)}\|\le\frac{dw}{2J}\left(\frac{n}{n_{T}}n^{\tau_{min}}\left(2G+\|\epsilon\|_{T}\right)+wG+G\right)^{-1}n^{-\tau_{\min}}
\]


we have 
\[
\|\hat{g}_{j}(\cdot|\lambda^{(1)})-\hat{g}_{j}(\cdot|\lambda^{(2)})\|_{D}\le d/J
\]


Hence
\[
\|\sum_{j=1}^{J}\hat{g}_{j}(\cdot|\lambda^{(1)})-\hat{g}_{j}(\cdot|\lambda^{(2)})\|_{D}\le d
\]



\subsubsection*{Proof}

Let $\lambda^{(1)},\lambda^{(2)}$ be the penalty parameters satisfying
the distance constraint above. Let $C$ be the constant defined in
the assumption 
\[
\|\lambda^{(1)}-\lambda^{(2)}\|\le dC
\]


Under the assumptions about the differentiable space and the local
optimality space, we know that for almost every pair $\lambda^{(1)},\lambda^{(2)}$,
there is a line 
\[
\mathcal{L}=\left\{ \alpha\lambda^{(1)}+(1-\alpha)\lambda^{(2)}:\alpha\in[0,1]\right\} 
\]


containing a finite set of points $\{\ell_{i}\}_{i=0}^{N+1}\subset\mathcal{L}$
where $\ell_{0}=\lambda^{(1)}$ and $\ell_{N+1}=\lambda^{(2)}$ such
that:

1. The differentiable spaces $\Omega^{L_{T}(\cdot,\ell_{i})}(\hat{g}(\cdot|\ell_{i}))$
satisfy the condition that the differentiable space is a local optimality
differentiable space conditions and 

2. The union of the differentiable spaces contains the entire line
$\mathcal{L}$:
\[
\mathcal{L}\subset\cup_{i=0}^{N+1}\Omega^{L_{T}(\cdot,\ell_{i})}(\hat{g}(\cdot|\ell_{i}))
\]


Now we partition $\mathcal{L}$ according to the differentiable spaces.
We will partition with the centers of each differentiable space and
points in the intersection of all the differentiable spaces. Let $\{\ell_{(i)}\}_{i=0}^{N}\subset\mathcal{L}$
be the points such that $\ell_{(i)}$ is in the differentiable space
$\Omega^{L_{T}(\cdot,\ell_{i})}(\hat{g}(\cdot|\ell_{i}))$ and $\Omega^{L_{T}(\cdot,\ell_{i+1})}(\hat{g}(\cdot|\ell_{i+1}))$.
That is, we choose
\[
\ell_{(i)}\in\Omega^{L_{T}(\cdot,\ell_{i})}(\hat{g}(\cdot|\ell_{i}))\cap\Omega^{L_{T}(\cdot,\ell_{i+1})}(\hat{g}(\cdot|\ell_{i+1}))
\]


Hence the following points form a partition of $\mathcal{L}$
\[
\left(\ell_{0},\ell_{(0)}\right),\left(\ell_{(0)},\ell_{1}\right),...,\left(\ell_{N},\ell_{(N)}\right),\left(\ell_{(N)},\ell_{N+1}\right)
\]


Note that
\[
\|\ell_{i}-\ell_{(i)}\|\le\frac{\|\ell_{i}-\ell_{(i)}\|}{\|\lambda^{(1)}-\lambda^{(2)}\|}dC
\]


Applying the smooth lemma to the pairs of points above, we have that

\[
\|g(\cdot|\ell_{i})-g(\cdot|\ell_{(i)})\|_{D}\le\frac{\|\ell_{i}-\ell_{(i)}\|}{\|\lambda^{(1)}-\lambda^{(2)}\|}d
\]


Similarly,
\[
\|g(\cdot|\ell_{i+1})-g(\cdot|\ell_{(i)})\|_{D}\le\frac{\|\ell_{i+1}-\ell_{(i)}\|}{\|\lambda^{(1)}-\lambda^{(2)}\|}d
\]


Hence
\begin{eqnarray*}
\|g(\cdot|\lambda^{(1)})-g(\cdot|\lambda^{(2)})\|_{D} & \le & \sum_{i=0}^{N}\|g(\cdot|\ell_{i})-g(\cdot|\ell_{(i)})\|_{D}+\|g(\cdot|\ell_{i+1})-g(\cdot|\ell_{i()})\|_{D}\\
 & \le & d\left(\sum_{i=0}^{N}\frac{\|\ell_{i+1}-\ell_{(i)}\|}{\|\lambda^{(1)}-\lambda^{(2)}\|}+\frac{\|\ell_{i}-\ell_{(i)}\|}{\|\lambda^{(1)}-\lambda^{(2)}\|}\right)\\
 & = & d
\end{eqnarray*}

\end{document}
