%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsbsy}
\usepackage{amssymb}
\usepackage{babel}
\begin{document}

\title{The effect of adding a small ridge penalty}

\maketitle
We consider the case of $p$-dimensional parametric models. Let the
original training criterion be denoted 
\[
L_{T}(\boldsymbol{\theta}|\boldsymbol{\lambda})=\frac{1}{2}\|y-f\left(\cdot|\boldsymbol{\theta}\right)\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}P_{j}(\boldsymbol{\theta})
\]


Let the minimizer to the perturbed training criterion be denoted for
any $w\ge0$,

\[
\hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}(w)=\arg\min_{\theta\in\mathbb{R}^{p}}L_{T}(\boldsymbol{\theta}|\boldsymbol{\lambda})+\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|\boldsymbol{\theta}\|^{2}
\]


We show that adding a small ridge penalty scaled by some constant
$w$ does not change the fitted model by very much. 

This document is organized as follows
\begin{enumerate}
\item We quantify the effect of the ridge penalty for $w$ that are small
enough. The proof uses the implicit function theorem and the mean
value inequality. We assume that the original training criterion is
strongly convex in some neighborhood around its minimizer. 

\begin{enumerate}
\item Caveat: To ensure our near-parametric convergence rates in Theorem
1, the results depend on this neighborhood around the minimizer not
to shrink in $n$ so quickly  that $w$ needs to be exponential in
$n$. It also depends on the strong convexity constant $m$ not to
be shrinking at some exponential rate in $m$. I think these are okay
assumptions?
\end{enumerate}
\item We extend the result to penalized regression problems with nonsmooth
penalties.
\end{enumerate}

\section{Result}

Let 
\[
D(w,\boldsymbol{\theta})=\nabla_{\theta}\left[L_{T}(\boldsymbol{\theta}|\boldsymbol{\lambda})+\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|\boldsymbol{\theta}\|^{2}\right]
\]


and suppose $D(w,\boldsymbol{\theta})$ is continuously differentiable
in a neighborhood $\Theta_{0}$ containing $\hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}(0)$.

Suppose that there is an $m>0$ such that 
\[
\nabla_{\theta}^{2}L_{T}(\boldsymbol{\theta})|_{\theta=\hat{\theta}_{\lambda}(0)}\succeq mI\mbox{ }
\]


There exists a $W>0$ such that for all $w\in[0,W)$ 
\begin{eqnarray*}
\left\Vert \hat{\boldsymbol{\theta}}_{\lambda}(0)-\hat{\boldsymbol{\theta}}_{\lambda}(w)\right\Vert  & \le & \frac{w}{m}\left(\sum_{j=1}^{J}\lambda_{j}\right)\|\hat{\boldsymbol{\theta}}_{\lambda}(0)\|
\end{eqnarray*}



\subsubsection*{Proof}

By the implicit function, since $D(w,\boldsymbol{\theta})$ is continuously
differentiable in a neighborhood $\Theta_{0}$ containing $\hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}(0)$
and $\nabla_{\theta}D(w,\boldsymbol{\theta})=\nabla_{\theta}^{2}L_{T}(\boldsymbol{\theta}|\boldsymbol{\lambda})$
is nonsingular at $\hat{\boldsymbol{\theta}}_{\lambda}(0)$, $\hat{\boldsymbol{\theta}}_{\lambda}(w)$
is continuously differentiable over $[0,W)$ for some $W>0$. Furthermore,
the implicit function theorem states that for all $w\in[0,W)$ 
\begin{eqnarray*}
\nabla_{w}\hat{\boldsymbol{\theta}}_{\lambda}(w) & = & -\left(\nabla_{\theta}^{2}L_{T}(\boldsymbol{\theta})|_{\theta=\hat{\theta}_{\lambda}(0)}\right)^{-1}\nabla_{w}D\left(w,\hat{\boldsymbol{\theta}}_{\lambda}(w)\right)\\
 & = & -\left(\nabla_{\theta}^{2}L_{T}(\boldsymbol{\theta})|_{\theta=\hat{\theta}_{\lambda}(0)}\right)^{-1}\left(\sum_{j=1}^{J}\lambda_{j}\right)\hat{\boldsymbol{\theta}}_{\lambda}(w)
\end{eqnarray*}


Since $\nabla_{\theta}^{2}L_{T}(\boldsymbol{\theta})|_{\theta=\hat{\theta}_{\lambda}(0)}\succeq mI\mbox{ }$,
then for all $w\in[0,W)$
\[
\left\Vert \nabla_{w}\hat{\boldsymbol{\theta}}_{\lambda}(w)\right\Vert \le m^{-1}\left(\sum_{j=1}^{J}\lambda_{j}\right)\|\hat{\boldsymbol{\theta}}_{\lambda}(w)\|
\]


We bound $\|\hat{\boldsymbol{\theta}}_{\lambda}(w)\|$ using the definitions
of $\hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}(0)$ and $\hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}(w)$:
\[
L_{T}(\hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}(w))+\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}(w)\|^{2}\le L_{T}(\hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}(0))+\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|\hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}(0)\|^{2}
\]


and 
\[
L_{T}(\hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}(0))\le L_{T}(\hat{\boldsymbol{\theta}}_{\boldsymbol{\lambda}}(w))
\]


Adding these two inequalities, we get that for all $w\in[0,W)$
\[
\|\hat{\boldsymbol{\theta}}_{\lambda}(w)\|^{2}\le\|\hat{\boldsymbol{\theta}}_{\lambda}(0)\|^{2}
\]


By the Mean Value Inequality, for all $w\in[0,W)$, there is a $w'\in(0,w)$
such that 
\begin{eqnarray*}
\left\Vert \hat{\boldsymbol{\theta}}_{\lambda}(0)-\hat{\boldsymbol{\theta}}_{\lambda}(w)\right\Vert  & \le & w\left\Vert \left.\nabla_{w}\hat{\boldsymbol{\theta}}_{\lambda}(w)\right|_{w=w'}\right\Vert \\
 & \le & \frac{w}{m}\left(\sum_{j=1}^{J}\lambda_{j}\right)\|\hat{\boldsymbol{\theta}}_{\lambda}(w')\|\\
 & \le & \frac{w}{m}\left(\sum_{j=1}^{J}\lambda_{j}\right)\|\hat{\boldsymbol{\theta}}_{\lambda}(0)\|
\end{eqnarray*}



\section{Nonsmooth Case}

Let the differentiable space at $\hat{\boldsymbol{\theta}}_{0}$ be
defined as 
\[
\Omega_{\lambda}=\left\{ \left.\boldsymbol{\eta}\right|\lim_{\epsilon\rightarrow0}\frac{L_{T}\left(\hat{\boldsymbol{\theta}}_{\lambda}(0)+\epsilon\boldsymbol{\eta}|\boldsymbol{\lambda}\right)-L_{T}\left(\hat{\boldsymbol{\theta}}_{\lambda}(0)|\boldsymbol{\lambda}\right)}{\epsilon}\mbox{ exists}\right\} 
\]


Let $U_{\lambda}$ be an orthonormal basis of $\Omega_{\lambda}$.
Suppose $U_{\lambda}$ has rank $q\le p$.

Suppose that for all $w<W'$, we have that
\[
\min_{\theta\in\mathbb{R}^{p}}L_{T}(\boldsymbol{\theta}|\boldsymbol{\lambda})+\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|\boldsymbol{\theta}\|^{2}=\min_{\beta\in\mathbb{R}^{q}}L_{T}(U_{\lambda}\boldsymbol{\beta}|\boldsymbol{\lambda})+\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|U_{\lambda}\boldsymbol{\beta}\|^{2}
\]


Let 
\[
\hat{\boldsymbol{\beta}}_{\lambda}(w)=\arg\min_{\beta\in\mathbb{R}^{q}}L_{T}(U_{\lambda}\boldsymbol{\beta}|\boldsymbol{\lambda})+\sum_{j=1}^{J}\lambda_{j}\frac{w}{2}\|U_{\lambda}\boldsymbol{\beta}\|^{2}
\]


Suppose $_{U_{\lambda}}\nabla_{\beta}^{2}L_{T}(U_{\lambda}\boldsymbol{\beta}|\boldsymbol{\lambda})$
is exists and is continuous in a neighborhood of $\hat{\boldsymbol{\beta}}_{\lambda}(w)$.
Furthermore suppose there is a $m>0$ such that 
\[
\left._{U_{\lambda}}\nabla_{\beta}^{2}L_{T}(U_{\lambda}\boldsymbol{\beta}|\boldsymbol{\lambda})\right|_{\beta=\hat{\boldsymbol{\beta}}_{\lambda}(w)}\succeq mI
\]


Then there is a $W>0$ such that for all $w\in[0,W)$, we have


\subsubsection*{
\begin{eqnarray*}
\left\Vert \hat{\boldsymbol{\theta}}_{\lambda}(0)-\hat{\boldsymbol{\theta}}_{\lambda}(w)\right\Vert _{2} & \le & \frac{w}{m}\left(\sum_{j=1}^{J}\lambda_{j}\right)\|\hat{\boldsymbol{\theta}}_{\lambda}(0)\|_{2}
\end{eqnarray*}
Proof}

By the result in Section 1, we know that
\begin{eqnarray*}
\left\Vert \hat{\boldsymbol{\beta}}_{\lambda}(0)-\hat{\boldsymbol{\beta}}_{\lambda}(w)\right\Vert _{2} & \le & \frac{w}{m}\left(\sum_{j=1}^{J}\lambda_{j}\right)\|\hat{\boldsymbol{\beta}}_{\lambda}(0)\|_{2}
\end{eqnarray*}


Since $\hat{\boldsymbol{\theta}}_{w}=U_{\lambda}\hat{\boldsymbol{\beta}}_{w}$
and $U_{\lambda}$ is an orthonormal matrix, the result follows.
\end{document}
