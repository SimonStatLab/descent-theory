%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsbsy}
\usepackage{amssymb}
\usepackage{wasysym}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\section{K-Fold Cross-Validation}

We are interested in modeling the conditional relationship between
$y$ and predictors $X$ using function class $\mathcal{G}$. Suppose
$X$ is from some bounded domain. Suppose $\mathcal{G}$ is a convex
function class. There is some constant $G$ s.t. $\sup_{g\in\mathcal{G}}\|g\|^{2}\le G<\infty$
(this envelope condition makes the proof simpler, though I think we
can easily drop it with some regularity assumptions?).

We fit the function using some penalty function $P$ taken to the
power $v\ge1$. Suppose $P$ is a semi-norm, smooth, and convex. 

Consider the joint optimization problem to find the best penalty parameter
$\lambda$ in $\Lambda$ via $K$-fold cross validation. Let $D$
be the entire dataset of $n$ observations. For $k=1,...,K$, let
$D_{k}$ represent the $k$th fold with $n_{k}$ observations and
$D_{-k}$ denote all the folds minus the $k$th fold.

Let $\|h\|^{2}=\int h(x)d\mu(x)$. Let $\|h\|_{k}^{2}=\frac{1}{n_{k}}\sum_{i\in D_{k}}h(x_{i})^{2}$
and similarly for $\|h\|_{-k}^{2}$ for the set $D_{-k}$ and $\|h\|_{D}^{2}$
for the set $D$. Let $\langle h,g\rangle_{k}=\frac{1}{n_{k}}\sum_{i\in D_{k}}h(x_{i})g(x_{i})$
and $\langle h,g\rangle_{-k}$ for the set $D_{-k}$ and $\langle h,g\rangle_{D}$
for the set $D$.

We perform $K$-fold cross-validation with a small additional ridge
penalty.

\[
\hat{\lambda}=\arg\min_{\lambda\in\Lambda}\frac{1}{2}\sum_{k=1}^{K}\|y-\hat{g}_{\lambda}(\cdot|D_{-k})\|_{k}^{2}
\]
\[
\hat{g}(\lambda|D_{-k})=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\|y-g\|_{-k}^{2}+\lambda\left(P^{v}(g)+\frac{w}{2}\|g\|^{2}\right)
\]


The model chosen via $K$-fold CV is 
\[
\hat{g}(\hat{\lambda}|D)=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\|y-g\|_{D}^{2}+\hat{\lambda}\left(P^{v}(g)+\frac{w}{2}\|g\|^{2}\right)
\]


Let the range of $\Lambda$ be from $\lambda_{min}=O_{P}(n^{-\tau_{min}})$
to $\lambda_{max}=O_{P}(1)$.

We show that
\[
\|\hat{g}_{\hat{\lambda}}(\cdot|D)-g^{*}\|_{D}\apprle\sqrt{\sum_{k=1}^{K}\left\Vert g^{*}-\hat{g}_{\tilde{\lambda}}(\cdot|D_{-k})\right\Vert _{k}^{2}}+G\left(\frac{(1+\log(v/w)+\log(4\sigma^{2}+P^{v-1}(g^{*})+G)+\kappa\log n)}{\min_{k=1:K}\{n_{k}\}}\right)^{1/2}
\]



\subsubsection*{Notation}

$a\lesssim b$ means that $a\le Cb+c$ where $C>0,c$ are constants
independent of $n$.


\section{Proof}


\subsubsection*{Step 1:}

Define the convex combination 
\[
\hat{\xi}_{\lambda}(x)=\frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\hat{g}_{\lambda}(x|D_{-k})
\]


By the triangle inequality,
\begin{eqnarray*}
\|\hat{g}_{\hat{\lambda}}(\cdot|D)-g^{*}\|_{D} & \le & \|\hat{g}_{\hat{\lambda}}(\cdot|D)-\hat{\xi}_{\hat{\lambda}}\|_{D}+\|\hat{\xi}_{\hat{\lambda}}-g^{*}\|_{D}
\end{eqnarray*}


We bound the first and second summands in steps 2 and 3, respectively.


\subsubsection*{Step 2: Bound $\|\hat{g}_{\hat{\lambda}}(\cdot|D)-\xi_{\hat{\lambda}}\|_{D}$}

Adding the two inequalities from Lemma 1 and 2, we have

\begin{eqnarray*}
 &  & \|\hat{g}_{\hat{\lambda}}(\cdot|D)-\hat{\xi}_{\hat{\lambda}}\|_{D}^{2}\\
 & \le & \frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\left(\left|\langle\epsilon,\hat{\xi}_{\hat{\lambda}}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\rangle_{-k}\right|+\left|\langle g^{*}-\hat{\xi}_{\hat{\lambda}},\hat{\xi}_{\hat{\lambda}}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\rangle_{-k}\right|\right)\\
 & \apprle & \sum_{k=1}^{K}\left\Vert g^{*}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\right\Vert _{k}^{2}+\left(\sum_{\ell=1}^{K}\left|\langle\epsilon,\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})-g^{*}\rangle_{-\ell}\right|+\left\Vert g^{*}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\right\Vert _{\ell}^{2}-\left\Vert g^{*}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\right\Vert _{k}^{2}\right)
\end{eqnarray*}


The Lemmas below allow us to bound each of the summands in the inequality.
Note that all the Lemmas have probability statements that condition
on $\|\epsilon\|_{k}\le2\sigma$. Recall that since $\epsilon$ is
sub-gaussian, Bernstein's inequality states that $\|\epsilon\|_{k}\le2\sigma$
indeed occurs with high probability: 
\[
Pr\left(\|\epsilon\|_{k}\ge2\sigma\right)\le\exp\left(-n_{k}\frac{\sigma^{2}}{c}\right)
\]


Now to bound the first term, Lemma 5 states that with high probability
\[
\sum_{k=1}^{K}\left\Vert g^{*}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\right\Vert _{k}^{2}\apprle\sum_{k=1}^{K}\left\Vert g^{*}-\hat{g}_{\tilde{\lambda}}(\cdot|D_{-k})\right\Vert _{k}^{2}+\delta^{2}
\]


To bound the last two terms, we first note that the entropy of the
function class $\hat{\mathcal{G}}(D_{-k})=\left\{ \hat{g}_{\lambda}(\cdot|D_{-k}):\lambda\in\Lambda\right\} $
is (for any empirical distribution $Q$) 
\[
H\left(d,\hat{\mathcal{G}}(D_{-k}),\|\cdot\|_{Q}\right)\apprle\psi(u)=\log\left(\frac{v}{wd}\right)+\kappa\log n+\log(4\sigma^{2}+P^{v-1}(g^{*})+G)
\]


So we have

\begin{eqnarray*}
\int_{0}^{2G}\psi^{1/2}(u)du & = & \int_{0}^{2G}\left(\log\left(\frac{v}{uw}\right)+\kappa\log n+\log(4\sigma^{2})\right)^{1/2}du\\
 & \apprle & 2G\left(\int_{0}^{1}\log\left(\frac{1}{u}\right)-\log(v/w)+\kappa\log n+\log(4\sigma^{2})du\right)^{1/2}\\
 & \le & 2G\left(1+\log(4\sigma^{2}+P^{v-1}(g^{*})+G)+\log(v/w)+\kappa\log n\right)^{1/2}
\end{eqnarray*}


By Lemma 3 and 4, let 
\[
\delta=CG\left(\left(\frac{1+\log(v/w)+\log(4\sigma^{2}+P^{v-1}(g^{*})+G)+\kappa\log n}{\min_{k=1:K}\{n_{k}\}}\right)^{1/2}\vee1\right)
\]


where the constant $C$ only depends on the subgaussian constants.

By Lemma 3, for some constant $c$, for all $\ell,k=1:K$, we have
\[
Pr\left(\sup_{\lambda\in\Lambda}\frac{\left|\langle\epsilon,\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})-g^{*}\rangle_{-\ell}\right|}{\|\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})-g^{*}\|_{-\ell}}\ge\delta\wedge\|\epsilon\|_{-k}\le2\sigma\wedge\|\epsilon\|_{-\ell}\le2\sigma\right)\le c\exp\left(-(n-n_{\ell})\frac{\delta^{2}}{c^{2}R^{2}}\right)
\]


By Lemma 4, for some constant $c$, we have for all $\ell,k=1:K$,
we have 
\[
Pr\left(\sup_{\lambda\in\Lambda}\frac{\left\Vert g^{*}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\right\Vert _{\ell}^{2}-\left\Vert g^{*}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\right\Vert _{k}^{2}}{\left\Vert g^{*}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\right\Vert _{k\cup\ell}}\ge\delta\wedge\|\epsilon\|_{-k}\le2\sigma\right)\le c\exp\left(-n_{\ell}\frac{\delta^{2}}{c^{2}R^{2}}\right)+c\exp\left(-n_{k}\frac{\delta^{2}}{c^{2}R^{2}}\right)
\]


Combining all the probability bounds above (and Easy Lemma 1), we
have with high probability
\[
\|\hat{g}_{\hat{\lambda}}(\cdot|D)-\hat{\xi}_{\hat{\lambda}}\|_{D}^{2}\apprle\sum_{k=1}^{K}\left\Vert g^{*}-\hat{g}_{\tilde{\lambda}}(\cdot|D_{-k})\right\Vert _{k}^{2}+\delta_{max}^{2}
\]



\subsubsection*{Step 3: Bound $\|\hat{\xi}_{\hat{\lambda}}-g^{*}\|_{D}$}

We know that 
\[
\|\hat{\xi}_{\hat{\lambda}}-g^{*}\|_{D}\apprle\sum_{k=1}^{k}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-g^{*}\|_{D}
\]


We bound each term in the summation separately. For every $k$, we
know

\begin{eqnarray*}
\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-g^{*}\|_{D}^{2} & \apprle & \sum_{\ell=1}^{k}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-g^{*}\|_{k}^{2}+\left(\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-g^{*}\|_{\ell}^{2}-\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-g^{*}\|_{k}^{2}\right)
\end{eqnarray*}


Again we know that $\|\epsilon\|_{k}\le2\sigma$ for all folds $D_{k}$
with high probability.

Conditioning on $\|\epsilon\|_{k}\le2\sigma$, Lemma 5 gives us that
with high probability 
\[
\sum_{\ell=1}^{k}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-g^{*}\|_{k}^{2}\apprle\sum_{\ell=1}^{k}\|\hat{g}_{\tilde{\lambda}}(x|D_{-k})-g^{*}\|_{k}^{2}+\delta_{max}^{2}
\]
 

As shown in Step 2, Lemma 4 and Easy Lemma 1 imply that the following
bound holds for all $\ell=1:K$ with high probability as long as $\|\epsilon\|_{k}\le2\sigma$
\[
\left|\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-g^{*}\|_{\ell}^{2}-\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-g^{*}\|_{k}^{2}\right|\le\delta_{max}\left(\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-g^{*}\|_{k}+\delta_{max}\right)
\]


Combining these two results, we get that with high probability,

\begin{eqnarray*}
\|\hat{\xi}_{\hat{\lambda}}-g^{*}\|_{D} & \apprle & \sqrt{\sum_{k=1}^{K}\left\Vert g^{*}-\hat{g}_{\tilde{\lambda}}(\cdot|D_{-k})\right\Vert _{k}^{2}}+\delta_{max}
\end{eqnarray*}



\subsection{Lemmas}


\subsubsection{Lemma 0}

Consider any empirical distributions $T$ and $Q$.

Consider the function class 
\[
\hat{\mathcal{G}}(T,\epsilon_{T})=\left\{ \hat{g}_{\lambda}(\cdot|T,\epsilon_{T})=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\|y-g\|_{T}^{2}+\lambda\left(P^{v}(g)+\frac{w}{2}\|g\|^{2}\right):\lambda\in\Lambda\right\} 
\]


Suppose the penalty function $P$ is a semi-norm, smooth, and convex.
Suppose $\min_{h\in\mathcal{G}:P(h)=1}\|h\|^{2}=O_{p}(n^{-u})$ and
for all $h$, $\|h\|_{Q}\le O_{p}(n^{p})P(h)$. 

Suppose $v\ge1$.

Suppose $\lambda_{min}=O_{P}(n^{-\tau_{min}})$ and $\lambda_{max}=O_{P}(1)$.

Then the entropy bound is 
\[
H\left(d,\hat{\mathcal{G}}(T,\epsilon_{T}),\|\cdot\|_{Q}\right)\apprle\log\left(\frac{1}{dw}\right)+\kappa\log n+\log\|\epsilon\|_{T}^{2}
\]


where $\kappa$ depends on $\tau_{min},v,u,p$.


\subsubsection*{Proof}

To find the covering number for $\hat{\mathcal{G}}$, we bound the
distance$\|\hat{g}_{\lambda_{0}}(\cdot|T)-\hat{g}_{\lambda_{0}+\delta}(\cdot|T)\|_{Q}$
for every $\lambda_{0}\in\Lambda$.

Consider the function $h=c\left(\hat{g}_{\lambda_{0}}-\hat{g}_{\lambda_{0}+\delta}\right)$
where $c>0$ is some constant s.t. $P(h)=1$. (We'll assume that $\|\hat{g}_{\lambda_{0}}-\hat{g}_{\lambda_{0}+\delta}\|_{Q}>0$,
since we'll be done otherwise.) Consider the 1-dimensional optimization
problem

\[
\hat{m}_{h}(\lambda)=\arg\min_{m}\frac{1}{2}\|y-(\hat{g}_{\lambda_{0}}+mh)\|_{T}^{2}+\lambda\left(P^{v}(\hat{g}_{\lambda_{0}}+mh)+\frac{w}{2}\|\hat{g}_{\lambda_{0}}+mh\|^{2}\right)
\]


Taking the derivative of the criterion wrt $m$, we get

\[
\left.-\langle h,y-(\hat{g}_{\lambda_{0}}+mh)\rangle_{T}+\lambda\left(\frac{\partial}{\partial m}P^{v}(\hat{g}_{\lambda_{0}}+mh)+w\langle h,\hat{g}_{\lambda_{0}}+mh\rangle\right)\right|_{m=\hat{m}_{h}(\lambda)}=0
\]


By implicit differentiation wrt $\lambda$, we have 
\begin{eqnarray*}
\frac{\partial}{\partial\lambda}\hat{m}_{h}(\lambda) & = & \left.-\left(\|h\|_{T}^{2}+\lambda\frac{\partial^{2}}{\partial m^{2}}P^{v}\left(\hat{g}_{\lambda_{0}}+mh\right)+\lambda w\|h\|^{2}\right)^{-1}\left(\frac{\partial}{\partial m}P^{v}(\hat{g}_{\lambda_{0}}+mh)+w\langle h,\hat{g}_{\lambda_{0}}+mh\rangle\right)\right|_{m=\hat{m}_{\lambda}(\delta)}
\end{eqnarray*}


To bound $|\frac{\partial}{\partial\lambda}\hat{m}_{h}(\lambda)|$,
we bound each multiplicand.

\textbf{1st multiplicand}: Since penalty $P$ is convex (regardless
of the direction of $h$),

\begin{eqnarray*}
\left|\|h\|_{T}^{2}+\lambda\frac{\partial^{2}}{\partial m^{2}}P^{v}\left(\hat{g}_{\lambda_{0}}+mh\right)+\lambda w\|h\|^{2}\right|^{-1} & \le & \lambda^{-1}w^{-1}\|h\|^{-2}\\
 & \apprle & \lambda^{-1}w^{-1}n^{u}
\end{eqnarray*}


where the second inequality comes from our assumption that $\min_{h:P(h)=1}\|h\|^{-2}=n^{u}$.

\textbf{2nd multiplicand}: By definition of $\hat{g}_{\lambda_{0}}+\hat{m}_{h}(\lambda)h$,

\[
\lambda P^{v}(\hat{g}_{\lambda_{0}}+\hat{m}_{h}(\lambda)h)\le\frac{1}{2}\|y-g^{*}\|_{T}^{2}+\lambda P^{v}(g^{*})
\]


Hence

\begin{eqnarray*}
P^{v-1}(\hat{g}_{\lambda_{0}}+\hat{m}_{h}(\lambda)h) & \le & \left(\frac{1}{2\lambda_{0}}\|\epsilon\|_{T}^{2}+P^{v}(g^{*})\right)^{(v-1)/v}\\
 & \le & \left(\frac{n^{\tau_{min}}}{2}\|\epsilon\|_{T}^{2}+P^{v}(g^{*})\right)^{(v-1)/v}
\end{eqnarray*}


\textbf{3rd multiplicand}: Note that since $P$ is a semi-norm, then

\[
\left|P(\hat{g}_{\lambda}+mh)-P(\hat{g}_{\lambda})\right|\le mP(h)
\]


Therefore as we take $m\rightarrow0$, we have 
\[
\left|\frac{\partial}{\partial m}P(\hat{g}_{\lambda}+mh)\right|\le P(h)=1
\]


Also by Cauchy Schwarz and the assumption that $\sup_{g\in\mathcal{G}}\|g\|\le G$,
we have 
\begin{eqnarray*}
\left|\langle h,\hat{g}_{\lambda_{0}}+mh\rangle\right| & \le & \|h\|\|\hat{g}_{\lambda_{0}}+mh\|\le n^{p}G
\end{eqnarray*}


Conbining the above bounds, we have 
\begin{eqnarray*}
\left|\frac{\partial}{\partial\lambda}\hat{m}_{h}(\lambda)\right| & \apprle & n^{\tau_{min}+u}w^{-1}\left(v\left(\frac{n^{\tau_{min}}}{2}\|\epsilon\|_{T}^{2}+P^{v}(g^{*})\right)^{(v-1)/v}+wn^{p}G\right)\\
 & \le & n^{\kappa}w^{-1}v\left(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G\right)
\end{eqnarray*}


for some constant $\kappa$ that depends on $\tau_{min},u,p,v$.

By the mean value theorem, there is some $\alpha\in[0,1]$ s.t 

\begin{eqnarray*}
\|\hat{g}_{\lambda}(\cdot|D_{-k})-\hat{g}_{\lambda+\delta}(\cdot|D_{-k})\|_{Q} & = & \hat{m}_{h}(\lambda)\|h\|_{Q}\\
 & \le & n^{p}\delta\left|\frac{\partial}{\partial\lambda}\hat{m}_{h}(\lambda+\alpha\delta)\right|
\end{eqnarray*}


Combining the inequalities above, we get a bound on the covering number

\[
N\left(d,\hat{\mathcal{G}}(T,\epsilon_{T}),\|\cdot\|_{Q}\right)\apprle\frac{1}{d}n^{\kappa}w^{-1}v\left(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G\right)
\]


and the entropy 
\[
H\left(d,\hat{\mathcal{G}}(T,\epsilon_{T}),\|\cdot\|_{Q}\right)\apprle\log\left(\frac{v}{dw}\right)+\kappa\log n+\log\left(\|\epsilon\|_{T}^{2}+P^{v-1}(g^{*})+G\right)
\]



\subsubsection{Lemma 1}

Define the convex combination $\hat{\xi}_{\lambda}(x)=\frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\hat{g}_{\lambda}(x|D_{-k})$.
Suppose $P^{v}$ is convex. Then

\begin{eqnarray*}
 &  & \frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D)\|_{D}^{2}+\hat{\lambda}\left(P^{v}(\hat{g}_{\hat{\lambda}}(\cdot|D))+\frac{w}{2}\|\hat{g}_{\hat{\lambda}}(\cdot|D)\|^{2}\right)\\
 & \ge & \frac{1}{2}\|y-\hat{\xi}_{\hat{\lambda}}\|_{D}^{2}+\hat{\lambda}\left(P^{v}(\hat{\xi}_{\hat{\lambda}})+\frac{w}{2}\|\hat{\xi}_{\hat{\lambda}}\|^{2}\right)+\frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\langle y-\hat{\xi}_{\hat{\lambda}},\hat{\xi}_{\hat{\lambda}}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\rangle_{-k}
\end{eqnarray*}


(This is a version of the beginning of the proof for Thrm 1 in Chetverikov,
Chaterjee probably does the same thing.)


\subsubsection*{Proof}

\begin{eqnarray*}
 &  & \frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D)\|_{D}^{2}+\hat{\lambda}\left(P^{v}(\hat{g}_{\hat{\lambda}}(\cdot|D))+\frac{w}{2}\|\hat{g}_{\hat{\lambda}}(\cdot|D)\|^{2}\right)\\
 & = & \frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\left(\frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D)\|_{-k}^{2}+\hat{\lambda}\left(P^{v}(\hat{g}_{\hat{\lambda}}(\cdot|D))+\frac{w}{2}\|\hat{g}_{\hat{\lambda}}(\cdot|D)\|^{2}\right)\right)\\
 & \ge & \frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\left(\frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|_{-k}^{2}+\hat{\lambda}\left(P^{v}(\hat{g}_{\hat{\lambda}}(\cdot|D_{-k}))+\frac{w}{2}\|\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|^{2}\right)\right)\\
 & \ge & \frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\left(\frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|_{-k}^{2}+\hat{\lambda}\frac{w}{2}\|\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|^{2}\right)+\hat{\lambda}\left(P^{v}(\hat{\xi}_{\hat{\lambda}})+\frac{w}{2}\|\hat{\xi}_{\hat{\lambda}}\|^{2}\right)
\end{eqnarray*}


The second inequality follows by convexity of $P^{v}$ and $\|\cdot\|^{2}$.

Now note that 
\begin{eqnarray*}
\frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|_{-k}^{2} & = & \frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\frac{1}{2}\|y-\hat{\xi}_{\hat{\lambda}}+\hat{\xi}_{\hat{\lambda}}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|_{-k}^{2}\\
 & \ge & \frac{1}{2}\|y-\hat{\xi}_{\hat{\lambda}}\|_{D}^{2}+\frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\langle y-\hat{\xi}_{\hat{\lambda}},\hat{\xi}_{\hat{\lambda}}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\rangle_{-k}
\end{eqnarray*}



\subsubsection{Lemma 2}

Consider any $\xi\in\mathcal{G}$ and $\lambda$. Suppose $P^{v}$
is convex.

Then

\[
\frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-\xi\|_{D}^{2}\le\frac{1}{2}\|y-\xi\|_{D}^{2}-\frac{1}{2}\|y-\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}+\lambda\left(P^{v}(\xi)+\frac{w}{2}\|\xi\|^{2}\right)-\lambda\left(P^{v}(\hat{g}_{\lambda}(\cdot|D))+\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|^{2}\right)
\]


(This is a version of Lemma 10 in Chetverikov, which is based on Chaterjee.)


\subsubsection*{Proof}

Since $P$ is convex, then for $t\in(0,1)$, we have

\begin{eqnarray*}
 &  & \frac{1}{2}\|y-\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}+\lambda\left(P^{v}(\hat{g}_{\lambda}(\cdot|D))+\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|^{2}\right)\\
 & \le & \frac{1}{2}\|y-\left(t\xi+(1-t)\hat{g}_{\lambda}(\cdot|D)\right)\|_{D}^{2}+\lambda\left(P^{v}(t\xi+(1-t)\hat{g}_{\lambda}(\cdot|D))+\frac{w}{2}\|t\xi+(1-t)\hat{g}_{\lambda}(\cdot|D)\|^{2}\right)\\
 & \le & \frac{1}{2}\|y-\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}+t\langle y-\hat{g}_{\lambda}(\cdot|D),\hat{g}_{\lambda}(\cdot|D)-\xi\rangle_{D}+t^{2}\|\xi-\hat{g}_{\lambda}\|_{D}^{2}\\
 &  & +\lambda\left(tP^{v}(\xi)+(1-t)P^{v}\left(\hat{g}_{\lambda}(\cdot|D)\right)+t\frac{w}{2}\|\xi\|^{2}+(1-t)\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|^{2}\right)\\
 & \le & \frac{1}{2}\|y-\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}+t\langle y-\hat{g}_{\lambda}(\cdot|D),\hat{g}_{\lambda}(\cdot|D)-\xi\rangle_{D}+\frac{t^{2}}{2}\|\xi-\hat{g}_{\lambda}\|_{D}^{2}\\
 &  & +\lambda\left(tP^{v}(\xi)+(1-t)P^{v}\left(\hat{g}_{\lambda}(\cdot|D)\right)+t\frac{w}{2}\|\xi\|^{2}+(1-t)\|\hat{g}_{\lambda}(\cdot|D)\|^{2}\right)
\end{eqnarray*}


Rearranging terms, we obain
\[
\lambda\left(P^{v}\left(\hat{g}_{\lambda}(\cdot|D)\right)+\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|^{2}-P^{v}(\xi)-\frac{w}{2}\|\xi\|^{2}\right)\le\langle y-\hat{g}_{\lambda}(\cdot|D),\hat{g}_{\lambda}(\cdot|D)-\xi\rangle_{D}+\frac{t}{2}\|\xi-\hat{g}_{\lambda}\|_{D}^{2}
\]


Since this is true for any $t$, we have that

\[
\lambda\left(P^{v}\left(\hat{g}_{\lambda}(\cdot|D)\right)+\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|^{2}-P^{v}(\xi)-\frac{w}{2}\|\xi\|^{2}\right)\le\langle y-\hat{g}_{\lambda}(\cdot|D),\hat{g}_{\lambda}(\cdot|D)-\xi\rangle_{D}
\]


Thus
\begin{eqnarray*}
 &  & \frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-\xi\|_{D}^{2}\\
 & \le & \frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-y+y-\xi\|_{D}^{2}\\
 & = & \frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-y\|_{D}^{2}+\frac{1}{2}\|y-\xi\|_{D}^{2}-\langle\hat{g}_{\lambda}(\cdot|D)-y,\xi-y\rangle_{D}\\
 & = & -\frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-y\|_{D}^{2}+\frac{1}{2}\|y-\xi\|_{D}^{2}-\langle\hat{g}_{\lambda}(\cdot|D)-y,\xi-\hat{g}_{\lambda}(\cdot|D)\rangle_{D}\\
 & \le & -\frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-y\|_{D}^{2}+\frac{1}{2}\|y-\xi\|_{D}^{2}-\lambda\left(P^{v}\left(\hat{g}_{\lambda}(\cdot|D)\right)+\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|^{2}-P^{v}(\xi)-\frac{w}{2}\|\xi\|^{2}\right)
\end{eqnarray*}



\subsubsection{Lemma 3}

Suppose $X,T$ are random (or fixed) covariate values. $X$ and $T$
might overlap.

Suppose $\epsilon_{X}$ are independent sub-gaussian RVs with constants
$K$ and $\sigma$ (corresponding to $X$). Same fot $\epsilon_{T}$.
Again, $\epsilon_{T}$ and $\epsilon_{X}$ might have overlapping
samples.

Suppose the (random) function class $\mathcal{F}(T,\epsilon_{T})$
has its entropy uniformly bounded by $\psi(\cdot)$, as long as $\|\epsilon\|_{T}\le\sigma$:
\[
H\left(u,\mathcal{F}(T,\epsilon_{T}),\|\cdot\|_{X}\right)\le\psi(u)
\]


Suppose $\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\|f\|_{X}\le R$. 

Then there exists some $C$ s.t. for all $\delta$ s.t.

\[
\sqrt{|X|}\delta\ge C\left(\int_{0}^{R}\psi^{1/2}(u)du\vee1\right)
\]


we have for some constant $c$

\[
Pr_{\epsilon}\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\frac{\left|\langle\epsilon,f\rangle_{X}\right|}{\|f\|_{X}}\ge\delta\wedge\|\epsilon\|_{X}\le\sigma\wedge\|\epsilon\|_{T}\le\sigma\right)\le C\exp\left(-|X|\frac{\delta^{2}}{c^{2}}\right)
\]



\subsubsection*{Proof}

We use the peeling device. Let $S=\min\{s\in0,1,...:2^{s}\delta>R\}$.
Conditional on $\|\epsilon\|_{X}\le\sigma$ and $\|\epsilon\|_{T}\le\sigma$,
we have

\begin{eqnarray*}
 &  & Pr_{\epsilon}\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\frac{\left|\langle\epsilon,f\rangle_{X}\right|}{\|f\|_{X}}\ge\delta\right)\\
 & = & \int1\left[\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\frac{\left|\langle\epsilon,f\rangle_{X}\right|}{\|f\|_{X}}\ge\delta\right]dF(\epsilon)\\
 & = & \int\sum_{s=0}^{S}1\left[\sup_{f\in\mathcal{F}(T,\epsilon_{T}):2^{s}\delta\le\|f\|_{X}\le2^{s+1}\delta}\frac{\left|\langle\epsilon,f\rangle_{X}\right|}{\|f\|_{X}}\ge\delta\right]dF(\epsilon)\\
 & = & \sum_{s=0}^{S}Pr\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T}):2^{s}\delta\le\|f\|_{X}\le2^{s+1}\delta}\frac{\left|\langle\epsilon,f\rangle_{X}\right|}{\|f\|_{X}}\ge\delta\right)\\
 & \le & \sum_{s=0}^{S}Pr\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T}):\|f\|_{X}\le2^{s+1}\delta}\left|\langle\epsilon,f\rangle_{X}\right|\ge2^{s}\delta^{2}\right)
\end{eqnarray*}


In the last equality, we swap the order of integration and summation.
This is allowed under the assumption that the identity functions are
measurable, which should be okay.

To bound the summation, apply Lemma 6. For all 
\[
\sqrt{|X|}\delta\ge C\left(\int_{0}^{R}\psi^{1/2}(u)du\vee1\right)
\]


there is some constant $c$ s.t.

\begin{eqnarray*}
Pr_{\epsilon}\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\frac{\left|\langle\epsilon,f(\cdot|\epsilon)\rangle_{X}\right|}{\|f(\cdot|\epsilon)\|_{X}}\ge\delta\wedge\|\epsilon\|_{X}\le\sigma\wedge\|\epsilon\|_{T}\le\sigma\right) & \le & \sum_{s=0}^{S}C\exp\left(-|X|\frac{2^{2s}\delta^{4}}{C^{2}2^{2s+2}\delta^{2}}\right)\\
 & \le & c\exp\left(-|X|\frac{\delta^{2}}{c^{2}}\right)
\end{eqnarray*}



\subsubsection{Lemma 4}

Suppose $X,Z,T$ are random (or fixed) covariate values. $X,Z,T$
might overlap.

Suppose $\epsilon_{X}$ are independent sub-gaussian RVs with constants
$K$ and $\sigma$ (corresponding to $X$). Same fot $\epsilon_{T}$.
Again, $\epsilon_{T}$ and $\epsilon_{X}$ might have overlapping
samples.

Suppose the (random) function class $\mathcal{F}(T,\epsilon_{T})$
has its entropy uniformly bounded by $\psi(\cdot)$, as long as $\|\epsilon\|_{T}\le\sigma$:
\[
H\left(u,\mathcal{F}(T,\epsilon_{T}),\|\cdot\|_{X\cup Z}\right)\le\psi(u)
\]


Suppose $\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\|f\|_{X}\le R$. 

Then there exists some $C$ s.t. for all $\delta$ s.t.

\[
\left(\min\{|X|,|Z|\}\right)^{1/2}\delta\ge C\left(\int_{0}^{R}\psi^{1/2}(u)du\vee1\right)
\]


we have

\[
Pr\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\frac{\left|\left\Vert f\right\Vert _{X}^{2}-\left\Vert f\right\Vert _{Z}^{2}\right|}{\left\Vert f\right\Vert _{X\cup Z}}\ge\delta\wedge\|\epsilon\|_{T}\le\sigma\right)\le c\exp\left(-|X|\frac{\delta^{2}}{c^{2}R^{2}}\right)+c\exp\left(-|X|\frac{\delta^{2}}{c^{2}R^{2}}\right)
\]



\subsubsection*{Proof}

We use a symmetrization argument. Let $W_{i}$ be Rademacher-like
RV s.t. $Pr(W_{i}=1)=\frac{|T|}{|T|+|X|}$ and $Pr(W_{i}=-\frac{|T|+|X|}{|T|})=\frac{|X|}{|T|+|X|}$
(so $EW_{i}=0$). Note that $W_{i}$ are sub-gaussian by Hoeffding's
inequality. Conditional on $\|\epsilon\|_{T}\le\sigma$, we have 
\begin{eqnarray*}
 &  & Pr_{X,T}\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\frac{\left|\left\Vert f\right\Vert _{X}^{2}-\left\Vert f\right\Vert _{Z}^{2}\right|}{\left\Vert f\right\Vert _{X\cup Z}}\ge\delta\right)\\
 & = & Pr_{W,X,T}\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\frac{\left|\langle W,f^{2}\rangle_{X}+\langle W,f^{2}\rangle_{T}\right|}{\left\Vert f\right\Vert _{X\cup Z}}\ge\delta\right)\\
 & \le & Pr_{W,X,T}\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\frac{\left|\langle W,f^{2}\rangle_{X}\right|}{\left\Vert f\right\Vert _{X}}\ge\frac{\delta}{2}\frac{|X|}{|T|+|X|}\right)+Pr_{W,X,T}\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\frac{\left|\langle W,f^{2}\rangle_{Z}\right|}{\left\Vert f\right\Vert _{Z}}\ge\frac{\delta}{2}\frac{|T|}{|T|+|X|}\right)
\end{eqnarray*}


Therefore we apply Lemma 3. (Note that the RVs $W$ determine the
model class $\hat{\mathcal{G}}(D_{-\ell})$, but Lemma 3 allows for
this.) 

Then there is a constant $C$ such that for all 
\[
\left(\min\{|X|,|Z|\}\right)^{1/2}\delta\ge C\left(\int_{0}^{R}\psi^{1/2}(u)du\vee1\right)
\]


we have for some constant $c$ (depends on the ratio $|X|/|Z|$),

\[
Pr_{W,X,T}\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\frac{\left|\langle W,f^{2}\rangle_{X}\right|}{\left\Vert f\right\Vert _{X}}\ge\frac{\delta}{2}\frac{|X|}{|T|+|X|}\wedge\|\epsilon\|_{T}\le\sigma\right)\le c\exp\left(-|X|\frac{\delta^{2}}{c^{2}R^{2}}\right)
\]


and similarly 

\[
Pr_{W,X,T}\left(\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\frac{\left|\langle W,f^{2}\rangle_{Z}\right|}{\left\Vert f\right\Vert _{Z}}\ge\frac{\delta}{2}\frac{|T|}{|T|+|X|}\wedge\|\epsilon\|_{T}\le\sigma\right)\le c\exp\left(-|Z|\frac{\delta^{2}}{c^{2}R^{2}}\right)
\]



\subsubsection{Lemma 5}

Let $\hat{\lambda}$ be chosen by CV. Let $\tilde{\lambda}$ be the
oracle. Suppose $\sup_{g\in\mathcal{G}}\|g\|_{D}\le G$. Then with
high probability,

\[
\sqrt{\sum_{k=1}^{K}\|\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})-g_{\tilde{\lambda}}(\cdot|D_{-k})\|_{k}^{2}}\apprle\left(\frac{1+\log4\sigma^{2}-\log w+\kappa\log n}{n}\right)^{1/2}+\sqrt{\sum_{k=1}^{K}\|\hat{g}_{\tilde{\lambda}}(\cdot|D_{-k})-g^{*}\|_{k}^{2}}
\]



\subsubsection*{Proof}

The basic inequality gives us

\begin{eqnarray*}
 &  & \sum_{k=1}^{K}\|\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})-\hat{g}_{\tilde{\lambda}}(\cdot|D_{-k})\|_{k}^{2}\\
 & \le & 2\left|\sum_{k=1}^{K}\left(\epsilon,\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})-\hat{g}_{\tilde{\lambda}}(\cdot|D_{-k})\right)_{k}\right|+2\left|\sum_{k=1}^{K}\left(g^{*}-\hat{g}_{\tilde{\lambda}}(\cdot|D_{-k}),\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})-\hat{g}_{\tilde{\lambda}}(\cdot|D_{-k})\right)_{k}\right|
\end{eqnarray*}


We bound the empirical process term by a standard peeling argument
(omitted). That is, one can show that for all 
\[
\delta\ge C\left(\frac{1+\log4\sigma^{2}-\log w+\kappa\log n}{\min_{k=1:K}\{n_{k}\}}\right)^{1/2}
\]


we have that for every $k$, 
\[
Pr\left(\frac{\left|\left(\epsilon,g_{\hat{\lambda}}(\cdot|D_{-k})-g_{\tilde{\lambda}}(\cdot|D_{-k})\right)_{k}\right|}{\|g_{\hat{\lambda}}(\cdot|D_{-k})-g_{\tilde{\lambda}}(\cdot|D_{-k})\|_{k}}\ge\delta\wedge\|\epsilon\|_{k}\le2\sigma\right)\le c\exp\left(-n_{k}\frac{\delta^{2}}{c^{2}}\right)
\]


Hence for constants $c_{k}$ (which depend on $n_{k}/n$), 
\begin{eqnarray*}
 &  & Pr\left(\frac{\left|\sum_{k=1}^{K}\left(\epsilon,g_{\hat{\lambda}}(\cdot|D_{-k})-g_{\tilde{\lambda}}(\cdot|D_{-k})\right)_{k}\right|}{\sqrt{\sum_{k=1}^{K}\|g_{\hat{\lambda}}(\cdot|D_{-k})-g_{\tilde{\lambda}}(\cdot|D_{-k})\|_{k}^{2}}}\ge\delta\wedge\|\epsilon\|_{D}\le2\sigma\right)\\
 & \le & \sum_{k=1}^{K}Pr\left(\frac{\left|\left(\epsilon,g_{\hat{\lambda}}(\cdot|D_{-k})-g_{\tilde{\lambda}}(\cdot|D_{-k})\right)_{k}\right|}{\|g_{\hat{\lambda}}(\cdot|D_{-k})-g_{\tilde{\lambda}}(\cdot|D_{-k})\|_{k}}\ge\frac{\delta c_{k}}{k}\wedge\|\epsilon\|_{D}\le2\sigma\right)\\
 & \le & c\exp\left(-\min_{k=1:K}\{n_{k}\}\frac{\delta^{2}}{c^{2}}\right)
\end{eqnarray*}


Also, by Cauchy-Schwarz, we have
\[
\frac{\left|\sum_{k=1}^{K}\left(g^{*}-g_{\tilde{\lambda}}(\cdot|D_{-k}),g_{\hat{\lambda}}(\cdot|D_{-k})-g_{\tilde{\lambda}}(\cdot|D_{-k})\right)_{k}\right|}{\sqrt{\sum_{k=1}^{K}\|g_{\hat{\lambda}}(\cdot|D_{-k})-g_{\tilde{\lambda}}(\cdot|D_{-k})\|_{k}^{2}}}\le\sqrt{\sum_{k=1}^{K}\|g^{*}-g_{\tilde{\lambda}}(\cdot|D_{-k})\|_{k}^{2}}
\]


Hence the result follows.


\subsubsection{Lemma 6}

Let $X$ be $n$ covariate values (potentially randomly drawn). It
is also possible for $T$ and $X$ to contain overlapping samples.

Suppose $\epsilon_{X}$ is a set of $n$ independent sub-gaussian
RVs with constants $K$ and $\sigma$ (corresponding to $X$). Suppose
$\epsilon_{T}$ are also independent sub-gaussian RVs with constants
$K$ and $\sigma$ (corresponding to $T$). Again, it is possible
for$\epsilon_{T}$ and $\epsilon_{X}$ can have overlapping samples. 

Suppose $\sup_{f\in\mathcal{F}(T,\epsilon_{T})}\|f\|_{X}\le R$.

Suppose we have (random) function classes $\mathcal{F}(T,\epsilon_{T})$
with entropy $H\left(\delta,\mathcal{F}(T,\epsilon_{T}),\|\cdot\|_{X}\right)$.
Suppose that there is a universal bound on the entropy if $\|\epsilon_{T}\|\le\sigma$:
\[
H\left(u,\mathcal{F}(T,\epsilon_{T}),\|\cdot\|_{X}\right)\le\psi(u)
\]


Then there exists some $C$ dependent only on $K,\sigma$ s.t. for
all

\[
\sqrt{n}\delta\ge C\left(\int_{0}^{R}\psi^{1/2}(u)du\vee R\right)
\]


we have

\[
Pr_{\epsilon}\left(\sup_{f_{\theta}(\cdot|\epsilon)\in\mathcal{F}(T,\epsilon_{T})}\left|\langle\epsilon,f_{\theta}(\cdot|\epsilon_{T})\rangle_{X}\right|\ge\delta\wedge\|\epsilon\|_{X}\le\sigma\wedge\|\epsilon\|_{T}\le\sigma\right)\le C\exp\left(-n\frac{\delta^{2}}{C^{2}R^{2}}\right)
\]



\subsubsection*{Proof}

Proof closely follows Lemma 3.2 from Vandegeer.

For a given set of RVs $\epsilon$, let $\{f_{j}^{s}(\cdot|\epsilon)\}_{j=1}^{N_{s}}$
be the $2^{-s}R$-covering set of $\mathcal{F}(T,\epsilon)$ where
$N_{s}=N_{s}(2^{-s}R,\mathcal{F}(T,\epsilon),\|\cdot\|_{X})\le\exp\left(\psi(2^{-s}R)\right)$.
Let $S=\min\{s:2^{-s}R\le\delta/2\sigma\}$. We can write $f_{\theta}^{S}(\cdot|\epsilon)=\sum_{s=1}^{S}f_{\theta}^{s}(\cdot|\epsilon)-f_{\theta}^{s-1}(\cdot|\epsilon)$
where $f_{\theta}^{0}(\cdot|\epsilon)=0$.

Also, consider a set of constants $\eta_{s}$ s.t. $\sum_{s=1}^{S}\eta_{s}\le1$.

Then as long as $\|\epsilon\|_{X}\le\sigma$ and $\|\epsilon\|_{T}\le\sigma$,
we have

\begin{eqnarray*}
 &  & Pr_{\epsilon}\left(\sup_{\theta\in\mathcal{F}(T,\epsilon_{T})}\left|\langle\epsilon,f_{\theta}(\cdot|\epsilon)\rangle_{X}\right|\ge\delta\right)\\
 & = & \int1\left[\sup_{\theta\in\mathcal{F}(T,\epsilon_{T})}\left|\langle\epsilon,f_{\theta}(\cdot|\epsilon)\rangle_{X}\right|\ge\delta/2\right]dF(\epsilon)\\
 & = & \int1\left[\sup_{\theta\in\mathcal{F}(T,\epsilon_{T})}\left|\left\langle \epsilon,f_{\theta}(\cdot|\epsilon)-f_{\theta}^{S}(\cdot|\epsilon)+\sum_{s=1}^{S}f_{\theta}^{s}(\cdot|\epsilon)-f_{\theta}^{s-1}(\cdot|\epsilon)\right\rangle _{X}\right|\ge\delta/2\right]dF(\epsilon)\\
 & \le & \int1\left[\sup_{\theta\in\mathcal{F}(T,\epsilon_{T})}\left|\left\langle \epsilon,f_{\theta}-f_{\theta}^{S}(\cdot|\epsilon)\right\rangle _{X}\right|\ge\delta/2\right]+\sum_{s=1}^{S}1\left[\sup_{\theta\in\mathcal{F}(T,\epsilon_{T})}\left|\left\langle \epsilon,f_{\theta}^{s}(\cdot|\epsilon)-f_{\theta}^{s-1}(\cdot|\epsilon)\right\rangle _{X}\right|\ge\delta\eta_{s}/2\right]dF(\epsilon)\\
 & = & Pr_{\epsilon}\left(\sup_{\theta\in\mathcal{F}(T,\epsilon_{T})}\left|\left\langle \epsilon,f_{\theta}-f_{\theta}^{S}(\cdot|\epsilon)\right\rangle _{X}\right|\ge\delta/2\right)+\sum_{s=1}^{S}Pr_{\epsilon}\left(\sup_{\theta\in\mathcal{F}(T,\epsilon_{T})}\left|\left\langle \epsilon,f_{\theta}^{s}(\cdot|\epsilon)-f_{\theta}^{s-1}(\cdot|\epsilon)\right\rangle _{X}\right|\ge\delta\eta_{s}/2\right)
\end{eqnarray*}


In the last equality, we swap the order of the summation and integration,
which is allowed under the assumption that identity functions are
measurable (I think this is the measurability assumption required
here) (If $\epsilon$ has a continuous probability measure, I think
this holds).

We know that the first summand is zero since by Cauchy-Schwarz,

\begin{eqnarray*}
\left|\langle\epsilon,f_{\theta}-f_{\theta}^{S}(\cdot|\epsilon)\rangle_{X}\right| & \le & \sigma\|f_{\theta}-f_{\theta}^{S}(\cdot|\epsilon)\|_{X}\\
 & \le & \delta/2
\end{eqnarray*}


Also, for any $\epsilon$, we must have$\|f_{\theta}^{s}(\cdot|\epsilon)-f_{\theta}^{s-1}(\cdot|\epsilon)\|\le3(2^{-s}R)$.
Furthermore, since$\epsilon$ is sub-gaussian,

\begin{eqnarray*}
Pr_{\epsilon}\left(\sup_{\theta\in\mathcal{F}(T,\epsilon_{T})}\left|\left\langle \epsilon,f_{\theta}^{s}(\cdot|\epsilon)-f_{\theta}^{s-1}(\cdot|\epsilon)\right\rangle _{X}\right|\ge\delta\eta_{s}/2\right) & \le & \exp\left(2\psi(2^{-s}R)-C\frac{n(\delta/2)^{2}\eta_{s}^{2}}{9(2^{-2s}R^{2})}\right)
\end{eqnarray*}


Now choose $\eta_{s}$ as Vandegeer does in Lemma 3.2. After a lot
of algebraic massaging, we get that for some constants $C_{1},C_{2}$

\[
Pr_{\epsilon}\left(\sup_{f_{\theta}\in\mathcal{F}(T,\epsilon_{T})}\left|\langle\epsilon,f_{\theta}\rangle_{X}\right|\ge\delta\wedge\|\epsilon\|_{X}\le\sigma\wedge\|\epsilon\|_{T}\le\sigma\right)\le C_{1}\exp\left(-n\frac{\delta^{2}}{C_{2}^{2}R^{2}}\right)
\]



\subsection{Easy algebra notes}


\subsubsection{Easy Lemma 1}

Suppose 
\[
\frac{\left|\left\Vert f\right\Vert _{X}^{2}-\left\Vert f\right\Vert _{Z}^{2}\right|}{\left\Vert f\right\Vert _{X\cup Z}}\le\delta
\]


then
\[
\left|\left\Vert f\right\Vert _{X}-\left\Vert f\right\Vert _{Z}\right|\le\delta
\]



\subsubsection*{Proof}

We know 
\begin{eqnarray*}
\sqrt{\left\Vert f\right\Vert _{X}^{2}+\left\Vert f\right\Vert _{Z}^{2}}\left|\left\Vert f\right\Vert _{X}-\left\Vert f\right\Vert _{Z}\right| & \le & \left|\left\Vert f\right\Vert _{X}+\left\Vert f\right\Vert _{Z}\right|\left|\left\Vert f\right\Vert _{X}-\left\Vert f\right\Vert _{Z}\right|\\
 & = & \left|\left\Vert f\right\Vert _{X}^{2}-\left\Vert f\right\Vert _{Z}^{2}\right|\\
 & \le & \delta\sqrt{\frac{|X|}{|X|+|Z|}\left\Vert f\right\Vert _{X}^{2}+\frac{|Z|}{|X|+|Z|}\left\Vert f\right\Vert _{Z}^{2}}\\
 & \le & \delta\sqrt{\left\Vert f\right\Vert _{X}^{2}+\left\Vert f\right\Vert _{Z}^{2}}
\end{eqnarray*}


hence 
\[
\left|\left\Vert f\right\Vert _{X}-\left\Vert f\right\Vert _{Z}\right|\le\delta
\]



\section{Corollaries}


\subsection{Convergence Rate Equivalence between the original regression problem
and the perturbed ridge problem}

Consider any $\lambda$.

Suppose the original regression problem is 
\[
\hat{g}=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\|y-g\|_{D}^{2}+\lambda P^{v}(g)
\]


and the new perturbed ridge problem is 
\[
\hat{f}=\arg\min_{f\in\mathcal{G}}\frac{1}{2}\|y-f\|_{D}^{2}+\lambda\left(P^{v}(f)+\frac{w}{2}\|f\|^{2}\right)
\]


Let $g^{*}$ be the true minimizer 
\[
g^{*}=\arg\min_{g\in\mathcal{G}}E\left[\|y-g\|^{2}\right]
\]


Suppose there are constants $K_{0},K_{1}>0$ s.t. 
\[
\frac{w}{2}\|g^{*}\|^{2}\le K_{0}P^{v}(g)+K_{1}
\]


Then the rate of convergence of 
\[
\left\Vert \hat{f}-g^{*}\right\Vert _{D}\apprle\left\Vert \hat{g}-g^{*}\right\Vert _{D}
\]


That is, the optimal rate of convergence determined by the oracle
$\tilde{\lambda}$ is preserved under the new perturbed ridge regression
problem.


\subsubsection*{Proof}

By definition, 
\begin{eqnarray*}
\frac{1}{2}\|y-\hat{f}\|_{D}^{2}+\lambda\left(P^{v}(\hat{f})+\frac{w}{2}\|\hat{f}\|^{2}\right) & \le & \frac{1}{2}\|y-g^{*}\|_{D}^{2}+\lambda\left(P^{v}(g^{*})+\frac{w}{2}\|g^{*}\|^{2}\right)\\
 & \le & \frac{1}{2}\|y-g^{*}\|_{D}^{2}+\lambda(1+K_{0})P^{v}(g^{*})+\lambda K_{1}
\end{eqnarray*}


Therefore
\[
\frac{1}{2}\|y-\hat{f}\|_{D}^{2}+\lambda P^{v}(\hat{f})\le\frac{1}{2}\|y-g^{*}\|_{D}^{2}+\lambda(1+K_{0})P^{v}(g^{*})+\lambda K_{1}
\]


Notice that this inequality is very similar to the inequality from
the original regression problem

\[
\frac{1}{2}\|y-\hat{f}\|_{D}^{2}+\lambda P^{v}(\hat{f})\le\frac{1}{2}\|y-g^{*}\|_{D}^{2}+\lambda P^{v}(g^{*})
\]


Therefore the arguments to bound the convergence rate of $\left\Vert \hat{g}-g^{*}\right\Vert _{D}$
should give the same convergence rate for $\left\Vert \hat{f}-g^{*}\right\Vert _{D}$.
(Example: refer to Thrm 10.2 in Vandegeer)


\subsection{Regression problems with smooth-almost-everywhere penalty functions}

If the regularization functions contain smooth-almost-everywhere penalty
functions, we still have the same convergence rate. To prove this,
we need an additional assumption similar to Condition 1 in the hillclimbing
paper. However since our function classes may be infinite-dimensional,
we need to generalize the definitions/condition:

Let $L_{T}(g,\lambda)$ be the penalized training criterion
\[
L_{T}(g,\lambda)=\frac{1}{2}\|y-g\|_{n}^{2}+\lambda\left(P^{v}(g)+\frac{w}{2}\|g\|^{2}\right)
\]


\textbf{Definition 1:} The differentiable space of $L_{T}$ at $g$
are the functions along which the directional derivative of $L_{T}$
exists: 
\[
\Omega^{L_{T}}(g)=\left\{ h:\lim_{r\rightarrow0}\frac{L_{T}(g+rh)+L_{T}(g)}{r}\mbox{ exists}\right\} 
\]


\textbf{Definition 2:} $S_{\lambda_{0}}$ is a local optimality space
of $L_{T}$ at $\lambda_{0}$ if there is a neighborhood containing
$\lambda_{0}$ s.t. for all $\lambda\in W$ 
\[
\arg\min_{g\in\mathcal{G}}L_{T}(g,\lambda)=\arg\min_{g\in S_{\lambda_{0}}}L_{T}(g,\lambda)
\]


\textbf{Assumption A:} For almost every $\lambda$, the differentiable
space $\Omega^{L_{T}}(\hat{g}(\lambda))$ contains the local optimality
space $S_{\lambda_{0}}$.

Now consider the function class 
\[
\hat{\mathcal{G}}(T,\epsilon_{T})=\left\{ \hat{g}_{\lambda}(\cdot|T,\epsilon_{T})=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\|y-g\|_{T}^{2}+\lambda\left(P^{v}(g)+\frac{w}{2}\|g\|^{2}\right):\lambda\in\Lambda\right\} 
\]


We have all the same assumptions as in Lemma 0, except that $P^{v}$
can be smooth almost everywhere. Also, we have assumed ``Assumption
A'' above. Then the same entropy bound holds:

\[
H\left(d,\hat{\mathcal{G}}(T,\epsilon_{T}),\|\cdot\|_{Q}\right)\apprle\log\left(\frac{1}{dw}\right)+\kappa\log n+\log\|\epsilon\|_{T}^{2}
\]


where $\kappa$ depends on $\tau_{min},v,u,p$.


\subsubsection*{Proof}

Using a generalized verion of Theorem 1 in the hillclimbing paper
(we should probably show this), we have that $L_{T}(\hat{g}(\lambda),\lambda)$
is continuously differentiable with respect to $\lambda$ for almost
every $\lambda$. Let $S$ be the set of knots at which $\frac{\partial}{\partial\lambda}L_{T}(\hat{g}(\lambda),\lambda)|_{\lambda=s}$
does not exist.

We are interested in bounding $\|\hat{g}_{\lambda}-\hat{g}_{\lambda+\delta}\|_{Q}$.
Suppose knots $s_{1}<s_{2}<...<s_{L}$ (where $L$ might equal infinity)
are in between between $\lambda$ and $\lambda+\delta$.

As done in Lemma 0, we can bound 
\[
\|\hat{g}_{s_{i}}-\hat{g}_{s_{i+1}}\|_{Q}\le(s_{i+1}-s_{i})n^{\kappa}w^{-1}v\left(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G\right)
\]


for some constant $\kappa$ (for details, go to Lemma 0).

Then

\begin{eqnarray*}
 &  & \|\hat{g}_{\lambda}-\hat{g}_{\lambda+\delta}\|_{Q}\\
 & \le & \|g_{\lambda}-g_{s_{1}}\|_{Q}+\|g_{\lambda+\delta}-g_{s_{L}}\|_{Q}+\sum_{i=1}^{L-1}\|g_{s_{i}}-g_{s_{i+1}}\|_{Q}\\
 & \le & n^{\kappa}w^{-1}v\left(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G\right)\left(s_{1}-\lambda+\lambda+\delta-s_{L}+\sum_{i=1}^{L-1}(s_{i+1}-s_{i})\right)\\
 & = & \delta n^{\kappa}w^{-1}v\left(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G\right)
\end{eqnarray*}


Therefore we have the same entropy bound as before.


\subsection{Multiple Regularization Parameters}

Suppose we are fitting model parameters $\theta$ but we'd like to
apply a separate penalty to $J$ partitions of the model parameters:
$P_{j}(\theta_{j})$ for $j=1,...,J$. Let $\boldsymbol{\lambda}=(\lambda_{1},...,\lambda_{J})$
be their corresponding penalty parameters. We suppose that $\Lambda$
is the box $[\lambda_{min},\lambda_{max}]^{J}$ where $\lambda_{min}=O_{p}(n^{-\tau_{min}})$
and $\lambda_{max}=O_{p}(1)$.

That is, the training criterion is now

\[
\hat{g}(\boldsymbol{\lambda}|D_{-k})=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\|y-g\|_{D_{-k}}^{2}+\sum_{j=1}^{J}\lambda_{j}\left(P_{j}^{v}(g)+\frac{w}{2}\|g\|^{2}\right)
\]


For simplicity, suppose the problem is smooth. (One can probably use
the same arguments to extend this to the case where the problem is
smooth almost everywhere.)

We show that the entropy is off by a constant from Lemma 0: 

\[
H\left(d,\hat{\mathcal{G}}(T,\epsilon_{T}),\|\cdot\|_{Q}\right)\le J\left(\log\frac{1}{d}+\kappa\log n+\log\left(\frac{v}{wJ}\left(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G\right)\right)\right)
\]


Hence the convergence rate is essentially increased by a factor of
$J^{1/2}$:

\[
\|\hat{g}_{\hat{\lambda}}(\cdot|D)-g^{*}\|_{D}\apprle\sqrt{\sum_{k=1}^{K}\left\Vert g^{*}-\hat{g}_{\tilde{\lambda}}(\cdot|D_{-k})\right\Vert _{k}^{2}}+G\left(\frac{J\left(1+\kappa\log n+\log\left(\frac{v}{wJ}\left(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G\right)\right)\right)}{\min_{k=1:K}\{n_{k}\}}\right)^{1/2}
\]



\subsubsection*{Proof for the entropy of the function class}

We first determine $\nabla_{\lambda_{j}}\hat{m}_{h}(\boldsymbol{\lambda})$
by implicit differentiation. The equations get bulky, but the logic
is straightforward.

Use the same directional derivative technique in Lemma 0. Since $\hat{m}_{h}(\boldsymbol{\lambda})$
is a local minima,

\[
\left.-\langle h,y-(\hat{g}_{\lambda_{0}}+mh)\rangle_{T}+\sum_{j=1}^{J}\lambda_{j}\left(\frac{\partial}{\partial m}P_{j}^{v}(\hat{g}_{\lambda_{0}}+mh)+w\langle h,\hat{g}_{\lambda_{0}}+mh\rangle\right)\right|_{m=\hat{m}_{h}(\boldsymbol{\lambda})}=0
\]


By implicit differentiation wrt $\lambda_{\ell}$ for $\ell=1:J$,
we have 
\begin{eqnarray*}
\nabla_{\lambda_{\ell}}\hat{m}_{h}(\boldsymbol{\lambda}) & = & \left.-\left(\|h\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}\frac{\partial^{2}}{\partial m^{2}}P_{j}^{v}\left(\hat{g}_{\lambda_{0}}+mh\right)+w\|h\|^{2}\sum_{j=1}^{J}\lambda_{j}\right)^{-1}\left(\frac{\partial}{\partial m}P_{\ell}^{v}(\hat{g}_{\lambda_{0}}+mh)+w\langle h,\hat{g}_{\lambda_{0}}+mh\rangle\right)\right|_{m=\hat{m}_{h}(\boldsymbol{\lambda})}
\end{eqnarray*}


Now we can bound $\|\hat{g}(\boldsymbol{\lambda})-\hat{g}(\boldsymbol{\lambda}+\boldsymbol{\delta})\|$.
Define $h=c\left(\hat{g}(\boldsymbol{\lambda})-\hat{g}(\boldsymbol{\lambda}+\boldsymbol{\delta})\right)$
where $c$ is some scaling constant s.t. $P(h)=1$. Following the
same logic given in Lemma 0, we get

\[
\left|\nabla_{\lambda_{\ell}}\hat{m}_{h}(\boldsymbol{\lambda})\right|\le w^{-1}J^{-1}n^{\tau_{min}+v}v\left(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G\right)
\]


Therefore, applying the mean value theorem, we get that for any vector
$\boldsymbol{\delta}$ there is some $\alpha\in[0,1]$ s.t. 
\begin{eqnarray*}
\|\hat{g}(\boldsymbol{\lambda})-\hat{g}(\boldsymbol{\lambda}+\boldsymbol{\delta})\|_{Q} & = & \left\Vert \hat{m}_{h}(\boldsymbol{\lambda}+\boldsymbol{\delta})h\right\Vert _{Q}\\
 & \le & n^{p}\hat{m}_{h}(\boldsymbol{\lambda}+\boldsymbol{\delta})\\
 & = & n^{p}\left|\langle\boldsymbol{\delta},\nabla_{\boldsymbol{\lambda}}\hat{m}_{h}(\boldsymbol{\lambda}+\alpha\boldsymbol{\delta})\rangle\right|\\
 & \le & \|\boldsymbol{\delta}\|w^{-1}J^{-1}n^{\kappa}v\left(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G\right)
\end{eqnarray*}


for some constant $\kappa$ dependent on $v,p,\tau_{min}$.

Hence we can form a $d$-covering set for $\hat{\mathcal{G}}(T,\epsilon_{T})$
by covering the box $\Lambda=[\lambda_{min},\lambda_{max}]^{J}$ with
balls of radius $dwJn^{-\kappa}\left(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G\right)^{-1}$.
Hence the $u-$covering number is 
\[
N\left(d,\hat{\mathcal{G}}(T,\epsilon_{T}),\|\cdot\|_{Q}\right)=d^{-J}\left(\frac{n^{\kappa}v}{wJ}\left(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G\right)\right)^{J}
\]


Hence

\[
H\left(d,\hat{\mathcal{G}}(T,\epsilon_{T}),\|\cdot\|_{Q}\right)\le J\log\frac{1}{d}+J\kappa\log n+J\log\left(\frac{v}{wJ}(\|\epsilon\|_{T}^{2(v-1)/v}+P^{v-1}(g^{*})+G)\right)
\]

\end{document}
