%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amssymb}
\usepackage{wasysym}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\section{K-Fold Cross-Validation}

Consider the joint optimization problem to find the best regularization
parameter $\lambda$ in $\Lambda$ via $K$-fold cross validation.
Let $D$ be the entire dataset. For $k=1,...,K$, let $D_{k}$ represent
the $k$th fold and $D_{-k}$ denote all the folds minus the $k$th
fold. For a given $\lambda$, train over $D_{-k}$ and then validate
over $D_{k}$. Let the number of observations for each fold be $n_{k}$
and let the total number of observations be $n$.

Let $\|h\|_{k}^{2}=\frac{1}{n_{k}}\sum_{i\in D_{k}}h(x_{i})^{2}$
and similarly for $\|h\|_{-k}^{2}$ for the set $D_{-k}$ and $\|h\|_{D}^{2}$
for the set $D$. Let $\left(h,g\right)_{k}=\frac{1}{n_{k}}\sum_{i\in D_{k}}h(x_{i})g(x_{i})$
and $(h,g)_{-k}$ for the set $D_{-k}$ and $(h,g)_{D}$ for the set
$D$.

Let us define

\[
\hat{\lambda}=\arg\min_{\lambda\in\Lambda}\frac{1}{2}\|y-\hat{g}_{\lambda}(\cdot|D_{-k})\|_{k}^{2}
\]
\[
\hat{g}(\lambda|D_{-k})=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\|y-g\|_{-k}^{2}+\lambda\left(P(g)+\frac{w}{2}\|g\|_{-k}^{2}\right)
\]


The $K$-fold CV model is 
\[
\hat{g}(\lambda|D)=\arg\min_{g\in\mathcal{G}}\frac{1}{2}\|y-g\|_{D}^{2}+\lambda\left(P(g)+\frac{w}{2}\|g\|_{D}^{2}\right)
\]


Let the range of $\Lambda=[\lambda_{min},\lambda_{max}]$. Both limits
can grow and shrink at any polynomial rate, e.g. $\mbox{\ensuremath{\lambda}}_{max}=O_{P}(1)$
and $\mbox{\ensuremath{\lambda}}_{min}=O_{P}(n^{-\tau_{min}})$. 

We show that 
\[
\|\hat{g}_{\hat{\lambda}}(\cdot|D)-g^{*}\|_{D}\le?+\sum_{k=1}^{K}\|g^{*}-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}+\|\hat{g}_{\tilde{\lambda}}(\cdot|D)-g^{*}\|_{D}
\]



\subsubsection*{Notation}

$a\lesssim b$ means that $a\le Cb+c$ where $C>0,c$ are constants
independent of $n$.


\section{Proof}

The proof is based on two main ideas.

First, we bound the error of the retrained $K$-fold CV model by a
convex combination of the $K$ trained models from each fold.

Second, the additional ridge regression penalty allows us to bound
the entropy of $\hat{\mathcal{G}_{k}}=\left\{ \hat{g}_{\lambda}(\cdot|D_{-k}):\lambda\in\Lambda\right\} $.
Once this is complete, we can use results similar to Vandegeer that
bound the Rademacher process 
\[
\sum_{i=1}^{n}W_{i}\hat{g}_{\lambda}(x_{i}|D_{-k})
\]
and the empirical process

\[
\sum_{i=1}^{n}\epsilon_{i}\hat{g}_{\lambda}(x_{i}|D_{-k})
\]



\subsubsection*{Step 1:}

Define the convex combination 
\[
\hat{\xi}_{\lambda}(x)=\frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\hat{g}_{\lambda}(x|D_{-k})
\]


By the triangle inequality,
\begin{eqnarray*}
\|\hat{g}_{\hat{\lambda}}(\cdot|D)-g^{*}\|_{D} & \le & \|\hat{g}_{\hat{\lambda}}(\cdot|D)-\xi_{\hat{\lambda}}\|_{D}+\|\xi_{\hat{\lambda}}-\xi_{\tilde{\lambda}}\|_{D}+\|\xi_{\tilde{\lambda}}-\hat{g}_{\tilde{\lambda}}(\cdot|D)\|_{D}+\|\hat{g}_{\tilde{\lambda}}(\cdot|D)-g^{*}\|_{D}\\
 & \le & \|\hat{g}_{\hat{\lambda}}(\cdot|D)-\xi_{\hat{\lambda}}\|_{D}+\|\xi_{\tilde{\lambda}}-\hat{g}_{\tilde{\lambda}}(\cdot|D)\|_{D}+\frac{1}{K-1}\sum_{k=1}^{K}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{D}+\|\hat{g}_{\tilde{\lambda}}(\cdot|D)-g^{*}\|_{D}
\end{eqnarray*}


We bound the first two terms in step 2. We bound the third term in
step 3.


\subsubsection*{Step 2:}

Adding the two inequalities from Lemma 1 and 2, we have

\[
\|\hat{g}_{\lambda}(\cdot|D)-\hat{\xi}_{\lambda}\|_{D}^{2}\le\frac{1}{K-1}\sum\frac{n-n_{k}}{n}\left(\left|\langle\epsilon,\hat{\xi}_{\lambda}-\hat{g}_{\lambda}(\cdot|D_{-k})\rangle_{-k}\right|+\left|\langle g^{*}-\hat{\xi}_{\lambda},\hat{\xi}_{\lambda}-\hat{g}_{\lambda}(\cdot|D_{-k})\rangle_{k}\right|\right)
\]


The first term is bounded by

\begin{eqnarray*}
\left|\langle\epsilon,\hat{\xi}_{\lambda}-\hat{g}_{\lambda}(\cdot|D_{-k})\rangle_{-k}\right| & = & \left|\langle\epsilon,\frac{1}{K-1}\sum_{\ell=1}^{K}\frac{n-n_{\ell}}{n}\hat{g}_{\lambda}(\cdot|D_{-\ell})-\hat{g}_{\lambda}(\cdot|D_{-k})\rangle_{-k}\right|\\
 & \lesssim & \sum_{\ell=1}^{K}\left|\langle\epsilon,\hat{g}_{\lambda}(\cdot|D_{-\ell})-g^{*}\rangle_{-k}\right|
\end{eqnarray*}


By Lemma 4, we know that $\sup_{\lambda}\|\hat{g}_{\lambda}(\cdot|D_{-\ell})-g^{*}\|\le F\sigma$.
Hence by Lemma 3, we have that for all $\delta\ge CR\sqrt{J}\left(\frac{1+\log(C/\sqrt{w})+\kappa\log n}{n-n_{k}}\right)^{1/2}$,
we have for all $\ell=1:K$ and $k=1:K$, 
\[
Pr\left(\sup_{\lambda}\left|\langle\epsilon,\hat{g}_{\lambda}(\cdot|D_{-\ell})-g^{*}\rangle_{-k}\right|\ge\delta\wedge\|\epsilon\|_{-k}\le2\sigma\right)\le C\exp\left(-(n-n_{k})\frac{\delta^{2}}{C^{2}R^{2}}\right)
\]


The second term is bounded by 
\begin{eqnarray*}
 &  & \left|\langle g^{*}-\hat{\xi}_{\lambda},\hat{\xi}_{\lambda}-\hat{g}_{\lambda}(\cdot|D_{-k})\rangle_{k}\right|\\
 & \le & \left|\left\langle \frac{1}{K-1}\sum_{\ell=1}^{K}\frac{n-n_{\ell}}{n}\left(g^{*}-\hat{g}_{\lambda}(\cdot|D_{-\ell})\right),\frac{1}{K-1}\sum_{\ell=1}^{K}\frac{n-n_{\ell}}{n}\left(g^{*}-\hat{\xi}_{\lambda}\right)+\hat{g}_{\lambda}(\cdot|D_{-k})-g^{*}\right\rangle _{k}\right|\\
 & \apprle & \frac{1}{K-1}\sum_{\ell=1}^{K}\frac{n-n_{k}}{n}\left\Vert g^{*}-\hat{g}_{\lambda}(\cdot|D_{-\ell})\right\Vert _{-k}^{2}\\
 & \le & \frac{1}{K-1}\sum_{\ell=1}^{K}\frac{n-n_{k}}{n}\sum_{h\ne k}\frac{n_{h}}{n-n_{k}}\left\Vert g^{*}-\hat{g}_{\lambda}(\cdot|D_{-\ell})\right\Vert _{h}^{2}\\
 & \apprle & \sum_{\ell=1}^{K}\left\Vert g^{*}-\hat{g}_{\lambda}(\cdot|D_{-\ell})\right\Vert _{\ell}^{2}+\left(\left\Vert g^{*}-\hat{g}_{\lambda}(\cdot|D_{-\ell})\right\Vert _{h}^{2}-\left\Vert g^{*}-\hat{g}_{\lambda}(\cdot|D_{-\ell})\right\Vert _{\ell}^{2}\right)
\end{eqnarray*}


We will use a symmetrization argument to bound the term in the parenthesis.
Let $W_{i}$ be RV s.t. $Pr(W_{i}=1)=\frac{n_{h}}{n_{h}+n_{k}}$ and
$Pr(W_{i}=-\frac{n_{h}+n_{k}}{n_{k}})=\frac{n_{k}}{n_{h}+n_{k}}$
(so $EW_{i}=0$). We have 
\begin{eqnarray*}
 &  & Pr_{X_{h},X_{\ell}}\left(\left\Vert g^{*}-\hat{g}_{\lambda}(\cdot|D_{-\ell})\right\Vert _{h}^{2}-\left\Vert g^{*}-\hat{g}_{\lambda}(\cdot|D_{-\ell})\right\Vert _{\ell}^{2}\ge\delta\right)\\
 & \le & 2Pr_{W,X_{h},X_{\ell}}\left(\sup_{\lambda}\frac{1}{n_{h}+n_{\ell}}\sum_{i\in D_{h}\cup D_{\ell}}W_{i}\left(g^{*}(x_{i})-\hat{g}_{\lambda}(x_{i}|D_{-\ell})\right)^{2}\ge\delta/2\right)
\end{eqnarray*}


where the second inequality follows from a symmetrization argument
(check this!)

Since $W_{i}$ are sub-gaussian, we can apply Lemma 3 again. For all
$\delta\ge CR\sqrt{J}\left(\frac{1+\log(C/\sqrt{w})+\kappa\log n}{n_{h}+n_{\ell}}\right)^{1/2}$,
we have for all $\ell=1:K$ and $h=1:K$, (and some constants $C,R$)
\[
Pr_{X_{h},X_{\ell}}\left(\sup_{\lambda}\left|\left\Vert g^{*}-\hat{g}_{\lambda}(\cdot|D_{-\ell})\right\Vert _{h}^{2}-\left\Vert g^{*}-\hat{g}_{\lambda}(\cdot|D_{-\ell})\right\Vert _{\ell}^{2}\right|\ge\delta\right)\le C\exp\left(-(n_{h}+n_{\ell})\frac{\delta^{2}}{C^{2}R^{2}}\right)
\]


Hence for all $\lambda\in\Lambda$, we have with high probability
that

\[
\|\hat{g}_{\lambda}(\cdot|D)-\hat{\xi}_{\lambda}\|_{D}^{2}\apprle\sum_{\ell=1}^{K}\left\Vert g^{*}-\hat{g}_{\tilde{\lambda}}(\cdot|D_{-\ell})\right\Vert _{\ell}^{2}+\max_{h,\ell}CR\sqrt{J}\left(\frac{1+\log(C/\sqrt{w})+\kappa\log n}{n_{h}+n_{\ell}}\right)^{1/2}
\]



\subsubsection*{Step 3:}

For every $k$, we have

\begin{eqnarray*}
 &  & \|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{D}^{2}\\
 & \le & \sum_{\ell\ne k}\frac{n_{\ell}}{n}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{\ell}^{2}+\frac{n_{k}}{n}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}^{2}\\
 & \le & \sum_{\ell\ne k}2\frac{n_{k}}{n}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}^{2}+\left(\frac{n_{\ell}}{n}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{\ell}^{2}-\frac{n_{k}}{n}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}^{2}\right)
\end{eqnarray*}


Using Lemma 3 (and the same arguments given above in Step 2), we get
that with high probability,
\[
\left|\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{\ell}^{2}-\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}^{2}\right|\apprle CR\sqrt{J}\left(\frac{1+\log(C/\sqrt{w})+\kappa\log n}{n_{h}+n_{\ell}}\right)
\]


Also, by the definition of $\hat{\lambda}$, the basic inequality
gives us that 
\begin{eqnarray*}
 &  & \sum_{k=1}^{K}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}^{2}\\
 & \le & \sum_{k=1}^{K}\left|\left\langle \epsilon,\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\right\rangle _{k}\right|+\sum_{k=1}^{K}\left|\left\langle g^{*}-\hat{g}_{\tilde{\lambda}}(x|D_{-k}),\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\right\rangle _{k}\right|
\end{eqnarray*}


If the first sum on the RHS (the empirical process term) is bigger,
then from the same arguments in Step 2, we can bound with high probability
that 
\[
\sum_{k=1}^{K}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}^{2}\le CR\sqrt{J}\left(\frac{1+\log(C/\sqrt{w})+\kappa\log n}{n_{h}+n_{\ell}}\right)^{1/2}(whp)
\]


If the second sum on the RHS is bigger, note that by Cauchy-Schwarz
\begin{eqnarray*}
\sum_{k=1}^{K}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}^{2} & \apprle & \sum_{k=1}^{K}\left|\left\langle g^{*}-\hat{g}_{\tilde{\lambda}}(x|D_{-k}),\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\right\rangle _{k}\right|\\
 & \le & \sqrt{\left(\sum_{k=1}^{K}\|g^{*}-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}^{2}\right)\left(\sum_{k=1}^{K}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}^{2}\right)}
\end{eqnarray*}


Hence with high probability,
\[
\sum_{k=1}^{K}\|\hat{g}_{\hat{\lambda}}(x|D_{-k})-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}^{2}\le\sum_{k=1}^{K}\|g^{*}-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}^{2}+CR\sqrt{J}\left(\frac{1+\log(C/\sqrt{w})+\kappa\log n}{n_{h}+n_{\ell}}\right)^{1/2}
\]



\subsubsection*{Step 4}

Combining the above steps, we have shown that with high probability,
\[
\|\hat{g}_{\hat{\lambda}}(\cdot|D)-g^{*}\|_{D}\le\max_{h,\ell}CR\sqrt{J}\left(\frac{1+\log(C/\sqrt{w})+\kappa\log n}{n_{h}+n_{\ell}}\right)^{1/2}+\sum_{k=1}^{K}\|g^{*}-\hat{g}_{\tilde{\lambda}}(x|D_{-k})\|_{k}
\]


Therefore CV converges at a parametric rate modulo a log term plus
the optimal rates of convergence.


\subsection{Lemmas}


\subsubsection{Lemma 0}

Consider any empirical distributions $Q_{1}$ and $Q_{2}$.

Suppose the penalty function is smooth. Suppose $O_{p}(n^{-u})=\min_{h:P(h)=1}\|h\|_{Q_{1}}^{2}$
and for all $h$,$\|h\|_{Q_{1}}\le O_{p}(n^{v})P(h)$ and $\|h\|_{Q_{2}}\le O_{p}(n^{v})P(h)$.
Suppose $\lambda_{min}=O_{P}(n^{-\tau_{min}})$ and $\lambda_{max}=O_{P}(n^{\tau_{max}})$.

Consider the function class 
\[
\hat{\mathcal{G}}=\left\{ \hat{g}_{\lambda}(\cdot|Q_{1}):\lambda\in\Lambda\right\} 
\]
We have that the entropy is bounded at a near-parametric rate:
\[
H\left(u,\hat{\mathcal{G}},\|\cdot\|_{Q_{2}}\right)\le\log\left(\frac{C}{u\sqrt{w}}\right)+\kappa\log n
\]



\subsubsection*{Proof}

To find the covering number for $\hat{\mathcal{G}}$, we bound the
distance$\|\hat{g}_{\lambda}(\cdot|Q_{1})-\hat{g}_{\lambda+\delta}(\cdot|Q_{1})\|_{Q_{2}}$
for every $\lambda\in\Lambda$.

Consider the function $h=c\left(\hat{g}_{\lambda}-\hat{g}_{\lambda+\delta}\right)$
where $c>0$ is some constant s.t. $P(h)=1$. Consider the 1-dimensional
optimization problem

\[
\hat{m}(\lambda+\delta)=\arg\min_{m}\frac{1}{2}\|y-(\hat{g}_{\lambda}+mh)\|_{Q_{1}}^{2}+(\lambda+\delta)\left(P(\hat{g}_{\lambda}+mh)+\frac{w}{2}\|\hat{g}_{\lambda}+mh\|_{Q_{1}}^{2}\right)
\]


Clearly $\hat{m}_{\lambda}=0$ and $\hat{m}_{\lambda+\delta}=c^{-1}$.

Taking the derivative of the criterion wrt $m$, we get

\[
-\langle h,y-(\hat{g}_{\lambda}+mh)\rangle_{Q_{1}}+\lambda\left(\frac{\partial}{\partial m}P(\hat{g}_{\lambda}+mh)+w\langle h,g+mh\rangle_{Q_{1}}\right)=0
\]


By implicit differentiation wrt $\delta$, we have 
\[
\frac{\partial}{\partial\delta}\hat{m}(\lambda+\delta)=\left.-\left(\|h\|_{T}^{2}+\lambda\frac{\partial^{2}}{\partial m^{2}}P\left(\hat{g}_{\lambda}+mh\right)+\lambda w\|h\|_{Q_{1}}^{2}\right)^{-1}\left(\frac{\partial}{\partial m}P(\hat{g}_{\lambda}+mh)+w\langle h,\hat{g}_{\lambda}+mh\rangle_{Q_{1}}\right)\right|_{m=\hat{m}(\lambda+\delta)}
\]


To bound $|\frac{\partial}{\partial\delta}\hat{m}(\lambda+\delta)|$,
consider each multiplicand.

Since penalty $P$ is convex (regardless of the direction of $h$),
the first multiplicand is bounded by

\begin{eqnarray*}
\left|\|h\|_{T}^{2}+\lambda\frac{\partial^{2}}{\partial m^{2}}P\left(\hat{g}_{\lambda}+mh\right)+\lambda w\|h\|_{Q_{1}}^{2}\right|^{-1} & \le & \left(\lambda w\|h\|_{Q_{1}}^{2}\right)^{-1}\\
 & \le & \lambda^{-1}w^{-1}O_{P}(n^{u})
\end{eqnarray*}


For the second multiplicand, note that 
\[
\left|\frac{\partial}{\partial m}P(\hat{g}_{\lambda}+mh)\right|\le P(h)
\]


and with high probability,

\begin{eqnarray*}
w\langle h,\hat{g}_{\lambda}+mh\rangle_{Q_{1}} & \le & w\|h\|_{Q_{1}}\|\hat{g}_{\lambda}+mh\|_{Q_{1}}\\
 & \le & O_{P}(n^{v})w\sqrt{(\lambda w)^{-1}4\sigma^{2}+w^{-1}P(g^{*})+\|g^{*}\|_{Q_{1}}^{2}}
\end{eqnarray*}


where we have bounded $\|g+m_{\lambda}h\|_{T}$ using the definition
\[
\frac{\lambda w}{2}\|g+m_{\lambda}h\|_{T}^{2}\le\frac{1}{2}\|y-g^{*}\|_{T}^{2}+\lambda P(g^{*})+\frac{\lambda w}{2}\|g^{*}\|_{Q_{1}}^{2}
\]


Hence there is a constant $C$ that only depends on $g^{*}$and $\sigma$
s.t. 
\begin{eqnarray*}
\left|\frac{\partial}{\partial\delta}\hat{m}(\lambda+\delta)\right| & = & \lambda^{-1}w^{-1}O_{P}(n^{u})\left|1+O_{P}(n^{v})w\sqrt{(\lambda w)^{-1}4\sigma^{2}+w^{-1}P(g^{*})+\|g^{*}\|_{Q_{1}}^{2}}\right|\\
 & \le & \lambda_{min}^{-1}O_{P}(n^{u+v})\sqrt{(\lambda_{min}w)^{-1}4\sigma^{2}+w^{-1}P(g^{*})+\|g^{*}\|_{Q_{1}}^{2}}\\
 & \le & \frac{O_{P}(n^{1.5\tau_{min}+u+v})}{\sqrt{w}}C
\end{eqnarray*}


Using the mean value theorem, there is some $\alpha\in[0,1]$ s.t 

\begin{eqnarray*}
\|\hat{g}_{\lambda}(\cdot|D_{-k})-\hat{g}_{\lambda+\delta}(\cdot|D_{-k})\|_{Q_{2}} & = & \hat{m}(\lambda+\delta)\|h\|_{Q_{2}}\\
 & \le & n^{-v}\delta\left|\frac{\partial}{\partial u}\hat{m}(\lambda+u)\right|_{u=\alpha\delta}\\
 & \le & \delta\frac{C}{\sqrt{w}}O_{P}(n^{1.5\tau_{min}+u})
\end{eqnarray*}


Therefore there is a constant $\kappa$ that linearly grows with $u,\tau_{min},\tau_{max}$
s.t. the covering number is

\[
N\left(u,\hat{\mathcal{G}},\|\cdot\|_{Q_{2}}\right)\le\frac{C}{u\sqrt{w}}O_{p}(n^{\kappa})
\]


so the entropy is 
\[
H\left(u,\hat{\mathcal{G}},\|\cdot\|_{Q_{2}}\right)\le\log\left(\frac{C}{u\sqrt{w}}\right)+\kappa\log n
\]



\subsubsection{Lemma 1}

Define the convex combination $\hat{\xi}_{\lambda}(x)=\frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\hat{g}_{\lambda}(x|D_{-k})$.
Then

\begin{eqnarray*}
 &  & \frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D)\|_{D}^{2}+\hat{\lambda}\left(P(\hat{g}_{\hat{\lambda}}(\cdot|D))+\frac{w}{2}\|\hat{g}_{\hat{\lambda}}(\cdot|D)\|_{D}^{2}\right)\\
 & \ge & \frac{1}{2}\|y-\hat{\xi}_{\hat{\lambda}}\|_{D}^{2}+\hat{\lambda}\left(P(\hat{\xi}_{\hat{\lambda}})+\frac{w}{2}\|\hat{\xi}_{\hat{\lambda}}\|_{D}^{2}\right)+\frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\langle y-\hat{\xi}_{\hat{\lambda}},\hat{\xi}_{\hat{\lambda}}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\rangle_{-k}
\end{eqnarray*}


(This is a version of the beginning of the proof for Thrm 1 in Chetverikov,
Chaterjee probably does the same thing.)


\subsubsection*{Proof}

\begin{eqnarray*}
 &  & \frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D)\|_{D}^{2}+\hat{\lambda}\left(P(\hat{g}_{\hat{\lambda}}(\cdot|D))+\frac{w}{2}\|\hat{g}_{\hat{\lambda}}(\cdot|D)\|_{D}^{2}\right)\\
 & = & \frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\left(\frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D)\|_{-k}^{2}+\hat{\lambda}\left(P(\hat{g}_{\hat{\lambda}}(\cdot|D))+\frac{w}{2}\|\hat{g}_{\hat{\lambda}}(\cdot|D)\|_{-k}^{2}\right)\right)\\
 & \ge & \frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\left(\frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|_{-k}^{2}+\hat{\lambda}\left(P(\hat{g}_{\hat{\lambda}}(\cdot|D_{-k}))+\frac{w}{2}\|\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|_{-k}^{2}\right)\right)\\
 & \ge & \frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\left(\frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|_{-k}^{2}+\hat{\lambda}\frac{w}{2}\|\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|_{-k}^{2}\right)+\hat{\lambda}\left(P(\hat{\xi}_{\hat{\lambda}})+\frac{w}{2}\|\hat{\xi}_{\hat{\lambda}}\|_{D}^{2}\right)
\end{eqnarray*}


The second inequality follows by convexity of $P$ and $\|\cdot\|^{2}$.

Now note that 
\begin{eqnarray*}
\frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\frac{1}{2}\|y-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|_{-k}^{2} & = & \frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\frac{1}{2}\|y-\hat{\xi}_{\hat{\lambda}}+\hat{\xi}_{\hat{\lambda}}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\|_{-k}^{2}\\
 & \ge & \frac{1}{2}\|y-\hat{\xi}_{\hat{\lambda}}\|_{D}^{2}+\frac{1}{K-1}\sum_{k=1}^{K}\frac{n-n_{k}}{n}\langle y-\hat{\xi}_{\hat{\lambda}},\hat{\xi}_{\hat{\lambda}}-\hat{g}_{\hat{\lambda}}(\cdot|D_{-k})\rangle_{-k}
\end{eqnarray*}



\subsubsection{Lemma 2}

Consider any $\xi\in\mathcal{G}$ and $\lambda\in\Lambda$. Suppose
$P$ is convex.

Then

\[
\frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-\xi\|_{D}^{2}\le\frac{1}{2}\|y-\xi\|_{D}^{2}-\frac{1}{2}\|y-\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}+\lambda\left(P(\xi)+\frac{w}{2}\|\xi\|_{D}^{2}\right)-\lambda\left(P(\hat{g}_{\lambda}(\cdot|D))+\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}\right)
\]


(This is a version of Lemma 10 in Chetverikov, which is based on Chaterjee.)


\subsubsection*{Proof}

Since $P$ is convex, then for $t\in(0,1)$, we have

\begin{eqnarray*}
 &  & \frac{1}{2}\|y-\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}+\lambda\left(P(\hat{g}_{\lambda}(\cdot|D))+\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}\right)\\
 & \le & \frac{1}{2}\|y-\left(t\xi+(1-t)\hat{g}_{\lambda}(\cdot|D)\right)\|_{D}^{2}+\lambda\left(P(t\xi+(1-t)\hat{g}_{\lambda}(\cdot|D))+\frac{w}{2}\|t\xi+(1-t)\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}\right)\\
 & \le & \frac{1}{2}\|y-\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}+t\langle y-\hat{g}_{\lambda}(\cdot|D),\hat{g}_{\lambda}(\cdot|D)-\xi\rangle_{D}+t^{2}\|\xi-\hat{g}_{\lambda}\|_{D}^{2}+\lambda\left(tP(\xi)+(1-t)P\left(\hat{g}_{\lambda}(\cdot|D)\right)+t\frac{w}{2}\|\xi\|_{D}^{2}+(1-t)\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}\right)\\
 & \le & \frac{1}{2}\|y-\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}+t\langle y-\hat{g}_{\lambda}(\cdot|D),\hat{g}_{\lambda}(\cdot|D)-\xi\rangle_{D}+\frac{t^{2}}{2}\|\xi-\hat{g}_{\lambda}\|_{D}^{2}+\lambda\left(tP(\xi)+(1-t)P\left(\hat{g}_{\lambda}(\cdot|D)\right)+t\frac{w}{2}\|\xi\|_{D}^{2}+(1-t)\|\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}\right)
\end{eqnarray*}


Rearranging terms, we obain
\[
\lambda\left(P\left(\hat{g}_{\lambda}(\cdot|D)\right)+\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}-P(\xi)-\frac{w}{2}\|\xi\|_{D}^{2}\right)\le\langle y-\hat{g}_{\lambda}(\cdot|D),\hat{g}_{\lambda}(\cdot|D)-\xi\rangle_{D}+\frac{t}{2}\|\xi-\hat{g}_{\lambda}\|_{D}^{2}
\]


Since this is true for any $t$, we have that

\[
\lambda\left(P\left(\hat{g}_{\lambda}(\cdot|D)\right)+\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}-P(\xi)-\frac{w}{2}\|\xi\|_{D}^{2}\right)\le\langle y-\hat{g}_{\lambda}(\cdot|D),\hat{g}_{\lambda}(\cdot|D)-\xi\rangle_{D}
\]


Thus
\begin{eqnarray*}
\frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-\xi\|_{D}^{2} & \le & \frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-y+y-\xi\|_{D}^{2}\\
 & = & \frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-y\|_{D}^{2}+\frac{1}{2}\|y-\xi\|_{D}^{2}-\langle\hat{g}_{\lambda}(\cdot|D)-y,\xi-y\rangle_{D}\\
 & = & -\frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-y\|_{D}^{2}+\frac{1}{2}\|y-\xi\|_{D}^{2}-\langle\hat{g}_{\lambda}(\cdot|D)-y,\xi-\hat{g}_{\lambda}(\cdot|D)\rangle_{D}\\
 & \le & -\frac{1}{2}\|\hat{g}_{\lambda}(\cdot|D)-y\|_{D}^{2}+\frac{1}{2}\|y-\xi\|_{D}^{2}-\lambda\left(P\left(\hat{g}_{\lambda}(\cdot|D)\right)+\frac{w}{2}\|\hat{g}_{\lambda}(\cdot|D)\|_{D}^{2}-P(\xi)-\frac{w}{2}\|\xi\|_{D}^{2}\right)
\end{eqnarray*}



\subsubsection{Lemma 3}

Suppose $\epsilon$ are independent sub-gaussian RV with constants
$K$ and $\sigma$.

Suppose $X$ are $n$ random (or fixed) covariate values.

Suppose for any empirical distribution $Q$, the (random) function
class $\mathcal{F}(X,\epsilon)$ has its entropy uniformly bounded
\[
H\left(u,\mathcal{F}(X,\epsilon),\|\cdot\|_{Q}\right)\le\psi(u)=J\left(\log\left(\frac{C}{u\sqrt{w}}\right)+\kappa\log n\right)
\]


for positive constants $J,C,w,\kappa$.

Suppose $\sup_{f\in\mathcal{F}(X,\epsilon)}\|f\|_{Q}\le R$. 

Then there exists some $C$ s.t. for all $\delta$ s.t. $R\ge\delta/\sigma$
and

\[
\delta\ge CR\sqrt{J}\left(\frac{1+\log(C/\sqrt{w})+\kappa\log n}{|Q|}\right)^{1/2}
\]


we have

\[
Pr\left(\sup_{f\in\mathcal{F}(X,\epsilon),\|f\|_{Q}\le R}\left|\langle\epsilon,f\rangle_{Q}\right|\ge\delta\wedge\|\epsilon\|_{Q}\le\sigma\right)\le C\exp\left(-|Q|\frac{\delta^{2}}{C^{2}R^{2}}\right)
\]



\subsubsection*{Proof}

\[
\langle\epsilon,\hat{g}_{\lambda}(\cdot|D_{-\ell})-g^{*}\rangle_{-k}
\]


We apply Lemma 10 in Vandegeer to determine the value $\delta$ s.t.
$\delta$ bounds the empirical process term with high probability.

For $R\ge\delta/\sqrt{2}\sigma$ ,

\begin{eqnarray*}
\int_{0}^{R}\psi^{1/2}(u)du & = & \sqrt{J}\int_{0}^{R}\left(\log\left(\frac{1}{u}\right)+\log(C/\sqrt{w})+\kappa\log n\right)^{1/2}du\\
 & \apprle & R\sqrt{J}\left(\int_{0}^{1}\log\left(\frac{1}{u}\right)+\log(C/\sqrt{w})+\kappa\log ndu\right)^{1/2}\\
 & \le & R\sqrt{J}\left(1+\log(C/\sqrt{w})+\kappa\log n\right)^{1/2}
\end{eqnarray*}


Apply Lemma 10 to $\delta>0$ s.t. 
\[
\delta\ge CR\sqrt{J}\left(\frac{1+\log(C/\sqrt{w})+\kappa\log n}{|Q|}\right)^{1/2}
\]



\subsubsection{Lemma 4}

Suppose we are working within a restricted domain, so there is some
constant $R$ s.t. $\|g^{*}\|_{\infty}\le R$.

Suppose $\epsilon$ is sub-gaussian with constants $K,\sigma$.

Then for some constant $C$ that depends on $\lambda_{max}$ and $g^{*}$,
we have
\[
Pr\left(\sup_{\lambda}\|\hat{g}_{\lambda}(\cdot|D_{-k})-g^{*}\|_{-k}\ge2\sigma+C\right)\le\exp\left(-(n-n_{k})\frac{\sigma^{2}}{12K^{2}}\right)
\]



\subsubsection*{Proof}

By triangle inequality 
\[
\|g^{*}-\hat{g}_{\lambda}(\cdot|D_{-k})\|_{-k}\le\|y-\hat{g}_{\lambda}(\cdot|D_{-k})\|_{-k}+\|y-g^{*}\|_{_{-k}}
\]


By definition of $\hat{g}_{\lambda}$, we have

\begin{eqnarray*}
\|y-\hat{g}_{\lambda}(\cdot|D_{-k})\|_{-k}^{2} & \le & \|y-g^{*}\|_{-k}^{2}+\lambda\left(P(g^{*})+\frac{w}{2}\|g^{*}\|_{-k}^{2}\right)\\
 & \le & \|y-g^{*}\|_{-k}^{2}+\lambda_{max}P(g^{*})+\frac{w}{2}R
\end{eqnarray*}


Since $\epsilon$ is sub-gaussian, then by Bernstein's inequality,
we have 
\[
Pr\left(\|\epsilon\|_{-k}^{2}\ge2\sigma^{2}\right)\le\exp\left(-(n-n_{k})\frac{\sigma^{2}}{12K^{2}}\right)
\]



\subsubsection{Lemma 10}

Suppose $\epsilon$ are $n$ independent sub-gaussian RVs with constants
$K$ and $\sigma$.

Let $X$ be $n$ covariate values (potentially randomly drawn).

Suppose that we have function classes $\mathcal{F}(X,\epsilon)$ dependent
on the sub-gaussian RV with entropy $H\left(\delta,\mathcal{F}(X,\epsilon),\|\cdot\|_{X}\right)$.
Suppose there is a universal bound
\[
H\left(u,\mathcal{F}(X,\epsilon),\|\cdot\|_{X}\right)\le\psi(u)
\]


Suppose $\sup_{f\in\mathcal{F}(X,\epsilon)}\|f\|_{X}\le R$ (with
high probability). 

Then there exists some $C$ dependent only on $K,\sigma$ s.t. for
all

\[
\delta\ge\int_{0}^{R}\psi^{1/2}(u)du
\]


we have

\[
Pr_{\epsilon}\left(\sup_{f_{\theta}\in\mathcal{F}(X,\epsilon)}\left|\langle\epsilon,f_{\theta}\rangle_{X}\right|\ge\delta\wedge\|\epsilon\|_{X}\le\sigma\right)\le C\exp\left(-|X|\frac{\delta^{2}}{C^{2}R^{2}}\right)
\]



\subsubsection*{Proof}

Proof closely follows Lemma 3.2 from Vandegeer.

Let $\{f_{j}^{s}(\cdot|\epsilon)\}_{j=1}^{N_{s}}$ be the $2^{-s}R$-covering
set of $\mathcal{F}(X,\epsilon)$ where $N_{s}=N_{s}(2^{-s}R,\mathcal{F}(X,\epsilon),X)\le\exp\left(\psi(2^{-s}R)\right)$.
Let $S=\min\{s:2^{-s}R\le\delta/2\sigma\}$

Let $f_{\theta}^{s}(\cdot|\epsilon)$ be the closest element to $f_{\theta}$
in the $2^{-s}R$-covering set. If $\|\epsilon\|_{X}\le\sigma$, then
\begin{eqnarray*}
\left|\langle\epsilon,f_{\theta}-f_{\theta}^{S}(\cdot|\epsilon)\rangle_{X}\right| & \le & \sigma\|f_{\theta}-f_{\theta}^{S}(\cdot|\epsilon)\|_{X}\\
 & \le & \delta/2
\end{eqnarray*}


Therefore it suffices to bound 
\[
Pr_{\epsilon}\left(\sup_{j=1:N_{S}}\left|\langle\epsilon,f_{j}^{s}(\cdot|\epsilon)\rangle_{X}\right|\ge\delta/2\wedge\|\epsilon\|_{X}\le\sigma\right)
\]


Let's chain! Let $f_{\theta}^{S}(\cdot|\epsilon)=\sum_{s=1}^{S}f_{\theta}^{s}(\cdot|\epsilon)-f_{\theta}^{s-1}(\cdot|\epsilon)$.
Note that
\begin{eqnarray*}
\|f_{\theta}^{s}(\cdot|\epsilon)-f_{\theta}^{s-1}(\cdot|\epsilon)\|_{X} & \le & \|f_{\theta}^{s}(\cdot|\epsilon)-f_{\theta}\|_{X}+\|f_{\theta}-f_{\theta}^{s-1}(\cdot|\epsilon)\|_{X}\\
 & \le & 3(2^{-s}R)
\end{eqnarray*}


Then for some positive numbers s.t. $\sum_{s=1}^{S}\eta_{s}\le1$,
we have 
\begin{eqnarray*}
 &  & Pr_{\epsilon}\left(\sup_{j=1:N_{s}}\left|\langle\epsilon,\sum_{s=1}^{S}f_{\theta}^{s}(\cdot|\epsilon)-f_{\theta}^{s-1}(\cdot|\epsilon)\rangle_{X}\right|\ge\delta/2\right)\\
 & \le & \sum_{s=1}^{S}Pr_{\epsilon}\left(\sup_{j=1:N_{s}}\left|\langle\epsilon,f_{\theta}^{s}(\cdot|\epsilon)-f_{\theta}^{s-1}(\cdot|\epsilon)\rangle_{X}\right|\ge\delta/2\eta_{s}\right)\\
 & \le & \sum_{s=1}^{S}\exp\left(2\psi(2^{-s}R)-C\frac{n(\delta/2)^{2}\eta_{s}^{2}}{9(2^{-2s}R^{2})}\right)
\end{eqnarray*}


Choose $\eta_{s}$ as Vandegeer does. Then after a lot of algebraic
massaging, we get that for some constants $C_{1},C_{2}$

\[
Pr_{\epsilon}\left(\sup_{f_{\theta}\in\mathcal{F}(X,\epsilon)}\left|\langle\epsilon,f_{\theta}\rangle_{X}\right|\ge\delta\wedge\|\epsilon\|_{X}\le\sigma\right)\le C_{1}\exp\left(-|X|\frac{\delta^{2}}{C_{2}^{2}R^{2}}\right)
\]



\subsection{OLD}


\subsubsection{Lemma 3}

Consider the function class $\mathcal{F}$ with entropy bound 
\[
H\left(u,\mathcal{F},\|\cdot\|_{Q}\right)\le J\left(\log\left(\frac{C}{u\sqrt{w}}\right)+\kappa\log n\right)
\]


We will suppose that $\sup_{f\in\mathcal{F}}\|f\|_{Q}\le F\sigma$.
(check this!!!)

Suppose $\epsilon$ are independent sub-gaussian RV with constants
$K$ and $\sigma$.

We get that there exists some $C$ s.t. for all $\delta$ s.t. $R\ge\delta/\sigma$
and

\[
\delta\ge CR\sqrt{J}\left(\frac{1+\log(C/\sqrt{w})+\kappa\log n}{|Q|}\right)^{1/2}
\]


we have

\[
Pr\left(\sup_{f_{1}:\|f_{1}\|_{Q}\le R}\left|\langle\epsilon,f_{1}\rangle_{Q}\right|\ge\delta\wedge\|\epsilon\|_{Q}\le2\sigma\right)\le C\exp\left(-|Q|\frac{\delta^{2}}{C^{2}R^{2}}\right)
\]



\subsubsection*{Proof}

Using Lemma $2\frac{3}{4}$, we bound the empirical process term by
a standard chaining argument (basically a copy of Thrm 9.1 in Vandegeer).

Let $S=\min\{s\in\{0,1,...\}:2^{s}>F\sigma\}$. For 
\[
\delta\ge16C\left(\frac{1+\log(C/\sqrt{w})+\kappa\log n}{|Q|}\right)^{1/2}
\]


we have

\begin{eqnarray*}
 &  & Pr\left(\sup_{f_{1}:\|f_{1}\|_{Q}\le F}\left|\langle\epsilon,f_{1}\rangle_{Q}\right|\ge\delta^{2}\wedge\|\epsilon\|_{Q}\le2\sigma\right)\\
 & \le & \sum_{s=0}^{S}Pr\left(\sup_{f_{1}:\|f_{1}\|_{Q}\le2^{s+1}\delta}\left|\langle\epsilon,f_{1}\rangle_{Q}\right|\ge2^{2s-1}\delta^{2}\wedge\|\epsilon\|_{Q}\le2\sigma\right)\\
 & \le & \sum_{s=0}^{S}C\exp\left(-|Q|\frac{2^{4s-2}\delta^{4}}{4C^{2}2^{2s+2}\delta^{2}}\right)\\
 & \le & C\exp\left(-|Q|\frac{\delta^{2}}{c^{2}}\right)
\end{eqnarray*}
Note that Lemma $2\frac{3}{4}$ can be applied since for $s=0,...,S$,
\[
\sqrt{|Q|}2^{2s+2}\delta^{2}\ge16C2^{s+1}\delta\left(1+\log(C/\sqrt{w})+\kappa\log n\right)^{1/2}
\]

\end{document}
