%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\section{Simple model}


\subsection*{Definitions}

We find the best model for $y$ over function class $\mathcal{G}$.
Presume $g^{*}\in\mathcal{G}$ is the true model and 
\[
y=g^{*}(X)+\epsilon
\]


where $\epsilon$ are sub-Gaussian errors for constants $K$ and $\sigma_{0}^{2}$
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Given a training set $T$ , We define the fitted models 
\[
\hat{g}_{\lambda}=\|y-g\|_{T}^{2}+\lambda^{2}I^{v}(g)
\]


Given a validation set $V$ , let the CV-fitted model be 
\[
\hat{g}_{\hat{\lambda}}=\arg\min_{\lambda}\|y-\hat{g}_{\lambda}\|_{V}^{2}
\]


We will suppose $I(g^{*})>0$.


\subsection*{Assumptions}

Suppose the entropy of the class $\mathcal{G}'$ is 
\begin{eqnarray}
H\left(\delta,\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} ,P_{T}\right) & \le & \tilde{A}\delta^{-\alpha}
\end{eqnarray}


Suppose $v>2\alpha/(2+\alpha)$. 

Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by $\|\hat{g}_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}\hat{g}_{\lambda}(x_{i})$.
See Lemma 1 below for the specific assumption. This assumption includes
Ridge, Lasso, Generalized Lasso, and the Group Lasso. 


\subsection*{Result 1: Single $\lambda$, Single Penalty, cross-validation over
general $X_{T},X_{V}$}

Suppose that the training and validation set are independently sampled,
so the values $X_{i}$ are not necessarily the same. Suppose the training
and validation sets are both of size $n$. Suppose $X$ is bounded
s.t. $|X|\le R_{X}$ and the domain of $g\in\mathcal{G}$ is over
$(-R_{X},R_{X})$.

Suppose the same entropy bound (2) for both the training set $P_{T}$
and validation set $P_{V}$.

Suppose for all $\lambda\in\Lambda$, there exists a compatibility
constant $M$ s.t. $I^{v}(\hat{g}_{\lambda})$ is upper bounded by
its $L_{2}$-norm with some constant $M$ (and $M_{0}$) such that

\[
I^{v}(\hat{g}_{\lambda})\le M\|\hat{g}_{\lambda}\|_{n}^{2}+M_{0}
\]


Suppose the entropy bound for both training set $P_{T}$ and validation
set $P_{V}$.

Suppose that 
\begin{eqnarray*}
\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{\infty}}{I(g)+I(g^{*})} & \le & K<\infty
\end{eqnarray*}


Let $\tilde{\lambda}$ be the optimal $\lambda$ by Vandegeer. Then

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}=O_{p}\left(n^{-1/(2+\alpha)}\right)\left(I^{\alpha/(2+\alpha)}(g^{*})+I(g^{*})\right)
\]


and $\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{V}$ is of the same order
(differs by some constant).

\textbf{Proof:}

By the triangle inequality, 
\[
\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{V}\le\|\hat{g}_{\hat{\lambda}}-\hat{g}_{\tilde{\lambda}}\|_{V}+\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V}
\]


We bound each component on the RHS separately.

First bound $\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V}$. By Vandegeer
Thrm 10.2 and Lemma 2,

\begin{eqnarray*}
\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V} & \le & \|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{T}+\left|\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V}-\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{T}\right|\\
 & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)I^{\alpha/(2+\alpha)}(g^{*})+O_{p}\left(n^{-1/(2+\alpha)}\right)\left(I(g^{*})+I(\hat{g}_{\tilde{\lambda}})\right)\\
 & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)\left(I^{\alpha/(2+\alpha)}(g^{*})+I(g^{*})\right)
\end{eqnarray*}


Next bound $\|\hat{g}_{\hat{\lambda}}-\hat{g}_{\tilde{\lambda}}\|_{V}$.
The basic inequality gives us

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}^{2}\le2\left|\left(\epsilon,\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{V}\right|+2\left|\left(g^{*}-\hat{g}_{\tilde{\lambda}},\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{V}\right|
\]


\textbf{Case a:} $\left|\left(\epsilon,\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{T}\right|$
is the bigger term on the RHS

By Vandegeer (10.6),
\begin{eqnarray*}
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}^{2} & \le & O_{P}(n^{-1/2})\|\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\|^{1-\alpha/2}\left(I(\hat{g}_{\tilde{\lambda}})+I(\hat{g}_{\hat{\lambda}})\right)^{\alpha/2}
\end{eqnarray*}


If $I(\hat{g}_{\tilde{\lambda}})>I(\hat{g}_{\hat{\lambda}})$, then

\begin{eqnarray*}
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V} & \le & O_{P}(n^{-1/(2+\alpha)})I(g^{*})^{\alpha/(2+\alpha)}
\end{eqnarray*}


Otherwise, suppose $I(\hat{g}_{\tilde{\lambda}})<I(\hat{g}_{\hat{\lambda}})$.
Since $I$ is a pseudo-norm,

\begin{eqnarray*}
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V} & \le & O_{P}(n^{-1/(2+\alpha)})I(\hat{g}_{\hat{\lambda}})^{\alpha/(2+\alpha)}\\
 & \le & O_{P}(n^{-1/(2+\alpha)})\left(I(\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}})+I(\hat{g}_{\tilde{\lambda}})\right)^{\alpha/(2+\alpha)}
\end{eqnarray*}


If $I(\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}})\le I(\hat{g}_{\tilde{\lambda}})$,
then we're done. Otherwise if $I(\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}})\ge I(\hat{g}_{\tilde{\lambda}})$,
by the assumption that $I^{V}(\cdot)$ is bounded by the L2 norm,

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}\le O_{P}(n^{-1/(2+\alpha)})\left(M\|\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\|_{V}^{2}+M_{0}\right)^{\alpha/v(2+\alpha)}
\]


If $M_{0}$ is bigger, we're done. Otherwise,

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}\le O_{P}(n^{-v/(2v-2\alpha+\alpha v)})<O_{P}(n^{-1/(2+\alpha)})
\]


\textbf{Case b:} $\left|\left(g^{*}-\hat{g}_{\tilde{\lambda}},\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{V}\right|$
is the bigger term on the RHS

By Cauchy Schwarz,
\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}\le O_{P}(1)\left\Vert g^{*}-\hat{g}_{\tilde{\lambda}}\right\Vert _{V}
\]


\pagebreak{}


\section{General Additive Model}


\subsection*{Definitions}

We find the best model for $y$ over function classes $\mathcal{G}=\left\{ \sum_{j=1}^{J}g_{j}:\mbox{ }g_{j}\in\mathcal{G}_{j}\right\} $.
Suppose we observe:

\[
y=\sum_{j=1}^{J}g_{j}^{*}+\epsilon
\]
 where $\sum_{j=1}^{J}g_{j}^{*}\in\mathcal{G}$. Suppose $\epsilon$
are sub-Gaussian errors for constants $K$ and $\sigma_{0}^{2}$:
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Given a training set $T$ , we fit models by least squares with multiple
penalties 
\[
\{\hat{g}_{\lambda,j}\}_{j=1}^{J}=\arg\min_{\sum g_{j}\in\mathcal{G}}\|y-\sum_{j=1}^{J}g_{j}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j})
\]


Given a validation set $V$ , let the CV-fitted model be 
\[
\{\hat{g}_{\hat{\lambda},j}\}_{j=1}^{J}=\arg\min_{\lambda}\|y-\sum_{j=1}^{J}\hat{g}_{\lambda,j}\|_{V}^{2}
\]


\textbf{Reasonable assumption:}
\begin{itemize}
\item The entropy bound (2) in result 2 comes from the assumptions in Lemma
3. The $\alpha$ below is $\alpha=\max_{j=1:J}\{\alpha_{j}\}$, so
convergence is only as fast as fitting the highest-entropy function
class. The constant $A$ must be appropriately inflated such that
the entropy bound holds for all $\delta\in(0,R]$.
\end{itemize}
\textbf{``Special'' assumptions:}
\begin{itemize}
\item We assume exponents $v_{j}=1$, whereas Vandegeer Thrm 10.2 only assumes
$v>2\alpha/(2+\alpha)$. Without this assumption, I wasn't able to
form inequalities between $\sum_{j=1}^{J}I_{j}(g_{j})\le something+\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j})$.
Indeed, Remark 1 in ``High-dimensional Additive Modeling'' (Vandegeer
2009) notes the importance of using the semi-norm instead of the square
of the semi-norm.
\item We suppose the following incoherence condition, in the spirit of Vandegeer
2014 ``The additive model with different smoothness for the components'':
Let $p_{V}(\vec{x})$ be the empirical density over the validation
set. Let $p_{Vj}$ be the marginal density of $x_{j}$ for the empirical
distribution of the validation set. Let 
\[
r_{V}(\vec{x})=\frac{p_{V}(\vec{x})}{\Pi_{j=1}^{J}p_{Vj}(x_{j})},\mbox{ }\gamma_{V}^{2}=\int r_{V}(\vec{x})\Pi_{j=1}^{J}p_{Vj}(x_{j})d\mu
\]
Suppose that $\gamma_{V}<1/(J-1)$. Furthermore, we will suppose that
$\int g_{j}p_{Vj}d\mu=0$ for $j=2,...,J$.
\end{itemize}

\subsection*{Result 2: Additive Model with multiple penalties, Single oracle $\lambda$
over $X_{T}$}

Suppose there is some $0<\alpha<2$ s.t. for all $\delta\in(0,R]$,
\begin{equation}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le A\delta^{-\alpha}
\end{equation}


If $\lambda$ is chosen s.t. 
\[
\tilde{\lambda}_{T}^{-1}=O_{p}\left(n^{1/(2+\alpha)}\right)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{(2-\alpha)/2(2+\alpha)}
\]


then
\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}=O_{p}\left(\tilde{\lambda}_{T}\right)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{1/2}
\]


and 
\[
\sum_{j=1}^{J}I_{j}(\hat{g}_{j})=O_{p}(1)\sum_{j=1}^{J}I_{j}(g_{j}^{*})
\]



\subsubsection*{Proof:}

The basic inequality gives us:

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le2\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|+\lambda^{2}\sum_{j=1}^{J}I_{j}(g_{j}^{*})
\]


\textbf{Case 1: $\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\le\lambda^{2}\sum_{j=1}^{J}I_{j}(g_{j}^{*})$}

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}(\lambda)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{1/2}
\]


\textbf{Case 2: $\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\ge\lambda^{2}\sum_{j=1}^{J}I_{j}(g_{j}^{*})$}

By Vandegeer (10.6), the basic inequality becomes 
\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le O_{p}\left(n^{-1/2}\right)\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha/2}\left(\sum_{j=1}^{J}I_{j}(\hat{g}_{j})+I_{j}(g_{j}^{*})\right)^{\alpha/2}
\]


\textbf{Case 2a:} $\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le\sum_{j=1}^{J}I_{j}(g_{j}^{*})$ 

Then 
\begin{eqnarray*}
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T} & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha/(2+\alpha)}
\end{eqnarray*}


\textbf{Case 2b:} $\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\ge\sum_{j=1}^{J}I_{j}(g_{j}^{*})$ 

Then

\begin{eqnarray*}
\sum_{j=1}^{J}I_{j}(\hat{g}_{j}) & \le & O_{p}\left(n^{-1/(2-\alpha)}\right)\lambda^{-4/(2-\alpha)}\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}
\end{eqnarray*}


Hence

\begin{eqnarray*}
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T} & \le & O_{p}\left(n^{-1/(2-\alpha)}\right)\lambda^{-2\alpha/(2-\alpha)}
\end{eqnarray*}



\subsection*{Result 3: Additive Model with multiple penalties, Single cross-validation
$\lambda$ over general $X_{T},X_{V}$}

Suppose that the training and validation set are independently sampled,
so the values $X_{i}$ are not necessarily the same. Suppose the training
and validation sets are both of size $n$. Suppose $X$ is bounded
s.t. $|X|\le R_{X}$ and the domain of $g\in\mathcal{G}$ is over
$(-R_{X},R_{X})$.

Suppose the same entropy bound (2) for both the training set $P_{T}$
and validation set $P_{V}$.

In addition to the assumptions in Result 4, suppose the infinity norm
is also bounded 
\begin{eqnarray*}
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|_{\infty}}{\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})} & \le & K<\infty
\end{eqnarray*}


Suppose there exist constants $M$,$M_{0}$ s.t. for all $j$ and
all $\lambda\in\Lambda$ 

\[
I_{j}\left(\hat{g}_{\lambda,j}\right)\le M\|\hat{g}_{\lambda,j}\|_{V}^{2}+M_{0}
\]


\textbf{Special assumption:} Suppose the incoherence condition $\gamma_{V}<1/(J-1)$.
We will also suppose $\int g_{j}p_{Vj}d\mu=0$ for $j=2,...,J$.

Let $\tilde{\lambda}$ be the optimal $\lambda$ as specified in Result
2. Then

\[
\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-\hat{g}_{\tilde{\lambda},j}\|_{V}=O_{p}\left(n^{-1/(2+\alpha)}\right)\left(1-\gamma(J-1)\right)^{\alpha/(2+\alpha)}\left(\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha/(2+\alpha)}+\sum_{j=1}^{J}I_{j}(g_{j}^{*})+\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}^{\alpha/2(2+\alpha)}\right)
\]


and $\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}$
is on the same order (differs by a constant).


\subsubsection*{Proof:}

By the triangle inequality, 
\[
\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}\le\left\Vert \sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}+\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}
\]


By Lemma 2 and Result 2, we can easily bound$\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}$.

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V} & \le & \left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{T}+\left|\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{T}-\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}\right|\\
 & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)\left(\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha/(2+\alpha)}+\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)
\end{eqnarray*}


Next bound $\left\Vert \sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}$.
By definition of $\hat{\lambda}$, we have the basic inequality

\[
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{2}\le2\left|\left(\epsilon,\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|+2\left|\left(\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j},\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|
\]


\textbf{Case 1:} $\left|\left(\epsilon,\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|$
is bigger

By Vandegeer (10.6),
\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(\sum_{j=1}^{J}I_{j}(\hat{g}_{\tilde{\lambda},j})+I_{j}(\hat{g}_{\hat{\lambda},j})\right)^{\alpha/2}
\end{eqnarray*}


If $\sum_{j=1}^{J}I_{j}(\hat{g}_{\tilde{\lambda},j})\ge\sum_{j=1}^{J}I_{j}(\hat{g}_{\hat{\lambda},j})$,
we're done.

Otherwise, suppose $\sum_{j=1}^{J}I_{j}(\hat{g}_{\tilde{\lambda},j})<\sum_{j=1}^{J}I_{j}(\hat{g}_{\hat{\lambda},j})$. 

Since all the penalties are bounded by the L2 norm, 

\begin{eqnarray*}
\sum_{j=1}^{J}I_{j}(\hat{g}_{\hat{\lambda},j}) & \le & M\sum_{j=1}^{J}\|\hat{g}_{\lambda j}\|_{V}^{2}+M_{0}J\\
 & \le & M\left(1-\gamma(J-1)\right)\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}\|_{V}^{2}+M_{0}J
\end{eqnarray*}


where the latter inequality is due to the incoherence assumption and
Lemma 4.

Then
\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(M\left(1-\gamma(J-1)\right)\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}\|_{V}^{2}+M_{0}J\right)^{\alpha/2}
\end{eqnarray*}


If $M_{0}J$ is the biggest, we're done. Otherwise,

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(1-\gamma(J-1)\right)^{\alpha/2}\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}\|_{V}^{\alpha}\\
 & \le & O_{p}(n^{-1/2})\left(1-\gamma(J-1)\right)^{\alpha/2}\left(\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}+\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-g_{j}^{*}\right\Vert _{V}+\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}\right)^{\alpha}
\end{eqnarray*}


If $\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}$or
$\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-g_{j}^{*}\right\Vert _{V}$
is the biggest on the RHS, then the rate is faster than $O_{p}(n^{-1/(2+\alpha)})$.\textbf{
}If\textbf{$\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}$ }is
the biggest, then

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V} & \le & O_{p}(n^{-1/(2+\alpha)})\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}^{\alpha/2(2+\alpha)}
\end{eqnarray*}


\textbf{Case 2:} $\left|\left(\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j},\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|$
is bigger

By Cauchy Schwarz, 

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V} & \le & O_{p}(1)\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}
\end{eqnarray*}
\pagebreak{}


\section{General Additive Model: Multiple Lambdas}


\subsection*{Definitions}

We find the best model for $y$ over function classes $\mathcal{G}=\left\{ \sum_{j=1}^{J}g_{j}:\mbox{ }g_{j}\in\mathcal{G}_{j}\right\} $.
Suppose we observe:

\[
y=\sum_{j=1}^{J}g_{j}^{*}+\epsilon
\]
 where $\sum_{j=1}^{J}g_{j}^{*}\in\mathcal{G}$. Suppose $\epsilon$
are sub-Gaussian errors for constants $K$ and $\sigma_{0}^{2}$:
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Given a training set $T$ , we fit models by least squares with multiple
penalties and tuning parameters 
\[
\{\hat{g}_{\lambda,j}\}_{j=1}^{J}=\arg\min_{\sum g_{j}\in\mathcal{G}}\|y-\sum_{j=1}^{J}g_{j}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{2}I_{j}^{v_{j}}(g_{j})
\]


Suppose $1\le v_{j}\le2$.

Given a validation set $V$ , let the CV-fitted model be 
\[
\{\hat{g}_{\hat{\lambda},j}\}_{j=1}^{J}=\arg\min_{\lambda}\|y-\sum_{j=1}^{J}\hat{g}_{\lambda,j}\|_{V}^{2}
\]



\subsection*{Result 4: Additive Model, Oracle $\{\lambda_{i}\}$ given $X_{T}$}

These results are implied by Vandegeer's paper ``The additive model
with different smoothness for the components.'' 

Suppose for all $j=1:J$
\[
\mathcal{H}\left(\delta,\left\{ \frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}\right\} ,\|\cdot\|_{n}\right)\le A_{j}\delta^{-\alpha_{j}}\forall\delta>0
\]


Let
\[
\lambda_{j}=O_{p}(n^{-1/(2+\alpha_{j})})
\]


and 

\[
\left(\sum_{j=1}^{J}I_{j}^{q_{j}}(g_{j}^{*})\right)^{1/2}\lambda_{\max}=O_{P}(1)R
\]


There are some constants $c_{1},c_{2}$ s.t. for $\lambda_{j}=O_{p}(n^{-1/(2+\alpha_{j})})$,
we have 
\[
\|\sum g_{j}^{*}-\sum\hat{g}_{\tilde{\lambda},j}\|\le c_{2}\lambda_{(j)}
\]


where $(j)=\arg\max\alpha_{j}$. That is, the convergence rate depends
on the highest-entropy function class (with respect to the penalty)

\[
\|\sum_{j=1}^{J}g_{j}^{*}-\sum_{j=1}^{J}\hat{g_{j}}\|_{T}=O_{p}(n^{-1/(2+\alpha_{(j)})})
\]


\textbf{Jean's version of the Proof for Vandegeer Thrm 3.1:}

Suppose for some constant $R$ , we define the function class 
\[
\mathcal{M}(R)=\left\{ \{g_{j}\}:(\lambda_{j}/R)^{(1-q_{j})/q_{j}}\lambda_{j}I_{j}(g_{j}-g_{j}^{*})\le R,\mbox{ }\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|_{T}\le R\right\} 
\]


Recall that 
\[
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\left|(\epsilon^{T},g_{j}-g_{j}^{*})\right|}{\left(I_{j}(g_{j})+I_{j}(g_{j}^{*})\right)^{\alpha_{j}/2}\|g_{j}-g_{j}^{*}\|^{1-\alpha_{j}/2}}=O_{p}(n^{-1/2})
\]


By our choice of $\lambda$, we have that for function sets $\{g_{j}-g_{j}^{*}\}\in\mathcal{M}(R)$,
the empirical process term decreases with $n$: 
\begin{eqnarray*}
\left|(\epsilon^{T},g_{j}-g_{j}^{*})\right| & \le & O_{P}(n^{-1/2})\left(I_{j}(g_{j})+I_{j}(g_{j}^{*})\right)^{\alpha_{j}/2}\|g_{j}-g_{j}^{*}\|^{1-\alpha_{j}/2}\\
 & \le & O_{P}(n^{-1/2})\left(\lambda_{j}^{-1/q_{j}}R^{1/q_{j}}\right)^{\alpha_{j}/2}R^{1-\alpha_{j}/2}\\
 & \le & O_{P}(n^{-1/(2+\alpha_{j})})R^{2}
\end{eqnarray*}


Hence for sufficiently large $n$, Vandegeer Lemma's 5.4 (Jean's version
below) states that the fitted functions $\hat{g_{j}}$ are also within
$R$ of the truth: 
\[
\{\hat{g_{j}}-g_{j}^{*}\}\in\mathcal{M}(R)\implies\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g_{j}}\|_{T}\le R
\]


Now we just need to determine the right value for $R$. Choose $n$
sufficiently large s.t. the penalty term for function $(j)$ is the
highest (for the truth) 
\[
\lambda_{j}^{2}I_{j}^{q_{j}}(g_{j}^{*})\le\lambda_{(j)}^{2}I_{(j)}^{q_{(j)}}(g_{(j)}^{*})\mbox{ }\forall j
\]


Then choose $R$ s.t. 
\[
\left(\lambda_{(j)}^{2}I_{(j)}^{q_{(j)}}(g_{(j)}^{*})\right)^{1/2}J^{1/2}=O_{P}(1)R
\]


Hence

\[
\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g_{j}}\|_{T}\le n^{-1/(2+\alpha_{(j)})}J^{1/2}I_{(j)}^{q_{(j)}/2}(g_{(j)}^{*})
\]



\subsection*{Result 5: Additive Model, Cross-validated $\{\lambda_{i}\}$ over
general $X_{T},X_{V}$}

Assume the same conditions as result 4, but also for the validation
set.

\textbf{Condition 2.4:} Incoherence condition on the validation set.
Let $p_{V}(\vec{x})$ be the empirical density over the validation
set. Let $p_{Vj}$ be the marginal density of $x_{j}$ for the empirical
distribution of the validation set. Let 
\[
r_{V}(\vec{x})=\frac{p_{V}(\vec{x})}{\Pi_{j=1}^{J}p_{Vj}(x_{j})},\mbox{ }\gamma_{V}^{2}=\int r_{V}(\vec{x})\Pi_{j=1}^{J}p_{Vj}(x_{j})d\mu
\]
Suppose that $\gamma_{V}<1/(J-1)$. Furthermore, we will suppose that
$\int g_{j}p_{Vj}d\mu=0$ for $j=2,...,J$.

Additionally, suppose there exist constants $M$,$M_{0}$ s.t. for
all $j$ and all $\lambda\in\Lambda$ 

\[
I_{j}\left(\hat{g}_{\lambda,j}\right)\le M\|\hat{g}_{\lambda,j}\|_{V}^{2}+M_{0}
\]


Let $\tilde{\lambda}$ be the optimal $\{\lambda_{i}\}$ as specified
in Result 4. Then

\[
\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-\hat{g}_{\tilde{\lambda},j}\|_{V}=O_{p}\left(n^{-1/(2+\alpha_{(j)})}\right)\left(1-\gamma(J-1)\right)^{\alpha_{(j)}/(2+\alpha_{(j)})}\left(\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha_{(j)}/(2+\alpha_{(j)})}+\sum_{j=1}^{J}I_{j}(g_{j}^{*})+\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}^{\alpha_{(j)}/2(2+\alpha_{(j)})}\right)
\]


and $\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}=O_{p}(1)\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{T}$
.


\subsubsection*{Proof: }

Exactly the same as Result 3

\pagebreak{}


\subsection*{Lemmas}


\subsubsection*{Lemma 1: }

Suppose for all $\lambda\in\Lambda$, the penalty function $I^{v}(g_{\lambda})$
is upper-bounded by $\|g_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}g_{\lambda}^{2}(x_{i})$
with constants $M_{0}$ and $M$:

\[
I^{v}(g_{\lambda})\le M\|g_{\lambda}\|_{n}^{2}+M_{0}
\]


Suppose there is some function $g\in\mathcal{G}$ such that
\[
\|g-g_{\lambda}\|_{n}^{1+\alpha/2}\le O_{p}(n^{-1/2})I^{\alpha/2}(g_{\lambda})
\]


Then

\[
\|g-g_{\lambda}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha v/(2+\alpha)}\|g\|_{n}^{2\alpha/v(2+\alpha)}
\]


\textbf{Proof:}

From the assumptions, we have

\begin{eqnarray*}
\|g-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(M\|g_{\lambda}\|_{n}^{2}+M_{0}\right)^{\alpha/2v}
\end{eqnarray*}


If $M_{0}>\|g_{\lambda}\|_{n}^{2}$, we're done. Otherwise,

\begin{eqnarray*}
\|g-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\|g_{\lambda}\|_{n}^{\alpha/v}\\
 & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\left(\|g_{\lambda}-g\|_{n}+\|g\|_{n}\right)^{\alpha/v}
\end{eqnarray*}


\textbf{Case 1:} $\|g_{\lambda}-g\|_{n}\ge\|g\|_{n}$

Then

\[
\|g-g_{\lambda}\|_{n}\le O_{p}(n^{-v/(2v+\alpha v-2\alpha)})M^{\alpha v^{2}/(2v+\alpha v-2\alpha)}
\]


Note that $\sup_{v}-\frac{v}{2v+\alpha v-2\alpha}=-\frac{1}{2+\alpha}$,
so this rate is faster than $O_{p}(n^{-\frac{1}{2+\alpha}})$.

\textbf{Case 2:} $\|g_{\lambda}-g\|_{n}\le\|g\|_{n}$

Then

\[
\|g-g_{\lambda}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha v/(2+\alpha)}\|g\|_{n}^{2\alpha/v(2+\alpha)}
\]


I believe we can often provide a good estimate of $M$ for the entire
class $\mathcal{G}$, which means that we can always estimate the
sample size needed to ensure this case never occurs. That is, I believe
we can often estimate $M$ s.t. 
\[
I^{v}(g)\le M\|g\|_{n}^{2}+M_{0}\forall g\in\mathcal{G}
\]



\subsubsection*{Lemma 2:}

Let $P_{n'}$ and $P_{n''}$ be empirical distributions over $\{X_{i}'\}_{i=1}^{n},\{X_{i}''\}_{i=1}^{n}$.
Let $P_{2n}=\frac{1}{2}\left(P_{n'}+P_{n''}\right)$. Suppose $X$
is bounded s.t. $|X|<R_{X}$.

Let $\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} $.
Suppose $g$ is defined over the domain over $X$ (and zero otherwise).
Suppose 
\[
\sup_{f\in\mathcal{G}'}\|f\|_{P_{2n}}\le R<\infty,\mbox{ }\sup_{f\in\mathcal{G}'}\|f\|_{\infty}\le K<\infty
\]


and 
\[
H\left(\delta,\mathcal{G}',P_{n'}\right)\le\tilde{A}\delta^{-\alpha},\mbox{ }H\left(\delta,\mathcal{G}',P_{n''}\right)\le\tilde{A}\delta^{-\alpha}
\]


Then 

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n'}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]



\paragraph{Proof:}

The proof is very similar to that in Pollard 1984 (page 32), so some
details below are omitted.

First note that for any function $f$ and $h$, we have 
\[
\|f\|_{P_{n'}}-\|h\|_{P_{n'}}\le\|f-h\|_{P_{n'}}\le\sqrt{2}\|f-h\|_{P_{2n}}
\]


Similarly for $P_{n''}$. 

Let $\{h_{j}\}_{j=1}^{N}$ be the $\sqrt{2}\delta$-cover for $\mathcal{G}'$
(where $N=N(\sqrt{2}\delta,\mathcal{G}',P_{2n})$). Let $h_{j}$ be
the closest function (in terms of $\|\cdot\|_{P_{2n}}$) to some $f\in\mathcal{G}'$.
Then 
\begin{eqnarray*}
\|f\|_{P_{n'}}-\|f\|_{P_{n''}} & \le & \|f-h_{j}\|_{P_{n'}}+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|+\|f-h_{j}\|_{P_{n''}}\\
 & \le & 4\delta+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|
\end{eqnarray*}


Therefore for $f=\frac{g^{*}-g}{I(g^{*})+I(g)}$, we have
\begin{eqnarray*}
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right) & \le & Pr\left(\sup_{j\in1:N}\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\\
 & \le & N\max_{j\in1:N}Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)
\end{eqnarray*}


Now note that 
\begin{eqnarray*}
\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right| & = & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\|h_{j}\|_{P_{n'}}+\|h_{j}\|_{P_{n''}}}\\
 & \le & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\sqrt{2}\|h_{j}\|_{P_{2n}}}
\end{eqnarray*}


By Hoeffding's inequality, 
\begin{eqnarray*}
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right) & \le & Pr\left(\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|\ge2\sqrt{2}\delta\|h_{j}\|_{P_{2n}}\right)\\
 & = & Pr\left(\left|\sum_{i=1}^{n}W_{i}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)\right|\ge2\sqrt{2}n\delta\|h_{j}\|_{P_{2n}}\right)\\
 & \le & 2\exp\left(-\frac{16\delta^{2}n^{2}\|h_{j}\|_{P_{2n}}^{2}}{4\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2}}\right)
\end{eqnarray*}


Since $\|h_{j}\|_{\infty}<K$, then

\begin{eqnarray*}
\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2} & \le & \sum_{i=1}^{n}h_{j}^{4}(x_{i}')+h_{j}^{4}(x_{i}'')\\
 & \le & nK^{2}\|h_{j}\|_{P_{2n}}^{2}
\end{eqnarray*}


Hence 
\[
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\le2\exp\left(-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Since (Pollard and Vandegeer say that) 
\[
N(\sqrt{2}\delta,\mathcal{G}',P_{2n})\le N(\delta,\mathcal{G}',P_{n''})+N(\delta,\mathcal{G}',P_{n''})
\]


then

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Using shorthand, we can write 
\[
\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}=O_{p}(n^{-1/(2+\alpha)})
\]



\subsubsection*{Lemma 3:}

Suppose the function classes $\mathcal{F}_{j}$ is a cone and $I_{j}:\mathcal{F}_{j}\mapsto[0,\infty)$
is a psuedonorm. Furthermore, suppose 
\[
H\left(\delta,\{f_{j}\in\mathcal{F}_{j}:I_{j}(f_{j})\le1\},\|\cdot\|_{n}\right)\le A_{j}\delta^{-\alpha_{j}}
\]
Then if $f_{j}^{*}\in\mathcal{F}_{j}$, then

\begin{eqnarray*}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}:f_{j}\in\mathcal{F}_{j},I_{j}(f_{j})+I_{j}(f_{j}^{*})>0\right\} ,\|\cdot\|_{n}\right) & \le & 2\sum_{j=1}^{J}A_{j}\left(\frac{\delta}{2J}\right)^{-\alpha_{j}}
\end{eqnarray*}



\paragraph*{Proof:}

Let $\tilde{f}_{j}=\frac{f_{j}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}$.
Then $\tilde{f}_{j}\in\mathcal{F}_{j}$ and $I_{j}(\tilde{f}_{j})\le1$.
Let $h_{(j)}$ be the closest function to $\tilde{f}_{j}$ in the
$\delta$ cover of $\mathcal{F}_{j}$. Similarly, let $h_{(j)}^{*}$
be the closest function to $\tilde{f}_{j}^{*}$ in the $\delta$ cover
of $\mathcal{F}_{j}$. Then 
\begin{eqnarray*}
\left\Vert \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-\left(\sum_{j=1}^{J}h_{(j)}-h_{(j)}^{*}\right)\right\Vert  & \le & \sum_{j=1}^{J}\left\Vert \frac{f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-\left(h_{(j)}-h_{(j)}^{*}\right)\right\Vert \\
 & \le & \sum_{j=1}^{J}\left\Vert \frac{f_{j}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-h_{(j)}\right\Vert +\left\Vert \frac{f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-h_{(j)}^{*}\right\Vert \\
 & \le & 2J\delta
\end{eqnarray*}


Hence

\[
H\left(2J\delta,\left\{ \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}:f_{j}\in\mathcal{F}_{j},I_{j}(f_{j})+I_{j}(f_{j}^{*})>0\right\} ,\|\cdot\|_{n}\right)\le2\sum_{j=1}^{J}A_{j}\delta^{-\alpha_{j}}
\]



\subsubsection*{Lemma 4:}

Let $p_{n}(\vec{x})$ be some empirical density and let $p_{nj}$
be the corresponding empirical marginal density of $x_{j}$. Let 
\[
r(\vec{x})=\frac{p_{n}(\vec{x})}{\Pi_{j=1}^{J}p_{nj}(x_{j})},\mbox{ }\gamma^{2}=\int(r(\vec{x})-1)^{2}\Pi_{j=1}^{J}p_{nj}(x_{j})d\mu
\]
Suppose $\gamma<1/(J-1)$. Furthermore, suppose $\int g_{j}p_{nj}d\mu=0$
for $j=2,...,J$. Then 
\[
\left\Vert \sum_{j=1}^{J}g_{j}\right\Vert _{n}^{2}\ge(1-\gamma(J-1))\left(\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}\right)
\]



\paragraph*{Proof:}

The proof is very similar to Lemma 5.1 in Vandegeer 2014 ``The additive
model with different smoothness for the components.''

\[
\left\Vert \sum_{j=1}^{J}g_{j}\right\Vert _{n}^{2}=\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}+\sum_{j\ne k}\int g_{j}g_{k}p_{n}(\vec{x})d\mu
\]


We bound the latter term:

\begin{eqnarray*}
\left|\int g_{j}g_{k}p_{n}(\vec{x})d\mu\right| & = & \left|\int g_{j}g_{k}\left(r(\vec{x})-1\right)\Pi_{j=1}^{J}p_{nj}(x_{j})d\mu\right|\\
 & \le & \gamma\left|\int g_{j}^{2}g_{k}^{2}\Pi_{j=1}^{J}p_{nj}(x_{j})d\mu\right|^{1/2}\\
 & = & \gamma\|g_{j}\|_{n}\|g_{k}\|_{n}
\end{eqnarray*}


Hence

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}g_{j}\right\Vert _{n}^{2} & \ge & \sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}-\gamma\sum_{j\ne k}\|g_{j}\|_{n}\|g_{k}\|_{n}\\
 & \ge & (1-\gamma(J-1))\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}+\gamma\sum_{j<k}\left(\|g_{j}\|_{n}-\|g_{k}\|_{n}\right)^{2}\\
 & \ge & (1-\gamma(J-1))\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}
\end{eqnarray*}



\subsubsection*{Vandegeer's Lemma 5.4 (Jean's version)}

Let 
\[
\tau_{R}(\{f_{j}\})=\|\sum f_{j}\|_{T}+\sum_{j=1}^{J}(\lambda_{j}/R)^{(1-q_{j})/q_{j}}\lambda_{j}I_{j}(f_{j})
\]


Suppose 
\[
\sum_{j=1}^{J}\lambda_{j}^{2}I_{j}^{q_{j}}(f_{j}^{*})\le\delta_{0}^{2}R^{2}
\]


and for all function sets $\{f_{j}\}$ s.t. $\tau_{R}(\{f_{j}\})\le R$,
suppose 
\[
\sup_{f_{j}}\left|\left(\epsilon_{T},f_{j}\right)\right|\le\delta_{0}^{2}R^{2}
\]


Let 
\[
\hat{f}_{j}=\arg\min\|y-\sum_{j=1}^{J}f_{j}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{2}I_{j}^{q_{j}}(f_{j})
\]


Then $\tau_{R}\left(\left\{ \hat{f}_{\lambda,j}-f_{j}^{*}\right\} \right)\le R$.


\paragraph*{Proof}

We use the convexity of the penalties and the least squares function.
Consider $\tilde{f_{j}}=t\hat{f}_{j}+(1-t)f_{j}^{*}$ where 
\begin{eqnarray*}
t & = & \frac{R}{R+\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\})}
\end{eqnarray*}


First note that by convexity, 
\begin{eqnarray*}
\tau_{R}(\{\tilde{f_{j}}-f_{j}^{*}\}) & = & \frac{R}{R+\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\})}\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\})\le R
\end{eqnarray*}


Hence 
\[
\sup_{f_{j}}\left|\left(\epsilon_{T},f_{j}^{*}-\tilde{f_{j}}\right)\right|\le\delta_{0}^{2}R^{2}
\]


So by the basic inequality,

\begin{eqnarray*}
\|\sum_{j=1}^{J}f_{j}^{*}-\sum_{j=1}^{J}\tilde{f_{j}}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{2}I_{j}^{q_{j}}(\tilde{f_{j}}) & \le & \sum_{j=1}^{J}\left|\left(\epsilon_{T},f_{j}^{*}-\tilde{f_{j}}\right)\right|+\sum_{j=1}^{J}\lambda_{j}^{2}I_{j}^{q_{j}}(f_{j}^{*})
\end{eqnarray*}


and with gross algebra, we can show that

\[
(\lambda_{j}/R)^{(1-q_{j})/q_{j}}\lambda_{j}I_{j}(\tilde{f_{j}}-f_{j}^{*})\le4\delta_{0}R
\]


Then

\begin{eqnarray*}
\frac{R}{R+\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\})}\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\}) & = & \tau_{R}(\{\tilde{f_{j}}-f_{j}^{*}\})\\
 & = & \|\sum\tilde{f_{j}}-f_{j}^{*}\|_{T}+\sum_{j=1}^{J}(\lambda_{j}/R)^{(1-q_{j})/q_{j}}\lambda_{j}I_{j}(\tilde{f_{j}}-f_{j}^{*})\\
 & \le & O_{P}(1)J\delta_{0}R
\end{eqnarray*}


So for small enough $\delta_{0}$, we have 
\[
\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\})\le R
\]


\pagebreak{}


\section{What if we can't bound the penalty?}

Now suppose the problem does not satisfy the assumption that 
\[
I(g_{\lambda})\le M\|g_{\lambda}\|^{2}+M_{0}
\]


Then let's consider a modified way of choosing $\lambda$.

Select lambda s.t. 
\[
\hat{\lambda}=\arg\min_{\lambda\in\Lambda}\|y-g_{\lambda}\|_{V}^{2}\mbox{ where }\Lambda=\left\{ \lambda:n^{-\tau}I(\hat{g}_{\lambda})\le\|y-\hat{g}_{\lambda}\|_{V}\right\} 
\]


To get the optimal convergence rate for $\|g^{*}-\hat{g}_{\lambda}\|_{V}$
, choose $\tau=\frac{1}{2(1+\alpha)}$. Then we get the rate 
\[
\|g^{*}-\hat{g}_{\lambda}\|_{V}=O_{P}(n^{-1/2(1+\alpha)})
\]



\subsubsection*{Proof:}

For sufficiently large $n$, $\tilde{\lambda}$ will be in the set
$\Lambda$ with high probability. To see this, note that 
\begin{eqnarray*}
n^{-\tau}I(\hat{g}_{\tilde{\lambda}}) & = & O_{p}(n^{-\tau})I(g^{*})\\
 & \le & \left|\|y-g^{*}\|_{V}-\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}\right|\\
 & \le & \|y-\hat{g}_{\tilde{\lambda}}\|_{V}
\end{eqnarray*}


where the first inequality comes from the fact that $\|y-g^{*}\|_{V}=O_{P}(\sigma)$
with high probability and $\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}=O_{P}(n^{-1/(2+\alpha)})$.

Now proceed with the basic inequality. We know that

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}^{2}\le2\left|\left(\epsilon,\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{V}\right|+2\left|\left(g^{*}-\hat{g}_{\tilde{\lambda}},\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{V}\right|
\]


The problematic case is when$\left|\left(\epsilon,\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{T}\right|$
is the bigger term on the RHS.

By Vandegeer (10.6),
\begin{eqnarray*}
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}^{2} & \le & O_{P}(n^{-1/2})\|\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\|^{1-\alpha/2}\left(I(\hat{g}_{\tilde{\lambda}})+I(\hat{g}_{\hat{\lambda}})\right)^{\alpha/2}
\end{eqnarray*}


We're done if $I(\hat{g}_{\tilde{\lambda}})>I(\hat{g}_{\hat{\lambda}})$.
Otherwise, suppose $I(\hat{g}_{\tilde{\lambda}})<I(\hat{g}_{\hat{\lambda}})$.
By definition of $\Lambda$ , we have

\begin{eqnarray*}
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}^{1+\alpha/2} & \le & O_{P}(n^{-1/2})I(\hat{g}_{\hat{\lambda}})^{\alpha/2}\\
 & \le & O_{P}(n^{(-1+\tau\alpha)/2})\|y-\hat{g}_{\hat{\lambda}}\|_{V}^{\alpha/2}\\
 & \le & O_{P}(n^{(-1+\tau\alpha)/2})\left(\|y-g^{*}\|_{V}+\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V}+\|\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\|_{V}\right)^{\alpha/2}
\end{eqnarray*}


The slowest case is when $\|y-g^{*}\|_{V}$ is the largest among the
three terms. We have the rate

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}\le O_{P}(n^{(-1+\tau\alpha)/(2+\alpha)})
\]


The optimal convergence rate is attained if we choose 
\[
-\tau=\frac{-1+\tau\alpha}{2+\alpha}
\]


That is, we get 
\[
\tau=\frac{1}{2(1+\alpha)}
\]


\pagebreak{}


\section{Examples}

Our goal here is to show that the assumptions hold for various examples.


\subsection{Penalties that are compatible with the L2 norm}

Lasso, Fused Lasso, Generalized Lasso, Ridge, Elastic Net

To see this works for the Generalized lasso:

Let $D$ be the fixed penalty matrix. Let $D_{max}$ be its maximum
eigenvalue. Suppose the smallest eigenvalue of $X^{T}X$ stays away
from zero. Then for some constants $M_{0}$ and $M$,

\begin{eqnarray*}
\|D\beta\|_{1} & \le & D_{max}\|\beta\|_{1}\\
 & \le & D_{max}M\|X^{T}\beta\|_{2}^{2}+M_{0}
\end{eqnarray*}



\subsection{Sobolev Norm}

Suppose $\mathcal{G}$ is the class of smooth functions $g:[0,1]\mapsto\mathbb{R}$
s.t. $I_{(k)}(g)=\sqrt{\int_{0}^{1}g(t)^{2}dt}+\sqrt{\int_{0}^{1}g^{(k)}(t)^{2}dt}<\infty$. 

\[
\arg\min_{g\in\mathcal{G}}\|y-g(x_{1})\|_{T}^{2}+\lambda_{g}^{2}I_{(k)}^{2}(g)
\]


If we reformulate this using the Reproducing Kernel Hilbert space
$\mathcal{H}$, the criterion becomes

\[
\arg\min_{g_{H}\in\mathcal{H},g_{\perp}\in\mathcal{H}^{\perp}}\|y-g_{H}(x_{1})+g_{\perp}(x_{1})\|_{T}^{2}+\lambda_{g}^{2}I_{(k)}^{2}(g_{H})
\]


If $\Omega$ is the kernel for $\mathcal{H}$ (with respect to this
dataset), then

\[
g_{H}=\Omega_{n}\alpha,\mbox{ }I_{(k)}^{2}(g_{H})=\alpha^{T}\Omega_{n}\alpha
\]


and for some other matrix $\Sigma$, we have 
\[
g_{\perp}=\Sigma\beta
\]


\textbf{Assumption 1:} Show for some constant $K$, 
\[
\frac{\|g\|_{\infty}}{I_{(k)}(g)}\le K
\]


\textbf{Proof:}

Let $c$ be some value s.t. $\|g\|_{2}=g(c)$. This must exist since
$g$ is continuous.

\begin{eqnarray*}
g(x) & \le & g(c)+\int_{c}^{x}g'(u)du\\
 & \le & \|g\|_{2}+\int_{0}^{1}g^{(m)}(u)du\\
 & \le & \|g\|_{2}+\int_{0}^{1}\left|g^{(m)}(u)\right|du\\
 & \le & \|g\|_{2}+C\sqrt{\int_{0}^{1}\left|g^{(m)}(u)\right|^{2}du}\\
 & \le & CI_{(k)}(g)
\end{eqnarray*}


\textbf{Lemma: }The entropy is bounded for the fitted model:

\[
\arg\min_{g_{H}\in\mathcal{H},g_{\perp}\in\mathcal{H}^{\perp}}\|y-g_{H}(x_{1})+g_{\perp}(x_{1})\|_{T}^{2}+\lambda_{g}^{2}I_{(k)}^{2}(g_{H})
\]


Using the Hilbert Space reproducing kernel, we find that an equivalent
optimization problem is 
\[
\hat{\alpha}_{\lambda^{2}},\hat{\beta}=\arg\min_{\alpha,\beta}\|y-Y\beta-\Omega\alpha\|_{T}^{2}+\lambda^{2}\alpha^{T}\Omega\alpha
\]


where $Y^{T}\Omega=0$. Suppose the $Y^{T}Y$ has its minimum eigenvalue
bounded away from zero and the minimum eigenvalue of $\Omega$ is
on the order of$O_{p}(n^{-\tau})$.

Suppose the maximum $\lambda\in\Lambda$ is 1. Then the entropy is
\[
H\left(\delta,\{\Omega\hat{\alpha}_{\lambda}:\lambda\in\Lambda\},\|\cdot\|_{T}\right)\le\log\left(\frac{K}{\delta}\right)+(\tau+1)\log n
\]


where $K$ is a constant s.t. $\|y-Y\hat{\beta}\|\le K$.

\textbf{Proof:}

First note that we know that 
\begin{eqnarray*}
\hat{\beta} & = & (Y^{T}Y)^{-1}Y^{T}y_{T}
\end{eqnarray*}


Given that $Y^{T}Y$ has a minimum eigenvalue value bounded away from
zero, there is some $K$ s.t. $\|y-Y\hat{\beta}\|\le K$.

Given $\hat{\beta}$ , we have 
\begin{eqnarray*}
\hat{\alpha}_{\lambda^{2}} & = & \frac{1}{n}\left(\frac{1}{n}\Omega^{T}\Omega+\lambda^{2}\Omega\right)^{-1}\Omega^{T}(y-Y\hat{\beta})
\end{eqnarray*}


We know that $\Omega$ is a PD matrix. We express $\Omega=QDQ^{T}$
where $Q$ is orthogonal and $D$ is a diagonal matrix with entries
$\omega_{i}$. Then

\[
\Omega\hat{\alpha}_{\lambda^{2}}=Q\left(D+n\lambda^{2}\right)^{-1}DQ^{T}(y-Y\hat{\beta})
\]


Hence we can measure the difference between models fitted with different
$\lambda$ values:

\begin{eqnarray*}
\|\Omega\hat{\alpha}_{\lambda^{2}}-\Omega\hat{\alpha}_{\lambda^{2}+\delta}\| & = & \left\Vert Q\left(\left(D+n\lambda^{2}\right)^{-1}-\left(D+n\lambda^{2}+n\delta\right)^{-1}\right)DQ^{T}(y-Y\hat{\beta})\right\Vert \\
 & \le & \max_{i=1:n}\left|\frac{n\delta\omega_{i}}{(\omega_{i}+n\lambda^{2})(\omega_{i}+n\lambda^{2}+n\delta)}\right|\|y-Y\hat{\beta}\|\\
 & \le & \delta n\max_{i=1:n}\left|\frac{\omega_{i}}{(\omega_{i}+n\lambda^{2})^{2}}\right|\|y-Y\hat{\beta}\|\\
 & \le & \delta n\max_{i=1:n}\left|\omega_{i}^{-1}\right|\|y-Y\hat{\beta}\|\\
 & = & \delta nO_{P}(n^{\tau})\|y-Y\hat{\beta}\|
\end{eqnarray*}


Suppose we have $\lambda_{max}^{2}=1$. 

Hence
\[
\|\Omega\hat{\alpha}_{\lambda^{2}}-\Omega\hat{\alpha}_{\lambda^{2}+\delta}\|\le\delta O_{P}(n^{\tau+1})K
\]


So we can define a $\tilde{\delta}$-covering set for $\{\Omega\hat{\alpha}_{\lambda^{2}}\}$
using $\{\Omega\hat{\alpha}_{i\delta}:i=0,1,...,\frac{1}{\delta}\}$
where $\delta=\tilde{\delta}O_{P}(n^{-(\tau+1)})/K$. So the covering
set contains a total of $\frac{K}{\tilde{\delta}}O_{P}(n^{\tau+1})$
functions. Hence the entropy is 
\[
\log\left(\frac{K}{\tilde{\delta}}\right)+(\tau+1)\log n
\]


\textbf{Corrollary:}

Now suppose we have a training and validation sets $(y_{T},X_{T})$
and $(y_{V},X_{V})$. We rewrite the joint optimization problem with
two kernel matrices $\Omega_{T}$ and$\Omega_{V}$

\[
\arg\min_{\lambda}\|y-Y\hat{\beta}-\Omega_{V}\hat{\alpha}_{\lambda^{2}}\|_{V}^{2}
\]


\[
\hat{\alpha}_{\lambda^{2}},\hat{\beta}=\arg\min_{\alpha,\beta}\|y-Y\beta-\Omega_{T}\alpha\|_{T}^{2}+\lambda^{2}\alpha^{T}\Omega_{T}\alpha
\]


where $Y^{T}\Omega_{T}=0$ and $Y^{T}\Omega_{V}=0$. Suppose the $Y^{T}Y$
has its minimum eigenvalue bounded away from zero.

Let $\vec{\omega_{T}}$ and $\vec{\omega_{V}}$ be the eigenvalues
of $\Omega_{V}$ and $\Omega_{T}$. Suppose that

\[
\frac{\max_{i=1:n}\left|\omega_{Vi}\right|}{\min_{i=1:n}\left|\omega_{Ti}\right|^{2}}=O_{p}(n^{\tau})
\]


Suppose the maximum $\lambda\in\Lambda$ is 1. 

Then the entropy is
\[
H\left(\delta,\{\Omega_{V}\hat{\alpha}_{\lambda}:\lambda\in\Lambda\},\|\cdot\|_{T}\right)\le\log\left(\frac{K}{\delta}\right)+(\tau+1)\log n
\]


where $K$ is a constant s.t. $\|y-Y\hat{\beta}\|\le K$.

\textbf{Proof:}

Proof structure is similar to above.

\begin{eqnarray*}
\Omega_{V}\hat{\alpha}_{\lambda^{2}} & = & \Omega_{V}\Omega^{-1}\left(\Omega+n\lambda^{2}I\right)^{-1}\Omega^{T}(y-Y\hat{\beta})\\
 & = & \Omega_{V}Q_{T}D_{T}^{-1}\left(D_{T}+n\lambda^{2}I\right)^{-1}D_{T}Q_{T}^{T}(y-Y\hat{\beta})
\end{eqnarray*}


Hence we can measure the difference between models fitted with different
$\lambda$ values:

\begin{eqnarray*}
\|\Omega_{V}\hat{\alpha}_{\lambda^{2}}-\Omega_{V}\hat{\alpha}_{\lambda^{2}+\delta}\| & = & \left\Vert \Omega_{V}Q_{T}D_{T}^{-1}\left(\left(D_{T}+n\lambda^{2}I\right)^{-1}-\left(D_{T}+n(\lambda^{2}+\delta)I\right)^{-1}\right)D_{T}Q_{T}^{T}(y-Y\hat{\beta})\right\Vert \\
 & \le & n\delta\max\left|\omega_{Vi}\right|\max_{i=1:n}\left|\frac{1}{(\omega_{Ti}+n\lambda^{2})(\omega_{Ti}+n\lambda^{2}+n\delta)}\right|\|y-Y\hat{\beta}\|\\
 & \le & \delta n\frac{\max_{i=1:n}|\omega_{Vi}|}{\min_{i=1:n}|\omega_{Ti}^{2}|}K\\
 & = & \delta O_{P}(n^{\tau+1})K
\end{eqnarray*}


So we can define a $\tilde{\delta}$-covering set for $\{\Omega_{V}\hat{\alpha}_{\lambda^{2}}\}$
using $\{\Omega_{V}\hat{\alpha}_{i\delta}:i=0,1,...,\frac{1}{\delta}\}$
where $\delta=\tilde{\delta}O_{P}(n^{-(\tau+1)})/K$. So the covering
set contains a total of $\frac{K}{\tilde{\delta}}O_{P}(n^{\tau+1})$
functions. Hence the entropy is 
\[
\log\left(\frac{K}{\tilde{\delta}}\right)+(\tau+1)\log n
\]

\end{document}
