%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\subsection*{Definitions}

We find the best model for $y$ over function class $\mathcal{G}$.
Presume $g^{*}\in\mathcal{G}$ is the true model and 
\[
y=g^{*}(X)+\epsilon
\]


Given a training set $T$ , We define the fitted models 
\[
\hat{g}_{\lambda}=\|y-g\|_{T}^{2}+\lambda^{2}I^{v}(g)
\]


Given a validation set $T$ , let the CV-fitted model be 
\[
\hat{g}_{\hat{\lambda}}=\arg\min_{\lambda}\|y-\hat{g}_{\lambda}\|_{V}^{2}
\]


We will suppose $I(g^{*})>0$.


\subsection*{Assumptions}

Suppose we have sub-Gaussian errors $\epsilon$ for constants $K$
and $\sigma_{0}^{2}$:
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Suppose $v>2\alpha/(2+\alpha)$. 

Suppose that the entropy of the class $\mathcal{G}'$ is 
\begin{eqnarray*}
H\left(\delta,\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} ,P_{n}\right) & \le & \tilde{A}\delta^{-\alpha}
\end{eqnarray*}
 

Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by $\|\hat{g}_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}\hat{g}_{\lambda}(x_{i})$.
See Lemma 1 below for the specific assumption. This assumption includes
Ridge, Lasso, Generalized Lasso, and the Group Lasso.


\subsection*{Result 1: Single $\lambda$, Single Penalty, cross-validation over
$X_{T}=X_{V}$}

For now, we will suppose $P_{n}=\{X_{i}\}_{i=1}^{n}$ are the same
between the validation and training set. 

Also, suppose the penalty normalizes the empirical norm such that:

\[
\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{n}}{I(g)+I(g^{*})}\le R<\infty
\]


Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by its $L_{2}$-norm with some constant $M$ and
$M_{0}$ such that

\[
I^{v}(\hat{g}_{\lambda})\le M\|\hat{g}_{\lambda}\|_{n}^{2}+M_{0}
\]


Then

\[
\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{n}=O_{p}(n^{-1/(2+\alpha)})\left(M^{\alpha/v(2+\alpha)}\|g^{*}\|_{n}^{\alpha/2v(2+\alpha)}\vee I^{2\alpha/(2+\alpha)}(g^{*})\right)
\]



\subsubsection*{Proof}

Let $\tilde{\lambda}$ be the optimal $\lambda$ under the given assumptions,
as specified by Van de geer. From the definition of $\hat{\lambda}$,
we get the following basic inequality 
\begin{eqnarray*}
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{V}^{2} & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}+2(\epsilon,\hat{g}_{\hat{\lambda}}-\hat{g}_{\tilde{\lambda}})_{V}\\
 & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}+2(\epsilon,\hat{g}_{\hat{\lambda}}-g^{*})_{V}+2(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\\
 & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}+2\left|(\epsilon,\hat{g}_{\hat{\lambda}}-g^{*})_{V}\right|+2\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|
\end{eqnarray*}


By considering the largest term on the RHS, we have following three
cases.

\textbf{Case 1:} $\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}$ is
the largest

Since we have assumed that the validation and training set are equal,
then $\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}$ converges at the optimal
rate $O_{p}(n^{-1/(2+\alpha)})$.

\textbf{Case 2:} $\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|$
is the largest

In this case, since $\epsilon_{V}$ is independent of $\hat{g}_{\tilde{\lambda}}$,
then by Cauchy Schwarz, 
\begin{eqnarray*}
\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right| & \le & \|\epsilon_{V}\|\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}\\
 & \le & O_{p}\left(n^{-1/2}\right)\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}
\end{eqnarray*}


Hence $\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|$
will shrink a bit faster than the optimal rate at a rate of $O_{p}(n^{-(\frac{1}{2+\alpha}+\frac{1}{2})})$.

\textbf{Case 3:} $\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|$
is the largest.

By the assumptions given, Vandegeer (10.6) gives us that 
\[
\sup_{g\in\mathcal{G}}\frac{|(\epsilon,g-g*)_{n}|}{\|g-g*\|_{n}^{1-\alpha/2}(I(g^{*})+I(g))^{\alpha/2}}=O_{p}(n^{-1/2})
\]


Hence 
\[
\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|\le O_{p}(n^{-1/2})\|\hat{g}_{\hat{\lambda}}-g*\|_{n}^{1-\alpha/2}(I(g^{*})+I(\hat{g}_{\hat{\lambda}}))^{\alpha/2}
\]


If $I(g^{*})\ge I(g_{\hat{\lambda}})$ , then

\[
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{V}\le O_{p}(n^{-1/(2+\alpha)})I(g^{*})^{\alpha/(2+\alpha)}
\]


Otherwise, we have

\[
\|\hat{g}_{\hat{\lambda}}-g*\|_{n}^{1+\alpha/2}\le O_{p}(n^{-1/2})I(\hat{g}_{\hat{\lambda}}){}^{\alpha/2}
\]


By Lemma 1 below, using the assumption that the penalty of $\hat{g}_{\lambda}$
is bounded above by its $L_{2}(P_{n})$ norm, we have that

\[
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha/v(2+\alpha)}\|g^{*}\|_{n}^{\alpha/2v(2+\alpha)}
\]



\subsection*{Result 2: Single $\lambda$, Single Penalty, cross-validation over
general $X_{T},X_{V}$}

Now suppose that the training and validation set are independently
sampled, so the values $X_{i}$ are not necessarily the same. Suppose
$X$ is bounded s.t. $|X|\le R_{X}$ and the domain of $g\in\mathcal{G}$
is over $(-R_{X},R_{X})$.

We suppose the training and validation sets are both of size $n$.

Suppose the penalty normalizes the empirical norm as follows:

\[
\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{T}}{I(g)+I(g^{*})}\le R<\infty,\mbox{ }\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{V}}{I(g)+I(g^{*})}\le R<\infty
\]


Suppose that 
\begin{eqnarray*}
\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{\infty}}{I(g)+I(g^{*})} & \le & K<\infty
\end{eqnarray*}


Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by its $L_{2}$-norm with constants $M$ and $M_{0}$:

\[
I^{v}(\hat{g}_{\lambda})\le M\left(\|\hat{g}_{\lambda}\|_{T}^{2}+\|\hat{g}_{\lambda}\|_{V}^{2}\right)+M_{0}=M\|\hat{g}_{\lambda}\|_{2n}^{2}+M_{0}
\]


Then for any $\xi>0$,

\[
\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{V}=O_{p}(n^{-1/(2+\alpha+\xi)})I(g^{*})
\]



\paragraph{Proof:}

We follow the same proof structure of going thru the three cases,
modifying the proofs as appropriate:

\textbf{Case 1:} $\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}$ is
the largest

By Lemma 2, we have
\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Hence for any $\xi>0$,
\[
\frac{\left|\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{T}-\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}\right|}{I(g^{*})+I(\hat{g}_{\tilde{\lambda}})}\le O_{p}(n^{-1/(2+\alpha+\xi)})
\]


Therefore
\begin{eqnarray*}
\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V} & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{T}+O_{p}(n^{-1/(2+\alpha+\xi)})\left(I(g^{*})+I(\hat{g}_{\tilde{\lambda}})\right)\\
 & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{T}+O_{p}(n^{-1/(2+\alpha+\xi)})I(g^{*})
\end{eqnarray*}


Hence we can attain a rate that is infinitely close to the optimal
rate.

\textbf{Case 2:} $\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|$
is the largest

The same proof still holds.

\textbf{Case 3:} $\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|$
is the largest.

Again, we have by Van de geer (10.6),
\[
\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|\le O_{p}(n^{-1/2})\|\hat{g}_{\hat{\lambda}}-g*\|_{V}^{1-\alpha/2}(I(g^{*})+I(\hat{g}_{\hat{\lambda}}))^{\alpha/2}
\]


If $I(g^{*})\ge I(g_{\hat{\lambda}})$ is true, then result is clearly
attained.

Otherwise, we have

\[
\|\hat{g}_{\hat{\lambda}}-g*\|_{V}^{1+\alpha/2}\le O_{p}(n^{-1/2})I(\hat{g}_{\hat{\lambda}}){}^{\alpha/2}
\]


By Lemma 1 below, since the penalty is bounded above by the $L_{2}(P_{n})$
norm, it follows that

\[
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{V}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha/v(2+\alpha)}\|g^{*}\|_{2n}^{\alpha/2v(2+\alpha)}
\]



\subsection*{Result 3: Single $\lambda$, Multiple Penalties, cross-validation
over general $X_{T},X_{V}$}

Consider an additive model:

\[
y=\sum_{j=1}^{J}g_{j}^{*}+\epsilon
\]


We fit the model by least squares with separate penalties for each
function $g_{j}$:
\[
\{\hat{g}_{j}\}_{j=1}^{J}=\arg\min_{g_{j}\in\mathcal{G}_{j}}\|y-\sum_{j=1}^{J}g_{j}\|_{T}^{2}+\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j})
\]


Suppose for all $j$, there is some $0<\alpha_{j}<2$ s.t. for all
$\delta>0$, 
\[
H\left(\delta,\left\{ \frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le A\delta^{-\alpha_{j}}
\]


If 
\[
\tilde{\lambda}_{T}^{-1}=O_{p}\left(n^{1/(2+\alpha_{max})}\right)I_{(j)}^{(2v_{(j)}-2\alpha_{max}+v_{(j)}\alpha_{max})/2(2+\alpha_{max})}(g_{(j)}^{*})
\]


then
\[
\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|_{T}^{2}=O_{p}\left(\tilde{\lambda}_{T}\right)\left(1\vee J^{\frac{1-\alpha_{max}}{2+\alpha_{max}}}\vee\max_{j\in1:J}\left\{ J^{\frac{v_{j}-v_{j}\alpha_{max}+\alpha_{max}}{2v_{j}+v_{j}\alpha_{max}-2\alpha_{max}}}\right\} \right)\max_{j\in1:J}\left(\left(I_{j}^{v_{j}}(g_{j}^{*})\right)^{1/2}\right)
\]



\subsubsection*{Proof:}

We have the basic inequality

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}(\hat{g}_{j})\le2\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|+\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}\left(g_{j}^{*}\right)
\]


\textbf{Case 1:}

Suppose the RHS is dominated by the penalty term:
\[
\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\le\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}\left(g_{j}^{*}\right)
\]


It follows that

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}^{v_{j}}(\hat{g}_{j})\le O_{p}(1)\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}\left(g_{j}^{*}\right)
\]


Obviously,

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}\le O_{p}(1)\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}\left(g_{j}^{*}\right)\le O_{p}(1)\lambda^{2}\max_{j\in1:J}I_{j}^{v_{j}}\left(g_{j}^{*}\right)
\]


Therefore

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}(\lambda)\left(\sup_{j\in1:J}I_{j}^{v_{j}}\left(g_{j}^{*}\right)\right)^{1/2}
\]


\textbf{Case 2:}

Suppose the RHS is dominated by the empirical process
\[
\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\ge\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}\left(g_{j}^{*}\right)
\]


We bound the empirical process as follows. By Lemma 5, we know for
sufficiently small $\delta>0$, 

\[
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\max_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le2AJ\left(\frac{\delta}{2J\left(1+R\right)}\right)^{-\alpha_{max}}
\]


Hence by Lemma 6,

\[
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\left|\left(\epsilon_{T},\sum_{j=1}^{J}g_{j}-g_{j}^{*}\right)\right|}{\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|^{1-\alpha_{max}/2}\max_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)^{\alpha_{max}/2}}=O_{p}\left(n^{-1/2}J^{(1-\alpha_{max})/2}\right)
\]


Consequently, in this case, the basic inequality becomes 
\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}(\hat{g}_{j})\le O_{p}\left(n^{-1/2}J^{(1-\alpha_{max})/2}\right)\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha_{max}/2}\max_{j\in1:J}\left(I(\hat{g}_{j})+I(g_{j}^{*})\right)^{\alpha_{max}/2}
\]


Let $(j)=\arg\max_{j\in1:J}I(\hat{g}_{j})+I(g_{j}^{*})$. 

\textbf{Case 2a:} Suppose $I(\hat{g}_{(j)})\le I(g_{(j)}^{*})$. 

Then 
\begin{eqnarray*}
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T} & \le & O_{p}\left(n^{-1/(2+\alpha_{max})}J^{(1-\alpha_{max})/(2+\alpha_{max})}\right)I_{(j)}^{\alpha_{max}/(2+\alpha_{max})}(g_{(j)}^{*})\\
 & \le & O_{p}(\lambda)J^{(1-\alpha_{max})/(2+\alpha_{max})}\sup_{j\in1:J}\left(I_{j}^{v_{j}}(g_{j}^{*})\right)^{1/2}
\end{eqnarray*}


\textbf{Case 2b:} Suppose $I(\hat{g}_{(j)})\ge I(g_{(j)}^{*})$. 

Then 
\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}\left(n^{-1/(2+\alpha_{max})}J^{(1-\alpha_{max})/(2+\alpha_{max})}\right)I_{(j)}^{\alpha_{max}/(2+\alpha_{max})}(\hat{g}_{(j)})
\]


and 
\[
\lambda^{2}I_{(j)}^{v_{(j)}}(\hat{g}_{(j)})\le\lambda^{2}\sum_{j=1}^{J}I_{j}^{v_{j}}(\hat{g}_{j})\le O_{p}\left(n^{-1/2}J^{(3-\alpha_{max})/2}\right)\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha_{max}/2}I_{(j)}^{\alpha_{max}/2}(\hat{g}_{(j)})
\]


Hence

\[
I_{(j)}^{v_{(j)}-\alpha_{max}/2}(\hat{g}_{(j)})\le O_{p}\left(n^{-1/2}J^{(3-\alpha_{max})/2}\right)\lambda^{-2}\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha_{max}/2}
\]


Simplifying, we get 
\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}\left(n^{-v_{(j)}/(2v_{(j)}-2\alpha_{max}+v_{(j)}\alpha_{max})}J^{\frac{v_{(j)}-v_{(j)}\alpha_{max}+\alpha_{max}}{2v_{(j)}+v_{(j)}\alpha_{max}-2\alpha_{max}}}\right)\lambda^{-2\alpha_{max}/(2v_{(j)}-2\alpha_{max}+v_{(j)}\alpha_{max})}
\]


By our choice of $\tilde{\lambda}$, we have
\begin{eqnarray*}
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T} & \le & O_{p}\left(\lambda\right)J^{\frac{v_{(j)}-v_{(j)}\alpha_{max}+\alpha_{max}}{2v_{(j)}+v_{(j)}\alpha_{max}-2\alpha_{max}}}I_{(j)}^{\alpha_{max}/(2+\alpha_{max})}(g_{(j)}^{*})\\
 & \le & O_{p}\left(\lambda\right)\max_{j\in1:J}\left(J^{\frac{v_{j}-v_{j}\alpha_{max}+\alpha_{max}}{2v_{j}+v_{j}\alpha_{max}-2\alpha_{max}}}\left(I_{j}^{v_{j}}(g_{j}^{*})\right)^{1/2}\right)
\end{eqnarray*}



\subsection*{Lemmas}


\subsubsection*{Lemma 1: }

Suppose for all $\lambda\in\Lambda$, the penalty function $I^{v}(g_{\lambda})$
is upper-bounded by $\|g_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}g_{\lambda}^{2}(x_{i})$
with constants $M_{0}$ and $M$:

\[
I^{v}(g_{\lambda})\le M\|g_{\lambda}\|_{n}^{2}+M_{0}
\]


Suppose there is some function $g^{*}\in\mathcal{G}$ such that 
\[
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2}\le O_{p}(n^{-1/2})I^{\alpha/2}(g_{\lambda})
\]
then for sufficiently large $n$, 
\[
\|g^{*}-g_{\lambda}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha/v(2+\alpha)}\|g^{*}\|_{n}^{\alpha/2v(2+\alpha)}
\]


\textbf{Proof:}

From the assumption that $I^{v}(g_{\lambda})$ is upper-bounded by
$\|g_{\lambda}\|_{n}^{2}$,

\begin{eqnarray*}
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(M\|g_{\lambda}\|_{n}^{2}+M_{0}\right)^{\alpha/2v}
\end{eqnarray*}


If $M_{0}>\|g_{\lambda}\|_{n}^{2}$, then the result immediately follows.

Otherwise, if $M_{0}\le\|g_{\lambda}\|_{n}^{2}$, then

\begin{eqnarray*}
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\|g_{\lambda}\|_{n}^{\alpha/v}\\
 & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\left(\|g_{\lambda}-g^{*}\|_{n}+\|g^{*}\|_{n}\right)^{\alpha/v}
\end{eqnarray*}


\textbf{Case 1:} $\|g_{\lambda}-g^{*}\|_{n}\le\|g^{*}\|_{n}$

The result immediately follows.

\textbf{Case 2:} $\|g_{\lambda}-g^{*}\|_{n}>\|g^{*}\|_{n}$

We show for sufficiently large $n$, this case will not occur. Suppose
this case occurs. Then 
\begin{eqnarray*}
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})M^{\alpha/v(2+\alpha)}\|g_{\lambda}-g^{*}\|_{n}^{\alpha/v}
\end{eqnarray*}


Rearranging, we have that

\[
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2-\alpha/v}\le O_{p}(n^{-1/2})M^{\alpha/v(2+\alpha)}
\]


Since the LHS exponent is $1+\alpha/2-\alpha/v>0$, $\|g^{*}-g_{\lambda}\|_{n}$
decreases with $n$. With sufficiently large $n$, we can ensure that
only Case 1 occurs.

Note: I believe we can often provide a good estimate of $M$ for the
entire class $\mathcal{G}$, which means that we can always estimate
the sample size needed to ensure this case never occurs. That is,
I believe we can often estimate $M$ s.t. 
\[
I^{v}(g)\le M\|g\|_{n}^{2}+M_{0}\forall g\in\mathcal{G}
\]



\subsubsection*{Lemma 2:}

Let $P_{n'}$ and $P_{n''}$ be empirical distributions over $\{X_{i}'\}_{i=1}^{n},\{X_{i}''\}_{i=1}^{n}$.
Let $P_{2n}=\frac{1}{2}\left(P_{n'}+P_{n''}\right)$. Suppose $X$
is bounded s.t. $|X|<R_{X}$.

Let $\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} $.
Suppose $g$ is defined over the domain over $X$ (and zero otherwise).
Suppose 
\[
\sup_{f\in\mathcal{G}'}\|f\|_{P_{2n}}\le R<\infty,\mbox{ }\sup_{f\in\mathcal{G}'}\|f\|_{\infty}\le K<\infty
\]


and 
\[
H\left(\delta,\mathcal{G}',P_{n'}\right)\le\tilde{A}\delta^{-\alpha},\mbox{ }H\left(\delta,\mathcal{G}',P_{n''}\right)\le\tilde{A}\delta^{-\alpha}
\]


Then 

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]



\paragraph{Proof:}

The proof is very similar to that in Pollard 1984 (page 32), so some
details below are omitted.

First note that for any function $f$ and $h$, we have 
\[
\|f\|_{P_{n'}}-\|h\|_{P_{n'}}\le\|f-h\|_{P_{n'}}\le\sqrt{2}\|f-h\|_{P_{2n}}
\]


Similarly for $P_{n''}$. 

Let $\{h_{j}\}_{j=1}^{N}$ be the $\sqrt{2}\delta$-cover for $\mathcal{G}'$
(where $N=N(\sqrt{2}\delta,\mathcal{G}',P_{2n})$). Let $h_{j}$ be
the closest function (in terms of $\|\cdot\|_{P_{2n}}$) to some $f\in\mathcal{G}'$.
Then 
\begin{eqnarray*}
\|f\|_{P_{n'}}-\|f\|_{P_{n''}} & \le & \|f-h_{j}\|_{P_{n'}}+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|+\|f-h_{j}\|_{P_{n''}}\\
 & \le & 4\delta+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|
\end{eqnarray*}


Therefore for $f=\frac{g^{*}-g}{I(g^{*})+I(g)}$, we have
\begin{eqnarray*}
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right) & \le & Pr\left(\sup_{j\in1:N}\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\\
 & \le & N\max_{j\in1:N}Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)
\end{eqnarray*}


Now note that 
\begin{eqnarray*}
\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right| & = & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\|h_{j}\|_{P_{n'}}+\|h_{j}\|_{P_{n''}}}\\
 & \le & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\sqrt{2}\|h_{j}\|_{P_{2n}}}
\end{eqnarray*}


By Hoeffding's inequality, 
\begin{eqnarray*}
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right) & \le & Pr\left(\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|\ge2\sqrt{2}\delta\|h_{j}\|_{P_{2n}}\right)\\
 & = & Pr\left(\left|\sum_{i=1}^{n}W_{i}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)\right|\ge2\sqrt{2}n\delta\|h_{j}\|_{P_{2n}}\right)\\
 & \le & 2\exp\left(-\frac{16\delta^{2}n^{2}\|h_{j}\|_{P_{2n}}^{2}}{4\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2}}\right)
\end{eqnarray*}


Since $\|h_{j}\|_{\infty}<K$, then

\begin{eqnarray*}
\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2} & \le & \sum_{i=1}^{n}h_{j}^{4}(x_{i}')+h_{j}^{4}(x_{i}'')\\
 & \le & nK^{2}\|h_{j}\|_{P_{2n}}^{2}
\end{eqnarray*}


Hence 
\[
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\le2\exp\left(-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Since (Pollard and Vandegeer say that) 
\[
N(\sqrt{2}\delta,\mathcal{G}',P_{2n})\le N(\delta,\mathcal{G}',P_{n''})+N(\delta,\mathcal{G}',P_{n''})
\]


then

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Using shorthand, we can write that for any $\xi>0$, 
\[
\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}=O_{p}(n^{-1/(2+\alpha+\xi)})
\]



\subsubsection*{Lemma 3:}

Suppose the function class $\mathcal{F}$ is bounded s.t. $\sup_{f\in\mathcal{F}}\|f\|_{n}\le R<\infty$.
Let 
\[
\tilde{\mathcal{F}}=\left\{ \gamma f\mbox{ }:\mbox{ }f\in\mathcal{F},\gamma\in(0,1]\right\} 
\]


\[
H\left(\delta(1+R+\delta),\tilde{\mathcal{F}},\|\cdot\|_{n}\right)\le\log(1+\lfloor\frac{1}{\delta}\rfloor)+H\left(\delta,\mathcal{F},\|\cdot\|_{n}\right)
\]



\paragraph*{Proof:}

Let $\{h_{i}\}_{i=1}^{N}$ be the $\delta$-cover for $\mathcal{F}$.
Consider any $f\in\mathcal{F}$ and let $h_{(f)}$ be the closest
function in $\delta$-cover for $\mathcal{F}$. Choose $j\in\mathbb{Z}^{+}$
such that $|\gamma-\delta j|<\delta$.

\begin{eqnarray*}
\|\gamma f-\delta jh_{(f)}\|_{n} & \le & \|\gamma f-\gamma h_{(f)}\|_{n}+\|\gamma h_{(f)}-\delta jh_{(f)}\|_{n}\\
 & \le & \gamma\|f-h_{(f)}\|_{n}+|\gamma-\delta j|\|h_{(f)}\|_{n}\\
 & \le & \gamma\delta+\delta\left(\|f-h_{(f)}\|_{n}+\|f\|_{n}\right)\\
 & \le & \gamma\delta+\delta\left(\delta+R\right)\\
 & \le & \delta\left(1+R+\delta\right)
\end{eqnarray*}


Hence we have found that the following $N(1+\lfloor\frac{1}{\delta}\rfloor)$
functions form a $\delta(1+R+\delta)$-cover for $\tilde{\mathcal{F}}$:
\[
\{h_{i}\}_{i=1}^{N}\cup\left\{ j\delta h_{i}\mbox{ }:\mbox{ }j\in1:\lfloor\frac{1}{\delta}\rfloor,i\in1:N\right\} 
\]



\subsubsection*{Lemma 4:}

Define function classes $\{\mathcal{F}_{j}\}_{j=1}^{J}$ and 
\[
\tilde{\mathcal{F}}=\left\{ \sum_{j=1}^{J}f_{j}\mbox{ }:\mbox{ }f_{j}\in\mathcal{F}_{j}\right\} 
\]


Then

\[
H\left(J\delta,\tilde{\mathcal{F}},\|\cdot\|_{n}\right)\le\sum_{j=1}^{J}H\left(\delta,\mathcal{F}_{j},\|\cdot\|_{n}\right)
\]



\paragraph*{Proof:}

For every $j=1:J$, consider any $f_{j}\in\mathcal{F}_{j}$ and let
$h_{(j)}$ be the closest function in the $\delta$-cover for $\mathcal{F}_{j}$.
\begin{eqnarray*}
\|\sum_{j=1}^{J}f_{j}-\sum_{j=1}^{J}h_{(j)}\| & \le & \sum_{j=1}^{J}\|f_{j}-h_{(j)}\|\le J\delta
\end{eqnarray*}


Hence$\exp\left(\sum_{j=1}^{J}H\left(\delta,\mathcal{F}_{j},\|\cdot\|_{n}\right)\right)$
functions form a $J\delta$-cover for $\tilde{\mathcal{F}}$.


\subsubsection*{Lemma 5:}

Suppose for all $j=1,...,J$, there is some $\alpha_{j}>0$ and $A_{j}>0$
s.t. the following entropy bound holds for all $\delta>0$

\[
H\left(\delta,\left\{ \frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le A\delta^{-\alpha_{j}}
\]


Then for sufficiently small$\delta>0$, we have

\[
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sup_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le2JA\left(\frac{\delta}{2J\left(1+R\right)}\right)^{-\alpha_{max}}
\]


where $\alpha_{max}=\max_{j\in1:J}\alpha_{j}$.


\paragraph*{Proof:}

By Lemma 3,
\[
H\left(\delta(1+R+\delta),\left\{ \gamma\frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}\mbox{ }:\mbox{ }g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0,\gamma\in(0,1]\right\} ,\|\cdot\|_{T}\right)\le\log(1+\lfloor\frac{1}{\delta}\rfloor)+A\delta^{-\alpha_{j}}
\]


Note that 
\[
\frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sup_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)}=\sum_{j=1}^{J}\left(\frac{I(g_{j})+I(g_{j}^{*})}{\sup_{\ell\in1:J}I(g_{\ell})+I(g_{\ell}^{*})}\right)\frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}
\]


By Lemma 4,

\[
H\left(J\delta(1+R+\delta),\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sup_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)}\mbox{ }:\mbox{}g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le J\log(1+\lfloor\frac{1}{\delta}\rfloor)+JA\delta^{-\alpha_{j}}
\]


Hence for sufficiently small $\delta$, 
\[
H\left(J\delta(1+R+\delta),\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sup_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)}\mbox{ }:\mbox{}g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le2JA\delta^{-\alpha_{max}}
\]


Rearranging, we get 
\begin{eqnarray*}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sup_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right) & \le & 2AJ\left(\sqrt{\left(\frac{1+R}{2}\right)^{2}+\frac{\delta}{J}}-\frac{1+R}{2}\right)^{-\alpha_{max}}\\
 & \le & 2AJ\left(\frac{\delta}{2J\left(1+R\right)}\right)^{-\alpha_{max}}
\end{eqnarray*}


(Used the fact that for $b>0$ small enough, $\sqrt{a^{2}+b}-a\ge\sqrt{(a+\frac{b}{4a})^{2}}-a=\frac{b}{4a}$)


\subsubsection*{Lemma 6:}

Suppose $\epsilon_{i}$ are sub-gaussian errors and for the function
class $\mathcal{F}$, we have that for some $0<\alpha<2$, $A'>0$,
and $J>0$

\[
H\left(\delta,\mathcal{F},\|\cdot\|_{T}\right)\le A'J^{\tau}\delta^{-\alpha}\mbox{ }\forall\delta>0
\]


Then for $T=2C_{1}CA'^{1/2}J^{\tau/2}2^{1-\alpha/2}$

\[
Pr\left(\sup_{f\in\mathcal{F}}\frac{\left|\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\epsilon_{i}f(z_{i})\right|}{\|f\|_{n}^{1-\alpha/2}}\ge T\right)\le c\exp(-T^{2}/c^{2})
\]



\paragraph*{Proof:}

Follow proof for Lemma 8.4 in Vandegeer, but with $A=A'J^{-\alpha}$.
Note that we then have $A_{0}=A'^{1/2}J^{\tau/2}$. We then get 
\[
Pr\left(\sup_{f\in\mathcal{F}}\frac{\left|\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\epsilon_{i}f(z_{i})\right|}{\|f\|_{n}^{1-\alpha/2}}\ge2C_{1}CA'^{1/2}J^{\tau/2}2^{1-\alpha/2}\right)\le c\exp(-T^{2}/c^{2})
\]


Note that we can write via shorthand that
\[
\sup_{f\in\mathcal{F}}\frac{\left|\frac{1}{n}\sum_{i=1}^{n}\epsilon_{i}f(z_{i})\right|}{\|f\|_{n}^{1-\alpha/2}}=O_{p}(J^{\tau/2}n^{-1/2})
\]



\subsubsection*{Example 1: Sobelov norm (NOT DONE)}

Consider the functions 
\[
\mathcal{G}=\left\{ g:[0,1]\mapsto\mathbb{R}:\int_{0}^{1}g^{(m)}(z)^{2}dz<\infty\right\} 
\]


Suppose $x_{i}$ are all unique. Then the Sobelov norm for the class
$\{\hat{g}_{\lambda}\in\mathcal{G}:\lambda\in\Lambda\}$ is bounded
above by its $L_{2}(P_{n})$ norm. 

\[
I^{2}(\hat{g}_{\lambda})=\int_{0}^{1}\left(\hat{g}_{\lambda}^{(m)}(z)\right)^{2}dz\le2\|\hat{g}_{\lambda}\|_{n}^{2}+4I^{2}(\tilde{g})+4\|y\|_{n}^{2}\mbox{ }\forall\lambda\in\Lambda
\]


PROBLEM: as defined, it is possible that $I^{2}(\tilde{g})$ grows
with $n$, which is not okay!

\textbf{Proof:}

Let $\tilde{g}$ satisfy $\tilde{g}(x_{i})=y_{i}$ and have the smallest
value for $\int_{0}^{1}\left(\tilde{g}^{(m)}(z)\right)^{2}dz$. This
function $\tilde{g}$ should always exist. 

\textbf{Case 1:} $\lambda\le1/2$

By definition of $\hat{g}_{\lambda}$

\[
\|y-\hat{g}_{\lambda}\|_{n}^{2}+\lambda^{2}I^{2}(\hat{g}_{\lambda})\le\|y-(\tilde{g}-\lambda\hat{g}_{\lambda})\|_{n}^{2}+\lambda^{2}I^{2}(\tilde{g}-\lambda\hat{g}_{\lambda})
\]


Note that 
\begin{eqnarray*}
I^{2}(\tilde{g}-\lambda\hat{g}_{\lambda}) & = & \int_{0}^{1}\left(\tilde{g}^{(m)}-\lambda\hat{g}_{\lambda}^{(m)}\right)^{2}dz\\
 & = & 2\int_{0}^{1}\max\left(\left|\tilde{g}^{(m)}\right|^{2},\left|\lambda\hat{g}_{\lambda}^{(m)}\right|^{2}\right)dz\\
 & = & 2\left(\int_{0}^{1}\left|\tilde{g}^{(m)}\right|^{2}dz+\int_{0}^{1}\left|\lambda\hat{g}_{\lambda}^{(m)}\right|^{2}dz\right)
\end{eqnarray*}


Hence

\[
\lambda^{2}I^{2}(\hat{g}_{\lambda})\le\lambda^{2}\|\hat{g}_{\lambda}\|_{n}^{2}+2\lambda^{2}I^{2}(\tilde{g})+2\lambda^{4}I^{2}(\hat{g}_{\lambda})
\]


The following ineq follows, where the RHS is maximized when $\lambda=1/2$

\[
I^{2}(\hat{g}_{\lambda})\le\frac{\lambda^{2}}{\lambda^{2}-2\lambda^{4}}\left(\|\hat{g}_{\lambda}\|_{n}^{2}+2I^{2}(\tilde{g})\right)\le2\|\hat{g}_{\lambda}\|_{n}^{2}+4I^{2}(\tilde{g})
\]


\textbf{Case 2:} $\lambda>1/2$

By definition of $\hat{g}_{\lambda}$

\[
\|y-\hat{g}_{\lambda}\|_{n}^{2}+\lambda^{2}I^{2}(\hat{g}_{\lambda})\le\|y\|_{n}^{2}
\]


The RHS is maximized when $\lambda=1/2$, so

\[
I^{2}(\hat{g}_{\lambda})\le4\|y\|_{n}^{2}
\]


Hence we have an upper bound for the Sobelov norm

\[
I^{2}(\hat{g}_{\lambda})\le2\|\hat{g}_{\lambda}\|_{n}^{2}+4I^{2}(\tilde{g})+4\|y\|_{n}^{2}
\]



\subsubsection*{Appendix}


\paragraph{A cute lemma I found but never used: }

Supposing that $I^{v}(\hat{g}_{\lambda})$ is continuous in $\lambda$,
then given training data $T$, 
\begin{eqnarray*}
\frac{\partial}{\partial\lambda}L_{T}(\hat{g}_{\lambda},\lambda) & = & 2\lambda I^{v}(\hat{g}_{\lambda})
\end{eqnarray*}


Also, $L_{T}$ is convex in $\lambda$.

\textbf{Proof:}

By definition, 
\[
L_{T}(\hat{g}_{\lambda},\lambda)=\|y-\hat{g}_{\lambda}\|_{T}^{2}+\lambda^{2}I^{v}(\hat{g}_{\lambda})\le\|y-\hat{g}_{\lambda'}\|_{T}^{2}+\lambda^{2}I^{v}(\hat{g}_{\lambda'})=L_{T}(\hat{g}_{\lambda'},\lambda)
\]


Then we can provide upper and lower bounds for $L_{T}(\hat{g}_{\lambda_{2}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{1}},\lambda_{1})$:
\begin{eqnarray*}
L_{T}(\hat{g}_{\lambda_{2}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{1}},\lambda_{1}) & \le & L_{T}(\hat{g}_{\lambda_{1}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{1}},\lambda_{1})\\
 & = & \|y-\hat{g}_{\lambda_{1}}\|_{T}^{2}+\lambda_{2}^{2}I^{v}(\hat{g}_{\lambda_{1}})-\|y-\hat{g}_{\lambda_{1}}\|_{T}^{2}-\lambda_{1}^{2}I^{v}(\hat{g}_{\lambda_{1}})\\
 & = & (\lambda_{2}^{2}-\lambda_{1}^{2})I^{v}(\hat{g}_{\lambda_{1}})
\end{eqnarray*}


\begin{eqnarray*}
L_{T}(\hat{g}_{\lambda_{2}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{1}},\lambda_{1}) & \ge & L_{T}(\hat{g}_{\lambda_{2}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{2}},\lambda_{1})\\
 & = & \|y-\hat{g}_{\lambda_{2}}\|_{T}^{2}+\lambda_{2}^{2}I^{v}(\hat{g}_{\lambda_{2}})-\|y-\hat{g}_{\lambda_{2}}\|_{T}^{2}-\lambda_{1}^{2}I^{v}(\hat{g}_{\lambda_{2}})\\
 & = & (\lambda_{2}^{2}-\lambda_{1}^{2})I^{v}(\hat{g}_{\lambda_{2}})
\end{eqnarray*}


So suppose WLOG $\lambda_{2}>\lambda_{1}$:
\[
(\lambda_{2}+\lambda_{1})I^{v}(\hat{g}_{\lambda_{2}})\le\frac{L_{T}(\hat{g}_{\lambda_{2}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{1}},\lambda_{1})}{\lambda_{2}-\lambda_{1}}\le(\lambda_{2}+\lambda_{1})I^{v}(\hat{g}_{\lambda_{1}})
\]


So as $\lambda_{1}\rightarrow\lambda_{2}=\lambda$, we have by the
sandwich theorem,

\[
\frac{\partial}{\partial\lambda}L_{T}(\hat{g}_{\lambda},\lambda)=2\lambda I^{v}(\hat{g}_{\lambda})
\]


Furthermore, given training data $T$ 
\[
\frac{\partial}{\partial\lambda}L_{T}(\hat{g}_{\lambda},\lambda)=\frac{\partial}{\partial\lambda}\|y-\hat{g}_{\lambda}\|_{T}^{2}+2\lambda I^{v}(\hat{g}_{\lambda})+\lambda^{2}\frac{\partial}{\partial\lambda}I^{v}(\hat{g}_{\lambda})
\]


then, combining this with the lemma, we have that 
\[
\frac{\partial}{\partial\lambda}\|y-\hat{g}_{\lambda}\|_{T}^{2}=-\lambda^{2}\frac{\partial}{\partial\lambda}I^{v}(\hat{g}_{\lambda})
\]


Finally, to see that $L_{T}$ is convex in $\lambda$, note that 
\[
\frac{\partial^{2}}{\partial\lambda^{2}}L_{T}(\hat{g}_{\lambda},\lambda)=2I^{v}(\hat{g}_{\lambda})+2\lambda vI^{v-1}(\hat{g}_{\lambda})\frac{\partial}{\partial\lambda}I(\hat{g}_{\lambda})>0
\]


since $\frac{\partial}{\partial\lambda}I(\hat{g}_{\lambda})>0$.
\end{document}
