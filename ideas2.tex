%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\section{Simple model}


\subsection*{Definitions}

We find the best model for $y$ over function class $\mathcal{G}$.
Presume $g^{*}\in\mathcal{G}$ is the true model and 
\[
y=g^{*}(X)+\epsilon
\]


where $\epsilon$ are sub-Gaussian errors for constants $K$ and $\sigma_{0}^{2}$
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Given a training set $T$ , We define the fitted models 
\[
\hat{g}_{\lambda}=\|y-g\|_{T}^{2}+\lambda^{2}I^{v}(g)
\]


Given a validation set $V$ , let the CV-fitted model be 
\[
\hat{g}_{\hat{\lambda}}=\arg\min_{\lambda}\|y-\hat{g}_{\lambda}\|_{V}^{2}
\]


We will suppose $I(g^{*})>0$.


\subsection*{Assumptions}

Suppose the entropy of the class $\mathcal{G}'$ is 
\begin{eqnarray}
H\left(\delta,\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} ,P_{T}\right) & \le & \tilde{A}\delta^{-\alpha}
\end{eqnarray}


Suppose $v>2\alpha/(2+\alpha)$. 

Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by $\|\hat{g}_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}\hat{g}_{\lambda}(x_{i})$.
See Lemma 1 below for the specific assumption. This assumption includes
Ridge, Lasso, Generalized Lasso, and the Group Lasso. 


\subsection*{Result 1: Single $\lambda$, Single Penalty, cross-validation over
general $X_{T},X_{V}$}

Suppose that the training and validation set are independently sampled,
so the values $X_{i}$ are not necessarily the same. Suppose the training
and validation sets are both of size $n$. Suppose $X$ is bounded
s.t. $|X|\le R_{X}$ and the domain of $g\in\mathcal{G}$ is over
$(-R_{X},R_{X})$.

Suppose the same entropy bound (2) for both the training set $P_{T}$
and validation set $P_{V}$.

Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by its $L_{2}$-norm with some constant $M$ and
$M_{0}$ such that

\[
I^{v}(\hat{g}_{\lambda})\le M\|\hat{g}_{\lambda}\|_{n}^{2}+M_{0}
\]


Suppose the entropy bound for both training set $P_{T}$ and validation
set $P_{V}$.

Suppose that 
\begin{eqnarray*}
\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{\infty}}{I(g)+I(g^{*})} & \le & K<\infty
\end{eqnarray*}


Let $\tilde{\lambda}$ be the optimal $\lambda$ by Vandegeer. Then

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}=O_{p}\left(n^{-1/(2+\alpha)}\right)\left(I^{\alpha/(2+\alpha)}(g^{*})+I(g^{*})\right)
\]


and $\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{V}$ is of the same order
(differs by some constant).

\textbf{Proof:}

By the triangle inequality, 
\[
\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{V}\le\|\hat{g}_{\hat{\lambda}}-\hat{g}_{\tilde{\lambda}}\|_{V}+\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V}
\]


We bound each component on the RHS separately.

First bound $\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V}$. By Vandegeer
Thrm 10.2 and Lemma 2,

\begin{eqnarray*}
\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V} & \le & \|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{T}+\left|\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V}-\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{T}\right|\\
 & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)I^{\alpha/(2+\alpha)}(g^{*})+O_{p}\left(n^{-1/(2+\alpha)}\right)\left(I(g^{*})+I(\hat{g}_{\tilde{\lambda}})\right)\\
 & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)\left(I^{\alpha/(2+\alpha)}(g^{*})+I(g^{*})\right)
\end{eqnarray*}


Next bound $\|\hat{g}_{\hat{\lambda}}-\hat{g}_{\tilde{\lambda}}\|_{V}$.
The basic inequality gives us

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}^{2}\le2\left|\left(\epsilon,\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{V}\right|+2\left|\left(g^{*}-\hat{g}_{\tilde{\lambda}},\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{V}\right|
\]


\textbf{Case a:} $\left|\left(\epsilon,\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{T}\right|$
is the bigger term on the RHS

By Vandegeer (10.6),
\begin{eqnarray*}
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}^{2} & \le & O_{P}(n^{-1/2})\|\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\|^{1-\alpha/2}\left(I(\hat{g}_{\tilde{\lambda}})+I(\hat{g}_{\hat{\lambda}})\right)^{\alpha/2}
\end{eqnarray*}


If $I(\hat{g}_{\tilde{\lambda}})>I(\hat{g}_{\hat{\lambda}})$, then

\begin{eqnarray*}
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V} & \le & O_{P}(n^{-1/(2+\alpha)})I(g^{*})^{\alpha/(2+\alpha)}
\end{eqnarray*}


Otherwise, suppose $I(\hat{g}_{\tilde{\lambda}})<I(\hat{g}_{\hat{\lambda}})$.
Since $I$ is a pseudo-norm,

\begin{eqnarray*}
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V} & \le & O_{P}(n^{-1/(2+\alpha)})I(\hat{g}_{\hat{\lambda}})^{\alpha/(2+\alpha)}\\
 & \le & O_{P}(n^{-1/(2+\alpha)})\left(I(\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}})+I(\hat{g}_{\tilde{\lambda}})\right)^{\alpha/(2+\alpha)}
\end{eqnarray*}


If $I(\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}})\le I(\hat{g}_{\tilde{\lambda}})$,
then we're done. Otherwise if $I(\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}})\ge I(\hat{g}_{\tilde{\lambda}})$,
by the assumption that $I^{V}(\cdot)$ is bounded by the L2 norm,

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}\le O_{P}(n^{-1/(2+\alpha)})\left(M\|\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\|_{V}^{2}+M_{0}\right)^{\alpha/v(2+\alpha)}
\]


If $M_{0}$ is bigger, we're done. Otherwise,

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}\le O_{P}(n^{-v/(2v-2\alpha+\alpha v)})<O_{P}(n^{-1/(2+\alpha)})
\]


\textbf{Case b:} $\left|\left(g^{*}-\hat{g}_{\tilde{\lambda}},\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{V}\right|$
is the bigger term on the RHS

By Cauchy Schwarz,
\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}\le O_{P}(1)\left\Vert g^{*}-\hat{g}_{\tilde{\lambda}}\right\Vert _{V}
\]


\pagebreak{}


\section{General Additive Model}


\subsection*{Definitions}

We find the best model for $y$ over function classes $\mathcal{G}=\left\{ \sum_{j=1}^{J}g_{j}:\mbox{ }g_{j}\in\mathcal{G}_{j}\right\} $.
Suppose we observe:

\[
y=\sum_{j=1}^{J}g_{j}^{*}+\epsilon
\]
 where $\sum_{j=1}^{J}g_{j}^{*}\in\mathcal{G}$. Suppose $\epsilon$
are sub-Gaussian errors for constants $K$ and $\sigma_{0}^{2}$:
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Given a training set $T$ , we fit models by least squares with multiple
penalties 
\[
\{\hat{g}_{\lambda,j}\}_{j=1}^{J}=\arg\min_{\sum g_{j}\in\mathcal{G}}\|y-\sum_{j=1}^{J}g_{j}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j})
\]


Given a validation set $V$ , let the CV-fitted model be 
\[
\{\hat{g}_{\hat{\lambda},j}\}_{j=1}^{J}=\arg\min_{\lambda}\|y-\sum_{j=1}^{J}\hat{g}_{\lambda,j}\|_{V}^{2}
\]


\textbf{Reasonable assumption:}
\begin{itemize}
\item The entropy bound (2) in result 2 comes from the assumptions in Lemma
3. The $\alpha$ below is $\alpha=\max_{j=1:J}\{\alpha_{j}\}$, so
convergence is only as fast as fitting the highest-entropy function
class. The constant $A$ must be appropriately inflated such that
the entropy bound holds for all $\delta\in(0,R]$.
\end{itemize}
\textbf{``Special'' assumptions:}
\begin{itemize}
\item We assume exponents $v_{j}=1$, whereas Vandegeer Thrm 10.2 only assumes
$v>2\alpha/(2+\alpha)$. Without this assumption, I wasn't able to
form inequalities between $\sum_{j=1}^{J}I_{j}(g_{j})\le something+\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j})$.
Indeed, Remark 1 in ``High-dimensional Additive Modeling'' (Vandegeer
2009) notes the importance of using the semi-norm instead of the square
of the semi-norm.
\item We suppose the following incoherence condition, in the spirit of Vandegeer
2014 ``The additive model with different smoothness for the components'':
Let $p_{V}(\vec{x})$ be the empirical density over the validation
set. Let $p_{Vj}$ be the marginal density of $x_{j}$ for the empirical
distribution of the validation set. Let 
\[
r_{V}(\vec{x})=\frac{p_{V}(\vec{x})}{\Pi_{j=1}^{J}p_{Vj}(x_{j})},\mbox{ }\gamma_{V}^{2}=\int r_{V}(\vec{x})\Pi_{j=1}^{J}p_{Vj}(x_{j})d\mu
\]
Suppose that $\gamma_{V}<1/(J-1)$. Furthermore, we will suppose that
$\int g_{j}p_{Vj}d\mu=0$ for $j=2,...,J$.
\end{itemize}

\subsection*{Result 2: Additive Model with multiple penalties, Single oracle $\lambda$
over $X_{T}$}

Suppose there is some $0<\alpha<2$ s.t. for all $\delta\in(0,R]$,
\begin{equation}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le A\delta^{-\alpha}
\end{equation}


If $\lambda$ is chosen s.t. 
\[
\tilde{\lambda}_{T}^{-1}=O_{p}\left(n^{1/(2+\alpha)}\right)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{(2-\alpha)/2(2+\alpha)}
\]


then
\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}=O_{p}\left(\tilde{\lambda}_{T}\right)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{1/2}
\]


and 
\[
\sum_{j=1}^{J}I_{j}(\hat{g}_{j})=O_{p}(1)\sum_{j=1}^{J}I_{j}(g_{j}^{*})
\]



\subsubsection*{Proof:}

The basic inequality gives us:

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le2\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|+\lambda^{2}\sum_{j=1}^{J}I_{j}(g_{j}^{*})
\]


\textbf{Case 1: $\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\le\lambda^{2}\sum_{j=1}^{J}I_{j}(g_{j}^{*})$}

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}(\lambda)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{1/2}
\]


\textbf{Case 2: $\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\ge\lambda^{2}\sum_{j=1}^{J}I_{j}(g_{j}^{*})$}

By Vandegeer (10.6), the basic inequality becomes 
\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le O_{p}\left(n^{-1/2}\right)\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha/2}\left(\sum_{j=1}^{J}I_{j}(\hat{g}_{j})+I_{j}(g_{j}^{*})\right)^{\alpha/2}
\]


\textbf{Case 2a:} $\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le\sum_{j=1}^{J}I_{j}(g_{j}^{*})$ 

Then 
\begin{eqnarray*}
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T} & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha/(2+\alpha)}
\end{eqnarray*}


\textbf{Case 2b:} $\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\ge\sum_{j=1}^{J}I_{j}(g_{j}^{*})$ 

Then

\begin{eqnarray*}
\sum_{j=1}^{J}I_{j}(\hat{g}_{j}) & \le & O_{p}\left(n^{-1/(2-\alpha)}\right)\lambda^{-4/(2-\alpha)}\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}
\end{eqnarray*}


Hence

\begin{eqnarray*}
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T} & \le & O_{p}\left(n^{-1/(2-\alpha)}\right)\lambda^{-2\alpha/(2-\alpha)}
\end{eqnarray*}



\subsection*{Result 3: Additive Model with multiple penalties, Single cross-validation
$\lambda$ over general $X_{T},X_{V}$}

Suppose that the training and validation set are independently sampled,
so the values $X_{i}$ are not necessarily the same. Suppose the training
and validation sets are both of size $n$. Suppose $X$ is bounded
s.t. $|X|\le R_{X}$ and the domain of $g\in\mathcal{G}$ is over
$(-R_{X},R_{X})$.

Suppose the same entropy bound (2) for both the training set $P_{T}$
and validation set $P_{V}$.

In addition to the assumptions in Result 4, suppose the infinity norm
is also bounded 
\begin{eqnarray*}
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|_{\infty}}{\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})} & \le & K<\infty
\end{eqnarray*}


Suppose there exist constants $M$,$M_{0}$ s.t. for all $j$ and
all $\lambda\in\Lambda$ 

\[
I_{j}\left(\hat{g}_{\lambda,j}\right)\le M\|\hat{g}_{\lambda,j}\|_{V}^{2}+M_{0}
\]


\textbf{Special assumption:} Suppose the incoherence condition $\gamma_{V}<1/(J-1)$.
We will also suppose $\int g_{j}p_{Vj}d\mu=0$ for $j=2,...,J$.

Let $\tilde{\lambda}$ be the optimal $\lambda$ as specified in Result
2. Then

\[
\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-\hat{g}_{\tilde{\lambda},j}\|_{V}=O_{p}\left(n^{-1/(2+\alpha)}\right)\left(1-\gamma(J-1)\right)^{\alpha/(2+\alpha)}\left(\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha/(2+\alpha)}+\sum_{j=1}^{J}I_{j}(g_{j}^{*})+\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}^{\alpha/2(2+\alpha)}\right)
\]


and $\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}$
is on the same order (differs by a constant).


\subsubsection*{Proof:}

By the triangle inequality, 
\[
\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}\le\left\Vert \sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}+\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}
\]


By Lemma 2 and Result 2, we can easily bound$\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}$.

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V} & \le & \left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{T}+\left|\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{T}-\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}\right|\\
 & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)\left(\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha/(2+\alpha)}+\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)
\end{eqnarray*}


Next bound $\left\Vert \sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}$.
By definition of $\hat{\lambda}$, we have the basic inequality

\[
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{2}\le2\left|\left(\epsilon,\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|+2\left|\left(\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j},\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|
\]


\textbf{Case 1:} $\left|\left(\epsilon,\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|$
is bigger

By Vandegeer (10.6),
\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(\sum_{j=1}^{J}I_{j}(\hat{g}_{\tilde{\lambda},j})+I_{j}(\hat{g}_{\hat{\lambda},j})\right)^{\alpha/2}
\end{eqnarray*}


If $\sum_{j=1}^{J}I_{j}(\hat{g}_{\tilde{\lambda},j})\ge\sum_{j=1}^{J}I_{j}(\hat{g}_{\hat{\lambda},j})$,
we're done.

Otherwise, suppose $\sum_{j=1}^{J}I_{j}(\hat{g}_{\tilde{\lambda},j})<\sum_{j=1}^{J}I_{j}(\hat{g}_{\hat{\lambda},j})$. 

By the incoherence assumption, we can apply Lemma 4

\begin{eqnarray*}
\sum_{j=1}^{J}I_{j}(\hat{g}_{\hat{\lambda},j}) & \le & M\sum_{j=1}^{J}\|\hat{g}_{\lambda j}\|_{V}^{2}+M_{0}J\\
 & \le & M\left(1-\gamma(J-1)\right)\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}\|_{V}^{2}+M_{0}J
\end{eqnarray*}


Then
\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(M\left(1-\gamma(J-1)\right)\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}\|_{V}^{2}+M_{0}J\right)^{\alpha/2}
\end{eqnarray*}


If $M_{0}J$ is the biggest, we're done. Otherwise,

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(1-\gamma(J-1)\right)^{\alpha/2}\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}\|_{V}^{\alpha}\\
 & \le & O_{p}(n^{-1/2})\left(1-\gamma(J-1)\right)^{\alpha/2}\left(\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}+\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-g_{j}^{*}\right\Vert _{V}+\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}\right)^{\alpha}
\end{eqnarray*}


If $\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}$or
$\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-g_{j}^{*}\right\Vert _{V}$
is the biggest on the RHS, then the rate is faster than $O_{p}(n^{-1/(2+\alpha)})$.\textbf{
}If\textbf{$\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}$ }is
the biggest, then

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V} & \le & O_{p}(n^{-1/(2+\alpha)})\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}^{\alpha/2(2+\alpha)}
\end{eqnarray*}


\textbf{Case 2:} $\left|\left(\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j},\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|$
is bigger

By Cauchy Schwarz, 

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V} & \le & O_{p}(1)\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}
\end{eqnarray*}
\pagebreak{}


\section{General Additive Model: Multiple Lambdas}


\subsection*{Definitions}

We find the best model for $y$ over function classes $\mathcal{G}=\left\{ \sum_{j=1}^{J}g_{j}:\mbox{ }g_{j}\in\mathcal{G}_{j}\right\} $.
Suppose we observe:

\[
y=\sum_{j=1}^{J}g_{j}^{*}+\epsilon
\]
 where $\sum_{j=1}^{J}g_{j}^{*}\in\mathcal{G}$. Suppose $\epsilon$
are sub-Gaussian errors for constants $K$ and $\sigma_{0}^{2}$:
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Given a training set $T$ , we fit models by least squares with multiple
penalties and tuning parameters 
\[
\{\hat{g}_{\lambda,j}\}_{j=1}^{J}=\arg\min_{\sum g_{j}\in\mathcal{G}}\|y-\sum_{j=1}^{J}g_{j}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{2}I_{j}^{v_{j}}(g_{j})
\]


Suppose $1\le v_{j}\le2$.

Given a validation set $V$ , let the CV-fitted model be 
\[
\{\hat{g}_{\hat{\lambda},j}\}_{j=1}^{J}=\arg\min_{\lambda}\|y-\sum_{j=1}^{J}\hat{g}_{\lambda,j}\|_{V}^{2}
\]



\subsection*{Result 4: Additive Model, Oracle $\{\lambda_{i}\}$ given $X_{T}$}

These results are implied by Vandegeer's paper ``The additive model
with different smoothness for the components.'' 

Suppose for all $j=1:J$
\[
\mathcal{H}\left(\delta,\left\{ \frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}\right\} ,\|\cdot\|_{n}\right)\le A_{j}\delta^{-\alpha_{j}}\forall\delta>0
\]


Let
\[
\lambda_{j}=O_{p}(n^{-1/(2+\alpha_{j})})
\]


and 

\[
\left(\sum_{j=1}^{J}I_{j}^{q_{j}}(g_{j}^{*})\right)^{1/2}\lambda_{\max}=O_{P}(1)R
\]


There are some constants $c_{1},c_{2}$ s.t. for $\lambda_{j}=O_{p}(n^{-1/(2+\alpha_{j})})$,
we have 
\[
\|\sum g_{j}^{*}-\sum\hat{g}_{\tilde{\lambda},j}\|\le c_{2}\lambda_{(j)}
\]


where $(j)=\arg\max\alpha_{j}$. That is, the convergence rate depends
on the highest-entropy function class (with respect to the penalty)

\[
\|\sum_{j=1}^{J}g_{j}^{*}-\sum_{j=1}^{J}\hat{g_{j}}\|_{T}=O_{p}(n^{-1/(2+\alpha_{(j)})})
\]


\textbf{Jean's version of the Proof for Vandegeer Thrm 3.1:}

Suppose for some constant $R$ , we define the function class $\mathcal{M}(R)=\left\{ \{g_{j}\}:(\lambda_{j}/R)^{(1-q_{j})/q_{j}}\lambda_{j}I_{j}(g_{j}-g_{j}^{*})\le R,\mbox{ }\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|_{T}\le R\right\} $. 

Recall that 
\[
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\left|(\epsilon^{T},g_{j}-g_{j}^{*})\right|}{\left(I_{j}(g_{j})+I_{j}(g_{j}^{*})\right)^{\alpha_{j}/2}\|g_{j}-g_{j}^{*}\|^{1-\alpha_{j}/2}}=O_{p}(n^{-1/2})
\]


By our choice of $\lambda$, we have that for function sets $\{g_{j}-g_{j}^{*}\}\in\mathcal{M}(R)$,
the empirical process term decreases with $n$: 
\begin{eqnarray*}
\left|(\epsilon^{T},g_{j}-g_{j}^{*})\right| & \le & O_{P}(n^{-1/2})\left(I_{j}(g_{j})+I_{j}(g_{j}^{*})\right)^{\alpha_{j}/2}\|g_{j}-g_{j}^{*}\|^{1-\alpha_{j}/2}\\
 & \le & O_{P}(n^{-1/2})\left(\lambda_{j}^{-1/q_{j}}R^{1/q_{j}}\right)^{\alpha_{j}/2}R^{1-\alpha_{j}/2}\\
 & \le & O_{P}(n^{-1/(2+\alpha_{j})})R^{2}
\end{eqnarray*}


Hence for sufficiently large $n$, Vandegeer Lemma's 5.4 (Jean's version
below) states that the fitted functions $\hat{g_{j}}$ are also with
$R$ of the truth: 
\[
\{\hat{g_{j}}-g_{j}^{*}\}\in\mathcal{M}(R)\implies\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g_{j}}\|_{T}\le R
\]


Now we just need to determine the right value for $R$. Choose $n$
sufficiently large s.t. the penalty term for function $(j)$ is the
highest (for the truth) 
\[
\lambda_{j}^{2}I_{j}^{q_{j}}(g_{j}^{*})\le\lambda_{(j)}^{2}I_{(j)}^{q_{(j)}}(g_{(j)}^{*})\mbox{ }\forall j
\]


Then choose $R$ s.t. 
\[
\left(\lambda_{(j)}^{2}I_{(j)}^{q_{(j)}}(g_{(j)}^{*})\right)^{1/2}J^{1/2}=O_{P}(1)R
\]


Hence

\[
\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g_{j}}\|_{T}\le n^{-1/(2+\alpha_{(j)})}J^{1/2}I_{(j)}^{q_{(j)}/2}(g_{(j)}^{*})
\]



\subsection*{Result 5: Additive Model, Cross-validated $\{\lambda_{i}\}$ over
general $X_{T},X_{V}$}

Assume the same conditions as result 4, but also for the validation
set.

\textbf{Condition 2.4:} Incoherence condition on the validation set.
Let $p_{V}(\vec{x})$ be the empirical density over the validation
set. Let $p_{Vj}$ be the marginal density of $x_{j}$ for the empirical
distribution of the validation set. Let 
\[
r_{V}(\vec{x})=\frac{p_{V}(\vec{x})}{\Pi_{j=1}^{J}p_{Vj}(x_{j})},\mbox{ }\gamma_{V}^{2}=\int r_{V}(\vec{x})\Pi_{j=1}^{J}p_{Vj}(x_{j})d\mu
\]
Suppose that $\gamma_{V}<1/(J-1)$. Furthermore, we will suppose that
$\int g_{j}p_{Vj}d\mu=0$ for $j=2,...,J$.

Additionally, suppose there exist constants $M$,$M_{0}$ s.t. for
all $j$ and all $\lambda\in\Lambda$ 

\[
I_{j}\left(\hat{g}_{\lambda,j}\right)\le M\|\hat{g}_{\lambda,j}\|_{V}^{2}+M_{0}
\]


Let $\tilde{\lambda}$ be the optimal $\{\lambda_{i}\}$ as specified
in Result 4. Then

\[
\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-\hat{g}_{\tilde{\lambda},j}\|_{V}=O_{p}\left(n^{-1/(2+\alpha_{(j)})}\right)\left(1-\gamma(J-1)\right)^{\alpha_{(j)}/(2+\alpha_{(j)})}\left(\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha_{(j)}/(2+\alpha_{(j)})}+\sum_{j=1}^{J}I_{j}(g_{j}^{*})+\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}^{\alpha_{(j)}/2(2+\alpha_{(j)})}\right)
\]


and $\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}=O_{p}(1)\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{T}$
.


\subsubsection*{Proof: }

Exactly the same as Result 3

\pagebreak{}


\subsection*{Lemmas}


\subsubsection*{Lemma 1: }

Suppose for all $\lambda\in\Lambda$, the penalty function $I^{v}(g_{\lambda})$
is upper-bounded by $\|g_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}g_{\lambda}^{2}(x_{i})$
with constants $M_{0}$ and $M$:

\[
I^{v}(g_{\lambda})\le M\|g_{\lambda}\|_{n}^{2}+M_{0}
\]


Suppose there is some function $g\in\mathcal{G}$ such that
\[
\|g-g_{\lambda}\|_{n}^{1+\alpha/2}\le O_{p}(n^{-1/2})I^{\alpha/2}(g_{\lambda})
\]


Then

\[
\|g-g_{\lambda}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha v/(2+\alpha)}\|g\|_{n}^{2\alpha/v(2+\alpha)}
\]


\textbf{Proof:}

From the assumptions, we have

\begin{eqnarray*}
\|g-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(M\|g_{\lambda}\|_{n}^{2}+M_{0}\right)^{\alpha/2v}
\end{eqnarray*}


If $M_{0}>\|g_{\lambda}\|_{n}^{2}$, we're done. Otherwise,

\begin{eqnarray*}
\|g-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\|g_{\lambda}\|_{n}^{\alpha/v}\\
 & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\left(\|g_{\lambda}-g\|_{n}+\|g\|_{n}\right)^{\alpha/v}
\end{eqnarray*}


\textbf{Case 1:} $\|g_{\lambda}-g\|_{n}\ge\|g\|_{n}$

Then

\[
\|g-g_{\lambda}\|_{n}\le O_{p}(n^{-v/(2v+\alpha v-2\alpha)})M^{\alpha v^{2}/(2v+\alpha v-2\alpha)}
\]


Note that $\sup_{v}-\frac{v}{2v+\alpha v-2\alpha}=-\frac{1}{2+\alpha}$,
so this rate is faster than $O_{p}(n^{-\frac{1}{2+\alpha}})$.

\textbf{Case 2:} $\|g_{\lambda}-g\|_{n}\le\|g\|_{n}$

Then

\[
\|g-g_{\lambda}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha v/(2+\alpha)}\|g\|_{n}^{2\alpha/v(2+\alpha)}
\]


I believe we can often provide a good estimate of $M$ for the entire
class $\mathcal{G}$, which means that we can always estimate the
sample size needed to ensure this case never occurs. That is, I believe
we can often estimate $M$ s.t. 
\[
I^{v}(g)\le M\|g\|_{n}^{2}+M_{0}\forall g\in\mathcal{G}
\]



\subsubsection*{Lemma 2:}

Let $P_{n'}$ and $P_{n''}$ be empirical distributions over $\{X_{i}'\}_{i=1}^{n},\{X_{i}''\}_{i=1}^{n}$.
Let $P_{2n}=\frac{1}{2}\left(P_{n'}+P_{n''}\right)$. Suppose $X$
is bounded s.t. $|X|<R_{X}$.

Let $\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} $.
Suppose $g$ is defined over the domain over $X$ (and zero otherwise).
Suppose 
\[
\sup_{f\in\mathcal{G}'}\|f\|_{P_{2n}}\le R<\infty,\mbox{ }\sup_{f\in\mathcal{G}'}\|f\|_{\infty}\le K<\infty
\]


and 
\[
H\left(\delta,\mathcal{G}',P_{n'}\right)\le\tilde{A}\delta^{-\alpha},\mbox{ }H\left(\delta,\mathcal{G}',P_{n''}\right)\le\tilde{A}\delta^{-\alpha}
\]


Then 

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n'}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]



\paragraph{Proof:}

The proof is very similar to that in Pollard 1984 (page 32), so some
details below are omitted.

First note that for any function $f$ and $h$, we have 
\[
\|f\|_{P_{n'}}-\|h\|_{P_{n'}}\le\|f-h\|_{P_{n'}}\le\sqrt{2}\|f-h\|_{P_{2n}}
\]


Similarly for $P_{n''}$. 

Let $\{h_{j}\}_{j=1}^{N}$ be the $\sqrt{2}\delta$-cover for $\mathcal{G}'$
(where $N=N(\sqrt{2}\delta,\mathcal{G}',P_{2n})$). Let $h_{j}$ be
the closest function (in terms of $\|\cdot\|_{P_{2n}}$) to some $f\in\mathcal{G}'$.
Then 
\begin{eqnarray*}
\|f\|_{P_{n'}}-\|f\|_{P_{n''}} & \le & \|f-h_{j}\|_{P_{n'}}+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|+\|f-h_{j}\|_{P_{n''}}\\
 & \le & 4\delta+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|
\end{eqnarray*}


Therefore for $f=\frac{g^{*}-g}{I(g^{*})+I(g)}$, we have
\begin{eqnarray*}
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right) & \le & Pr\left(\sup_{j\in1:N}\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\\
 & \le & N\max_{j\in1:N}Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)
\end{eqnarray*}


Now note that 
\begin{eqnarray*}
\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right| & = & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\|h_{j}\|_{P_{n'}}+\|h_{j}\|_{P_{n''}}}\\
 & \le & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\sqrt{2}\|h_{j}\|_{P_{2n}}}
\end{eqnarray*}


By Hoeffding's inequality, 
\begin{eqnarray*}
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right) & \le & Pr\left(\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|\ge2\sqrt{2}\delta\|h_{j}\|_{P_{2n}}\right)\\
 & = & Pr\left(\left|\sum_{i=1}^{n}W_{i}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)\right|\ge2\sqrt{2}n\delta\|h_{j}\|_{P_{2n}}\right)\\
 & \le & 2\exp\left(-\frac{16\delta^{2}n^{2}\|h_{j}\|_{P_{2n}}^{2}}{4\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2}}\right)
\end{eqnarray*}


Since $\|h_{j}\|_{\infty}<K$, then

\begin{eqnarray*}
\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2} & \le & \sum_{i=1}^{n}h_{j}^{4}(x_{i}')+h_{j}^{4}(x_{i}'')\\
 & \le & nK^{2}\|h_{j}\|_{P_{2n}}^{2}
\end{eqnarray*}


Hence 
\[
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\le2\exp\left(-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Since (Pollard and Vandegeer say that) 
\[
N(\sqrt{2}\delta,\mathcal{G}',P_{2n})\le N(\delta,\mathcal{G}',P_{n''})+N(\delta,\mathcal{G}',P_{n''})
\]


then

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Using shorthand, we can write 
\[
\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}=O_{p}(n^{-1/(2+\alpha)})
\]



\subsubsection*{Lemma 3:}

Suppose the function classes $\mathcal{F}_{j}$ is a cone and $I_{j}:\mathcal{F}_{j}\mapsto[0,\infty)$
is a psuedonorm. Furthermore, suppose 
\[
H\left(\delta,\{f_{j}\in\mathcal{F}_{j}:I_{j}(f_{j})\le1\},\|\cdot\|_{n}\right)\le A_{j}\delta^{-\alpha_{j}}
\]
Then if $f_{j}^{*}\in\mathcal{F}_{j}$, then

\begin{eqnarray*}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}:f_{j}\in\mathcal{F}_{j},I_{j}(f_{j})+I_{j}(f_{j}^{*})>0\right\} ,\|\cdot\|_{n}\right) & \le & 2\sum_{j=1}^{J}A_{j}\left(\frac{\delta}{2J}\right)^{-\alpha_{j}}
\end{eqnarray*}



\paragraph*{Proof:}

Let $\tilde{f}_{j}=\frac{f_{j}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}$.
Then $\tilde{f}_{j}\in\mathcal{F}_{j}$ and $I_{j}(\tilde{f}_{j})\le1$.
Let $h_{(j)}$ be the closest function to $\tilde{f}_{j}$ in the
$\delta$ cover of $\mathcal{F}_{j}$. Similarly, let $h_{(j)}^{*}$
be the closest function to $\tilde{f}_{j}^{*}$ in the $\delta$ cover
of $\mathcal{F}_{j}$. Then 
\begin{eqnarray*}
\left\Vert \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-\left(\sum_{j=1}^{J}h_{(j)}-h_{(j)}^{*}\right)\right\Vert  & \le & \sum_{j=1}^{J}\left\Vert \frac{f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-\left(h_{(j)}-h_{(j)}^{*}\right)\right\Vert \\
 & \le & \sum_{j=1}^{J}\left\Vert \frac{f_{j}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-h_{(j)}\right\Vert +\left\Vert \frac{f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-h_{(j)}^{*}\right\Vert \\
 & \le & 2J\delta
\end{eqnarray*}


Hence

\[
H\left(2J\delta,\left\{ \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}:f_{j}\in\mathcal{F}_{j},I_{j}(f_{j})+I_{j}(f_{j}^{*})>0\right\} ,\|\cdot\|_{n}\right)\le2\sum_{j=1}^{J}A_{j}\delta^{-\alpha_{j}}
\]



\subsubsection*{Lemma 4:}

Let $p_{n}(\vec{x})$ be some empirical density and let $p_{nj}$
be the corresponding empirical marginal density of $x_{j}$. Let 
\[
r(\vec{x})=\frac{p_{n}(\vec{x})}{\Pi_{j=1}^{J}p_{nj}(x_{j})},\mbox{ }\gamma^{2}=\int(r(\vec{x})-1)^{2}\Pi_{j=1}^{J}p_{nj}(x_{j})d\mu
\]
Suppose $\gamma<1/(J-1)$. Furthermore, suppose $\int g_{j}p_{nj}d\mu=0$
for $j=2,...,J$. Then 
\[
\left\Vert \sum_{j=1}^{J}g_{j}\right\Vert _{n}^{2}\ge(1-\gamma(J-1))\left(\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}\right)
\]



\paragraph*{Proof:}

The proof is very similar to Lemma 5.1 in Vandegeer 2014 ``The additive
model with different smoothness for the components.''

\[
\left\Vert \sum_{j=1}^{J}g_{j}\right\Vert _{n}^{2}=\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}+\sum_{j\ne k}\int g_{j}g_{k}p_{n}(\vec{x})d\mu
\]


We bound the latter term:

\begin{eqnarray*}
\left|\int g_{j}g_{k}p_{n}(\vec{x})d\mu\right| & = & \left|\int g_{j}g_{k}\left(r(\vec{x})-1\right)\Pi_{j=1}^{J}p_{nj}(x_{j})d\mu\right|\\
 & \le & \gamma\left|\int g_{j}^{2}g_{k}^{2}\Pi_{j=1}^{J}p_{nj}(x_{j})d\mu\right|^{1/2}\\
 & = & \gamma\|g_{j}\|_{n}\|g_{k}\|_{n}
\end{eqnarray*}


Hence

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}g_{j}\right\Vert _{n}^{2} & \ge & \sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}-\gamma\sum_{j\ne k}\|g_{j}\|_{n}\|g_{k}\|_{n}\\
 & \ge & (1-\gamma(J-1))\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}+\gamma\sum_{j<k}\left(\|g_{j}\|_{n}-\|g_{k}\|_{n}\right)^{2}\\
 & \ge & (1-\gamma(J-1))\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}
\end{eqnarray*}



\subsubsection*{Vandegeer's Lemma 5.4 (Jean's version)}

Let 
\[
\tau_{R}(\{f_{j}\})=\|\sum f_{j}\|_{T}+\sum_{j=1}^{J}(\lambda_{j}/R)^{(1-q_{j})/q_{j}}\lambda_{j}I_{j}(f_{j})
\]


Suppose 
\[
\sum_{j=1}^{J}\lambda_{j}^{2}I_{j}^{q_{j}}(f_{j}^{*})\le\delta_{0}^{2}R^{2}
\]


and for all function sets $\{f_{j}\}$ s.t. $\tau_{R}(\{f_{j}\})\le R$,
suppose 
\[
\sup_{f_{j}}\left|\left(\epsilon_{T},f_{j}\right)\right|\le\delta_{0}^{2}R^{2}
\]


Let 
\[
\hat{f}_{j}=\arg\min\|y-\sum_{j=1}^{J}f_{j}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{2}I_{j}^{q_{j}}(f_{j})
\]


Then $\tau_{R}\left(\left\{ \hat{f}_{\lambda,j}-f_{j}^{*}\right\} \right)\le R$.


\paragraph*{Proof}

We use the convexity of the penalties and the least squares function.
Consider $\tilde{f_{j}}=t\hat{f}_{j}+(1-t)f_{j}^{*}$ where 
\begin{eqnarray*}
t & = & \frac{R}{R+\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\})}
\end{eqnarray*}


Then 
\begin{eqnarray*}
\tau_{R}(\{\tilde{f_{j}}-f_{j}^{*}\}) & = & \frac{R}{R+\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\})}\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\})\le R
\end{eqnarray*}


Also, by the basic inequality, we can see that

\begin{eqnarray*}
\|\sum_{j=1}^{J}f_{j}^{*}-\sum_{j=1}^{J}\tilde{f_{j}}\|_{T}^{2}+\sum_{j=1}^{J}\lambda_{j}^{2}I_{j}^{q_{j}}(\tilde{f_{j}}) & \le & \sum_{j=1}^{J}\left|\left(\epsilon_{T},f_{j}^{*}-\tilde{f_{j}}\right)\right|+\sum_{j=1}^{J}\lambda_{j}^{2}I_{j}^{q_{j}}(f_{j}^{*})
\end{eqnarray*}


With gross algebra, we show that

\[
(\lambda_{j}/R)^{(1-q_{j})/q_{j}}\lambda_{j}I_{j}(\tilde{f_{j}}-f_{j}^{*})\le4\delta_{0}R
\]


Since

\[
\tau_{R}(\{\tilde{f_{j}}-f_{j}^{*}\})=\frac{R}{R+\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\})}\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\})\le O_{P}(1)J\delta_{0}R
\]


Then for small enough $\delta_{0}$, we have that
\[
\tau_{R}(\{\hat{f}_{j}-f_{j}^{*}\})\le R
\]


\pagebreak{}


\section{Examples}

Our goal here is to show that the assumptions hold for various examples.


\subsection{Sobolev Norm}

Suppose $\mathcal{G}$ is the class of smooth functions $g:[0,1]\mapsto\mathbb{R}$
s.t. $I_{(k)}^{2}(g)=\int_{0}^{1}g^{(k)}(t)^{2}dt<\infty$. 

\[
\arg\min_{g\in\mathcal{G},f\in\mathcal{F}}\|y-g(x_{1})+f(x_{2})\|_{T}^{2}+\lambda_{g}^{2}I_{(k)}(g)+\lambda_{f}^{2}I_{(k)}(f)
\]


Note that it can be shown that $g$ can always be expressed using
natural $2k$-order B-splines with knots at the training points $x_{1T}$.
(De Boor, Thrm XIII.5) So we can express $g(t)=\sum_{i=1}^{n}\beta_{i}(t)\gamma_{i}=B\gamma$
where $B$ is positive definite. Similarly, $I_{(k)}^{2}(g)=\gamma^{T}\Omega\gamma$
where $\Omega_{ij}=\int_{0}^{1}\beta_{i}^{(k)}(u)\beta_{j}^{(k)}(u)du$.

\textbf{Assumption 1:} Show for some constant $K$, 
\[
\frac{\|g\|_{\infty}}{I_{(k)}(g)}\le K
\]


\textbf{Proof:}

From De Boor (p.110), B-splines have the property that $\left|\beta_{i}(t)\right|\le1$
and $\beta_{i}(t)\ge0$. Hence $\|g\|_{\infty}\le\max_{i}|\gamma_{i}|$.
Then
\[
\frac{\|g\|_{\infty}}{I_{(k)}(g)}\le\frac{\|\gamma\|_{\infty}}{\|\Omega^{1/2}\gamma\|}
\]


Assuming that the smallest nonzero eigenvalue of $\Omega^{1/2}$ is
at least greater than some constant $c>0$ (in fact, the nonzero eigenvalues
of $\Omega^{1/2}$ likely grow with $n$), then

\[
\frac{\|g\|_{\infty}}{I_{(k)}(g)}\le\frac{\|\gamma\|_{\infty}}{c\|\gamma\|_{2}}\le\frac{1}{c}
\]


\textbf{Assumption 2: }Show that there are constants $M,M_{0}$ s.t.
the penalty is bounded by the squared L2 norm: 
\[
I_{(k)}^{2}(g_{\hat{\lambda}})=MI_{(k)}^{2}(g_{\tilde{\lambda}})\|g_{\hat{\lambda}}\|^{2}+M_{0}
\]


\textbf{Proof:}

Let's suppose that $I_{(k)}^{2}(g_{\tilde{\lambda}})\ne0$. Hence
$\gamma_{\lambda}\notin\mathcal{N}(\Omega)$. Let $c$ be the smallest
nonzero eigenvalue and $C$ be the largest eigenvalue of $\Omega$
(largest and smallest by absolute value). 

Then 
\[
\frac{I_{(k)}^{2}(g_{\lambda})}{I_{(k)}^{2}(g_{\tilde{\lambda}})}=\frac{\gamma_{\lambda}^{T}\Omega\gamma_{\lambda}}{\gamma_{\tilde{\lambda}}\Omega\gamma_{\tilde{\lambda}}}\le\frac{C\|\gamma_{\lambda}\|^{2}}{c\|\gamma_{\tilde{\lambda}}\|^{2}}
\]


Let's suppose the as $n$ increases, suppose $C/c=O_{p}(1)$. For
example, in the case where the points are uniformly spaced $1/n$,
$\int_{0}^{1}\beta_{i}^{(k)}(u)\beta_{j}^{(k)}(u)du$ grows at a rate
of $O_{p}(n^{2k-1})$, but this is fine since the condition number
of $\Omega$ remains constant. In a sense, we are assuming that $\Omega$
stays ``well-conditioned'' as $n$ grows, though the formal definition
of a condition number is slightly different. 

Since $\|\gamma_{\lambda}\|=\|B^{-1}g_{\lambda}\|$ and $\|g^{*}-g_{\tilde{\lambda}}\|=O_{p}(n^{-1/(2+\alpha)})$,
then there are constants $M,M_{0}$ s.t. 
\[
I_{(k)}^{2}(g_{\lambda})\le MI_{(k)}^{2}(g_{\tilde{\lambda}})\|g_{\lambda}\|^{2}+M_{0}
\]


(and obviously we also have $I_{(k)}(g_{\lambda})$ is bounded by
$\|g_{\lambda}\|^{2}$ modulo some other set of constants)
\end{document}
