%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\section{Simple model}


\subsection*{Definitions}

We find the best model for $y$ over function class $\mathcal{G}$.
Presume $g^{*}\in\mathcal{G}$ is the true model and 
\[
y=g^{*}(X)+\epsilon
\]


where $\epsilon$ are sub-Gaussian errors for constants $K$ and $\sigma_{0}^{2}$
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Given a training set $T$ , We define the fitted models 
\[
\hat{g}_{\lambda}=\|y-g\|_{T}^{2}+\lambda^{2}I^{v}(g)
\]


Given a validation set $V$ , let the CV-fitted model be 
\[
\hat{g}_{\hat{\lambda}}=\arg\min_{\lambda}\|y-\hat{g}_{\lambda}\|_{V}^{2}
\]


We will suppose $I(g^{*})>0$.


\subsection*{Assumptions}

Suppose the entropy of the class $\mathcal{G}'$ is 
\begin{eqnarray}
H\left(\delta,\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} ,P_{T}\right) & \le & \tilde{A}\delta^{-\alpha}
\end{eqnarray}


Suppose $v>2\alpha/(2+\alpha)$. 

Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by $\|\hat{g}_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}\hat{g}_{\lambda}(x_{i})$.
See Lemma 1 below for the specific assumption. This assumption includes
Ridge, Lasso, Generalized Lasso, and the Group Lasso. 


\subsection*{Result 1: Single $\lambda$, Single Penalty, cross-validation over
general $X_{T},X_{V}$}

Suppose that the training and validation set are independently sampled,
so the values $X_{i}$ are not necessarily the same. Suppose the training
and validation sets are both of size $n$. Suppose $X$ is bounded
s.t. $|X|\le R_{X}$ and the domain of $g\in\mathcal{G}$ is over
$(-R_{X},R_{X})$.

Suppose the same entropy bound (2) for both the training set $P_{T}$
and validation set $P_{V}$.

Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by its $L_{2}$-norm with some constant $M$ and
$M_{0}$ such that

\[
I^{v}(\hat{g}_{\lambda})\le M\|\hat{g}_{\lambda}\|_{n}^{2}+M_{0}
\]


Suppose the entropy bound for both training set $P_{T}$ and validation
set $P_{V}$.

Suppose that 
\begin{eqnarray*}
\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{\infty}}{I(g)+I(g^{*})} & \le & K<\infty
\end{eqnarray*}


Let $\tilde{\lambda}$ be the optimal $\lambda$ by Vandegeer. Then

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}=O_{p}\left(n^{-1/(2+\alpha)}\right)\left(I^{\alpha/(2+\alpha)}(g^{*})+I(g^{*})\right)
\]


and $\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{V}$ is of the same order
(differs by some constant).

\textbf{Proof:}

By the triangle inequality, 
\[
\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{V}\le\|\hat{g}_{\hat{\lambda}}-\hat{g}_{\tilde{\lambda}}\|_{V}+\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V}
\]


We bound each component on the RHS separately.

First bound $\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V}$. By Vandegeer
Thrm 10.2 and Lemma 2,

\begin{eqnarray*}
\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V} & \le & \|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{T}+\left|\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{V}-\|\hat{g}_{\tilde{\lambda}}-g^{*}\|_{T}\right|\\
 & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)I^{\alpha/(2+\alpha)}(g^{*})+O_{p}\left(n^{-1/(2+\alpha)}\right)\left(I(g^{*})+I(\hat{g}_{\tilde{\lambda}})\right)\\
 & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)\left(I^{\alpha/(2+\alpha)}(g^{*})+I(g^{*})\right)
\end{eqnarray*}


Next bound $\|\hat{g}_{\hat{\lambda}}-\hat{g}_{\tilde{\lambda}}\|_{V}$.
The basic inequality gives us

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}^{2}\le2\left|\left(\epsilon,\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{V}\right|+2\left|\left(g^{*}-\hat{g}_{\tilde{\lambda}},\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{V}\right|
\]


\textbf{Case a:} $\left|\left(\epsilon,\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{T}\right|$
is the bigger term on the RHS

By Vandegeer (10.6),
\begin{eqnarray*}
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}^{2} & \le & O_{P}(n^{-1/2})\|\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\|^{1-\alpha/2}\left(I(\hat{g}_{\tilde{\lambda}})+I(\hat{g}_{\hat{\lambda}})\right)^{\alpha/2}
\end{eqnarray*}


If $I(\hat{g}_{\tilde{\lambda}})>I(\hat{g}_{\hat{\lambda}})$, then

\begin{eqnarray*}
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V} & \le & O_{P}(n^{-1/(2+\alpha)})I(g^{*})^{\alpha/(2+\alpha)}
\end{eqnarray*}


Otherwise, suppose $I(\hat{g}_{\tilde{\lambda}})<I(\hat{g}_{\hat{\lambda}})$.
Since $I$ is a pseudo-norm,

\begin{eqnarray*}
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V} & \le & O_{P}(n^{-1/(2+\alpha)})I(\hat{g}_{\hat{\lambda}})^{\alpha/(2+\alpha)}\\
 & \le & O_{P}(n^{-1/(2+\alpha)})\left(I(\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}})+I(\hat{g}_{\tilde{\lambda}})\right)^{\alpha/(2+\alpha)}
\end{eqnarray*}


If $I(\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}})\le I(\hat{g}_{\tilde{\lambda}})$,
then we're done. Otherwise if $I(\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}})\ge I(\hat{g}_{\tilde{\lambda}})$,
by the assumption that $I^{V}(\cdot)$ is bounded by the L2 norm,

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}\le O_{P}(n^{-1/(2+\alpha)})\left(M\|\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\|_{V}^{2}+M_{0}\right)^{\alpha/v(2+\alpha)}
\]


If $M_{0}$ is bigger, we're done. Otherwise,

\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}\le O_{P}(n^{-v/(2v-2\alpha+\alpha v)})<O_{P}(n^{-1/(2+\alpha)})
\]


\textbf{Case b:} $\left|\left(g^{*}-\hat{g}_{\tilde{\lambda}},\hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right)_{V}\right|$
is the bigger term on the RHS

By Cauchy Schwarz,
\[
\left\Vert \hat{g}_{\tilde{\lambda}}-\hat{g}_{\hat{\lambda}}\right\Vert _{V}\le O_{P}(1)\left\Vert g^{*}-\hat{g}_{\tilde{\lambda}}\right\Vert _{V}
\]


\pagebreak{}


\section{General Additive Model}


\subsection*{Definitions}

We find the best model for $y$ over function classes $\mathcal{G}=\left\{ \sum_{j=1}^{J}g_{j}:\mbox{ }g_{j}\in\mathcal{G}_{j}\right\} $.
Suppose we observe:

\[
y=\sum_{j=1}^{J}g_{j}^{*}+\epsilon
\]
 where $\sum_{j=1}^{J}g_{j}^{*}\in\mathcal{G}$. Suppose $\epsilon$
are sub-Gaussian errors for constants $K$ and $\sigma_{0}^{2}$:
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Given a training set $T$ , we fit models by least squares with multiple
penalties 
\[
\{\hat{g}_{\lambda,j}\}_{j=1}^{J}=\arg\min_{\sum g_{j}\in\mathcal{G}}\|y-\sum_{j=1}^{J}g_{j}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j})
\]


Given a validation set $V$ , let the CV-fitted model be 
\[
\{\hat{g}_{\hat{\lambda},j}\}_{j=1}^{J}=\arg\min_{\lambda}\|y-\sum_{j=1}^{J}\hat{g}_{\lambda,j}\|_{V}^{2}
\]


\textbf{Reasonable assumption:}
\begin{itemize}
\item The entropy bound (2) in result 2 comes from the assumptions in Lemma
3. The $\alpha$ below is $\alpha=\max_{j=1:J}\{\alpha_{j}\}$, so
convergence is only as fast as fitting the highest-entropy function
class. The constant $A$ must be appropriately inflated such that
the entropy bound holds for all $\delta\in(0,R]$.
\end{itemize}
\textbf{``Special'' assumptions:}
\begin{itemize}
\item We assume exponents $v_{j}=1$, whereas Vandegeer Thrm 10.2 only assumes
$v>2\alpha/(2+\alpha)$. Without this assumption, I wasn't able to
form inequalities between $\sum_{j=1}^{J}I_{j}(g_{j})\le something+\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j})$.
Indeed, Remark 1 in ``High-dimensional Additive Modeling'' (Vandegeer
2009) notes the importance of using the semi-norm instead of the square
of the semi-norm.
\item We suppose the following incoherence condition, in the spirit of Vandegeer
2014 ``The additive model with different smoothness for the components'':
Let $p_{V}(\vec{x})$ be the empirical density over the validation
set. Let $p_{Vj}$ be the marginal density of $x_{j}$ for the empirical
distribution of the validation set. Let 
\[
r_{V}(\vec{x})=\frac{p_{V}(\vec{x})}{\Pi_{j=1}^{J}p_{Vj}(x_{j})},\mbox{ }\gamma_{V}^{2}=\int r_{V}(\vec{x})\Pi_{j=1}^{J}p_{Vj}(x_{j})d\mu
\]
Suppose that $\gamma_{V}<1/(J-1)$. Furthermore, we will suppose that
$\int g_{j}p_{Vj}d\mu=0$ for $j=2,...,J$.
\end{itemize}

\subsection*{Result 2: Additive Model with multiple penalties, Single oracle $\lambda$
over $X_{T}$}

Suppose there is some $0<\alpha<2$ s.t. for all $\delta\in(0,R]$,
\begin{equation}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le A\delta^{-\alpha}
\end{equation}


If $\lambda$ is chosen s.t. 
\[
\tilde{\lambda}_{T}^{-1}=O_{p}\left(n^{1/(2+\alpha)}\right)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{(2-\alpha)/2(2+\alpha)}
\]


then
\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}=O_{p}\left(\tilde{\lambda}_{T}\right)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{1/2}
\]


and 
\[
\sum_{j=1}^{J}I_{j}(\hat{g}_{j})=O_{p}(1)\sum_{j=1}^{J}I_{j}(g_{j}^{*})
\]



\subsubsection*{Proof:}

The basic inequality gives us:

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le2\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|+\lambda^{2}\sum_{j=1}^{J}I_{j}(g_{j}^{*})
\]


\textbf{Case 1: $\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\le\lambda^{2}\sum_{j=1}^{J}I_{j}(g_{j}^{*})$}

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}(\lambda)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{1/2}
\]


\textbf{Case 2: $\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\ge\lambda^{2}\sum_{j=1}^{J}I_{j}(g_{j}^{*})$}

By Vandegeer (10.6), the basic inequality becomes 
\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le O_{p}\left(n^{-1/2}\right)\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha/2}\left(\sum_{j=1}^{J}I_{j}(\hat{g}_{j})+I_{j}(g_{j}^{*})\right)^{\alpha/2}
\]


\textbf{Case 2a:} $\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le\sum_{j=1}^{J}I_{j}(g_{j}^{*})$ 

Then 
\begin{eqnarray*}
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T} & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha/(2+\alpha)}
\end{eqnarray*}


\textbf{Case 2b:} $\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\ge\sum_{j=1}^{J}I_{j}(g_{j}^{*})$ 

Then

\begin{eqnarray*}
\sum_{j=1}^{J}I_{j}(\hat{g}_{j}) & \le & O_{p}\left(n^{-1/(2-\alpha)}\right)\lambda^{-4/(2-\alpha)}\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}
\end{eqnarray*}


Hence

\begin{eqnarray*}
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T} & \le & O_{p}\left(n^{-1/(2-\alpha)}\right)\lambda^{-2\alpha/(2-\alpha)}
\end{eqnarray*}



\subsection*{Result 3: Additive Model with multiple penalties, Single cross-validation
$\lambda$ over general $X_{T},X_{V}$}

Suppose that the training and validation set are independently sampled,
so the values $X_{i}$ are not necessarily the same. Suppose the training
and validation sets are both of size $n$. Suppose $X$ is bounded
s.t. $|X|\le R_{X}$ and the domain of $g\in\mathcal{G}$ is over
$(-R_{X},R_{X})$.

Suppose the same entropy bound (2) for both the training set $P_{T}$
and validation set $P_{V}$.

In addition to the assumptions in Result 4, suppose the infinity norm
is also bounded 
\begin{eqnarray*}
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|_{\infty}}{\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})} & \le & K<\infty
\end{eqnarray*}


Suppose there exist constants $M$,$M_{0}$ s.t. for all $j$ and
all $\lambda\in\Lambda$ 

\[
I_{j}\left(\hat{g}_{\lambda,j}\right)\le M\|\hat{g}_{\lambda,j}\|_{V}^{2}+M_{0}
\]


\textbf{Special assumption:} Suppose the incoherence condition $\gamma_{V}<1/(J-1)$.
We will also suppose $\int g_{j}p_{Vj}d\mu=0$ for $j=2,...,J$.

Let $\tilde{\lambda}$ be the optimal $\lambda$ as specified in Result
2. Then

\[
\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-\hat{g}_{\tilde{\lambda},j}\|_{V}=O_{p}\left(n^{-1/(2+\alpha)}\right)\left(1-\gamma(J-1)\right)^{\alpha/(2+\alpha)}\left(\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha/(2+\alpha)}+\sum_{j=1}^{J}I_{j}(g_{j}^{*})+\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}^{\alpha/2(2+\alpha)}\right)
\]


and $\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}$
is on the same order (differs by a constant).


\subsubsection*{Proof:}

By the triangle inequality, 
\[
\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}\le\left\Vert \sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}+\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}
\]


By Lemma 2 and Result 2, we can easily bound$\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}$.

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V} & \le & \left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{T}+\left|\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{T}-\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}\right|\\
 & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)\left(\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha/(2+\alpha)}+\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)
\end{eqnarray*}


Next bound $\left\Vert \sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}$.
By definition of $\hat{\lambda}$, we have the basic inequality

\[
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{2}\le2\left|\left(\epsilon,\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|+2\left|\left(\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j},\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|
\]


\textbf{Case 1:} $\left|\left(\epsilon,\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|$
is bigger

By Vandegeer (10.6),
\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(\sum_{j=1}^{J}I_{j}(\hat{g}_{\tilde{\lambda},j})+I_{j}(\hat{g}_{\hat{\lambda},j})\right)^{\alpha/2}
\end{eqnarray*}


If $\sum_{j=1}^{J}I_{j}(\hat{g}_{\tilde{\lambda},j})\ge\sum_{j=1}^{J}I_{j}(\hat{g}_{\hat{\lambda},j})$,
we're done.

Otherwise, suppose $\sum_{j=1}^{J}I_{j}(\hat{g}_{\tilde{\lambda},j})<\sum_{j=1}^{J}I_{j}(\hat{g}_{\hat{\lambda},j})$. 

By the incoherence assumption, we can apply Lemma 4

\begin{eqnarray*}
\sum_{j=1}^{J}I_{j}(\hat{g}_{\hat{\lambda},j}) & \le & M\sum_{j=1}^{J}\|\hat{g}_{\lambda j}\|_{V}^{2}+M_{0}J\\
 & \le & M\left(1-\gamma(J-1)\right)\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}\|_{V}^{2}+M_{0}J
\end{eqnarray*}


Then
\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(M\left(1-\gamma(J-1)\right)\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}\|_{V}^{2}+M_{0}J\right)^{\alpha/2}
\end{eqnarray*}


If $M_{0}J$ is the biggest, we're done. Otherwise,

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(1-\gamma(J-1)\right)^{\alpha/2}\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}\|_{V}^{\alpha}\\
 & \le & O_{p}(n^{-1/2})\left(1-\gamma(J-1)\right)^{\alpha/2}\left(\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}+\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-g_{j}^{*}\right\Vert _{V}+\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}\right)^{\alpha}
\end{eqnarray*}


If $\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V}$or
$\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-g_{j}^{*}\right\Vert _{V}$
is the biggest on the RHS, then the rate is faster than $O_{p}(n^{-1/(2+\alpha)})$.\textbf{
}If\textbf{$\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}$ }is
the biggest, then

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V} & \le & O_{p}(n^{-1/(2+\alpha)})\left\Vert \sum_{j=1}^{J}g_{j}^{*}\right\Vert _{V}^{\alpha/2(2+\alpha)}
\end{eqnarray*}


\textbf{Case 2:} $\left|\left(\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j},\sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right)_{V}\right|$
is bigger

By Cauchy Schwarz, 

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\hat{g}_{\tilde{\lambda},j}-\hat{g}_{\hat{\lambda},j}\right\Vert _{V} & \le & O_{p}(1)\left\Vert \sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right\Vert _{V}
\end{eqnarray*}


\pagebreak{}


\subsection*{Lemmas}


\subsubsection*{Lemma 1: }

Suppose for all $\lambda\in\Lambda$, the penalty function $I^{v}(g_{\lambda})$
is upper-bounded by $\|g_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}g_{\lambda}^{2}(x_{i})$
with constants $M_{0}$ and $M$:

\[
I^{v}(g_{\lambda})\le M\|g_{\lambda}\|_{n}^{2}+M_{0}
\]


Suppose there is some function $g\in\mathcal{G}$ such that
\[
\|g-g_{\lambda}\|_{n}^{1+\alpha/2}\le O_{p}(n^{-1/2})I^{\alpha/2}(g_{\lambda})
\]


Then

\[
\|g-g_{\lambda}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha v/(2+\alpha)}\|g\|_{n}^{2\alpha/v(2+\alpha)}
\]


\textbf{Proof:}

From the assumptions, we have

\begin{eqnarray*}
\|g-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(M\|g_{\lambda}\|_{n}^{2}+M_{0}\right)^{\alpha/2v}
\end{eqnarray*}


If $M_{0}>\|g_{\lambda}\|_{n}^{2}$, we're done. Otherwise,

\begin{eqnarray*}
\|g-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\|g_{\lambda}\|_{n}^{\alpha/v}\\
 & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\left(\|g_{\lambda}-g\|_{n}+\|g\|_{n}\right)^{\alpha/v}
\end{eqnarray*}


\textbf{Case 1:} $\|g_{\lambda}-g\|_{n}\ge\|g\|_{n}$

Then

\[
\|g-g_{\lambda}\|_{n}\le O_{p}(n^{-v/(2v+\alpha v-2\alpha)})M^{\alpha v^{2}/(2v+\alpha v-2\alpha)}
\]


Note that $\sup_{v}-\frac{v}{2v+\alpha v-2\alpha}=-\frac{1}{2+\alpha}$,
so this rate is faster than $O_{p}(n^{-\frac{1}{2+\alpha}})$.

\textbf{Case 2:} $\|g_{\lambda}-g\|_{n}\le\|g\|_{n}$

Then

\[
\|g-g_{\lambda}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha v/(2+\alpha)}\|g\|_{n}^{2\alpha/v(2+\alpha)}
\]


I believe we can often provide a good estimate of $M$ for the entire
class $\mathcal{G}$, which means that we can always estimate the
sample size needed to ensure this case never occurs. That is, I believe
we can often estimate $M$ s.t. 
\[
I^{v}(g)\le M\|g\|_{n}^{2}+M_{0}\forall g\in\mathcal{G}
\]



\subsubsection*{Lemma 2:}

Let $P_{n'}$ and $P_{n''}$ be empirical distributions over $\{X_{i}'\}_{i=1}^{n},\{X_{i}''\}_{i=1}^{n}$.
Let $P_{2n}=\frac{1}{2}\left(P_{n'}+P_{n''}\right)$. Suppose $X$
is bounded s.t. $|X|<R_{X}$.

Let $\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} $.
Suppose $g$ is defined over the domain over $X$ (and zero otherwise).
Suppose 
\[
\sup_{f\in\mathcal{G}'}\|f\|_{P_{2n}}\le R<\infty,\mbox{ }\sup_{f\in\mathcal{G}'}\|f\|_{\infty}\le K<\infty
\]


and 
\[
H\left(\delta,\mathcal{G}',P_{n'}\right)\le\tilde{A}\delta^{-\alpha},\mbox{ }H\left(\delta,\mathcal{G}',P_{n''}\right)\le\tilde{A}\delta^{-\alpha}
\]


Then 

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n'}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]



\paragraph{Proof:}

The proof is very similar to that in Pollard 1984 (page 32), so some
details below are omitted.

First note that for any function $f$ and $h$, we have 
\[
\|f\|_{P_{n'}}-\|h\|_{P_{n'}}\le\|f-h\|_{P_{n'}}\le\sqrt{2}\|f-h\|_{P_{2n}}
\]


Similarly for $P_{n''}$. 

Let $\{h_{j}\}_{j=1}^{N}$ be the $\sqrt{2}\delta$-cover for $\mathcal{G}'$
(where $N=N(\sqrt{2}\delta,\mathcal{G}',P_{2n})$). Let $h_{j}$ be
the closest function (in terms of $\|\cdot\|_{P_{2n}}$) to some $f\in\mathcal{G}'$.
Then 
\begin{eqnarray*}
\|f\|_{P_{n'}}-\|f\|_{P_{n''}} & \le & \|f-h_{j}\|_{P_{n'}}+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|+\|f-h_{j}\|_{P_{n''}}\\
 & \le & 4\delta+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|
\end{eqnarray*}


Therefore for $f=\frac{g^{*}-g}{I(g^{*})+I(g)}$, we have
\begin{eqnarray*}
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right) & \le & Pr\left(\sup_{j\in1:N}\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\\
 & \le & N\max_{j\in1:N}Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)
\end{eqnarray*}


Now note that 
\begin{eqnarray*}
\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right| & = & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\|h_{j}\|_{P_{n'}}+\|h_{j}\|_{P_{n''}}}\\
 & \le & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\sqrt{2}\|h_{j}\|_{P_{2n}}}
\end{eqnarray*}


By Hoeffding's inequality, 
\begin{eqnarray*}
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right) & \le & Pr\left(\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|\ge2\sqrt{2}\delta\|h_{j}\|_{P_{2n}}\right)\\
 & = & Pr\left(\left|\sum_{i=1}^{n}W_{i}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)\right|\ge2\sqrt{2}n\delta\|h_{j}\|_{P_{2n}}\right)\\
 & \le & 2\exp\left(-\frac{16\delta^{2}n^{2}\|h_{j}\|_{P_{2n}}^{2}}{4\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2}}\right)
\end{eqnarray*}


Since $\|h_{j}\|_{\infty}<K$, then

\begin{eqnarray*}
\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2} & \le & \sum_{i=1}^{n}h_{j}^{4}(x_{i}')+h_{j}^{4}(x_{i}'')\\
 & \le & nK^{2}\|h_{j}\|_{P_{2n}}^{2}
\end{eqnarray*}


Hence 
\[
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\le2\exp\left(-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Since (Pollard and Vandegeer say that) 
\[
N(\sqrt{2}\delta,\mathcal{G}',P_{2n})\le N(\delta,\mathcal{G}',P_{n''})+N(\delta,\mathcal{G}',P_{n''})
\]


then

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Using shorthand, we can write 
\[
\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}=O_{p}(n^{-1/(2+\alpha)})
\]



\subsubsection*{Lemma 3:}

Suppose the function classes $\mathcal{F}_{j}$ is a cone and $I_{j}:\mathcal{F}_{j}\mapsto[0,\infty)$
is a psuedonorm. Furthermore, suppose 
\[
H\left(\delta,\{f_{j}\in\mathcal{F}_{j}:I_{j}(f_{j})\le1\},\|\cdot\|_{n}\right)\le A_{j}\delta^{-\alpha_{j}}
\]
Then if $f_{j}^{*}\in\mathcal{F}_{j}$, then

\begin{eqnarray*}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}:f_{j}\in\mathcal{F}_{j},I_{j}(f_{j})+I_{j}(f_{j}^{*})>0\right\} ,\|\cdot\|_{n}\right) & \le & 2\sum_{j=1}^{J}A_{j}\left(\frac{\delta}{2J}\right)^{-\alpha_{j}}
\end{eqnarray*}



\paragraph*{Proof:}

Let $\tilde{f}_{j}=\frac{f_{j}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}$.
Then $\tilde{f}_{j}\in\mathcal{F}_{j}$ and $I_{j}(\tilde{f}_{j})\le1$.
Let $h_{(j)}$ be the closest function to $\tilde{f}_{j}$ in the
$\delta$ cover of $\mathcal{F}_{j}$. Similarly, let $h_{(j)}^{*}$
be the closest function to $\tilde{f}_{j}^{*}$ in the $\delta$ cover
of $\mathcal{F}_{j}$. Then 
\begin{eqnarray*}
\left\Vert \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-\left(\sum_{j=1}^{J}h_{(j)}-h_{(j)}^{*}\right)\right\Vert  & \le & \sum_{j=1}^{J}\left\Vert \frac{f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-\left(h_{(j)}-h_{(j)}^{*}\right)\right\Vert \\
 & \le & \sum_{j=1}^{J}\left\Vert \frac{f_{j}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-h_{(j)}\right\Vert +\left\Vert \frac{f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-h_{(j)}^{*}\right\Vert \\
 & \le & 2J\delta
\end{eqnarray*}


Hence

\[
H\left(2J\delta,\left\{ \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}:f_{j}\in\mathcal{F}_{j},I_{j}(f_{j})+I_{j}(f_{j}^{*})>0\right\} ,\|\cdot\|_{n}\right)\le2\sum_{j=1}^{J}A_{j}\delta^{-\alpha_{j}}
\]



\subsubsection*{Lemma 4:}

Let $p_{n}(\vec{x})$ be some empirical density and let $p_{nj}$
be the corresponding empirical marginal density of $x_{j}$. Let 
\[
r(\vec{x})=\frac{p_{n}(\vec{x})}{\Pi_{j=1}^{J}p_{nj}(x_{j})},\mbox{ }\gamma^{2}=\int(r(\vec{x})-1)^{2}\Pi_{j=1}^{J}p_{nj}(x_{j})d\mu
\]
Suppose $\gamma<1/(J-1)$. Furthermore, suppose $\int g_{j}p_{nj}d\mu=0$
for $j=2,...,J$. Then 
\[
\left\Vert \sum_{j=1}^{J}g_{j}\right\Vert _{n}^{2}\ge(1-\gamma(J-1))\left(\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}\right)
\]



\paragraph*{Proof:}

The proof is very similar to Lemma 5.1 in Vandegeer 2014 ``The additive
model with different smoothness for the components.''

\[
\left\Vert \sum_{j=1}^{J}g_{j}\right\Vert _{n}^{2}=\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}+\sum_{j\ne k}\int g_{j}g_{k}p_{n}(\vec{x})d\mu
\]


We bound the latter term:

\begin{eqnarray*}
\left|\int g_{j}g_{k}p_{n}(\vec{x})d\mu\right| & = & \left|\int g_{j}g_{k}\left(r(\vec{x})-1\right)\Pi_{j=1}^{J}p_{nj}(x_{j})d\mu\right|\\
 & \le & \gamma\left|\int g_{j}^{2}g_{k}^{2}\Pi_{j=1}^{J}p_{nj}(x_{j})d\mu\right|^{1/2}\\
 & = & \gamma\|g_{j}\|_{n}\|g_{k}\|_{n}
\end{eqnarray*}


Hence

\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}g_{j}\right\Vert _{n}^{2} & \ge & \sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}-\gamma\sum_{j\ne k}\|g_{j}\|_{n}\|g_{k}\|_{n}\\
 & \ge & (1-\gamma(J-1))\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}+\gamma\sum_{j<k}\left(\|g_{j}\|_{n}-\|g_{k}\|_{n}\right)^{2}\\
 & \ge & (1-\gamma(J-1))\sum_{j=1}^{J}\left\Vert g_{j}\right\Vert _{n}^{2}
\end{eqnarray*}

\end{document}
