%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{esint}
\usepackage{babel}
\begin{document}

\subsection*{Definitions}

We find the best model for $y$ over function class $\mathcal{G}$.
Presume $g^{*}\in\mathcal{G}$ is the true model and 
\[
y=g^{*}(X)+\epsilon
\]


Given a training set $T$ , We define the fitted models 
\[
\hat{g}_{\lambda}=\|y-g\|_{T}^{2}+\lambda^{2}I^{v}(g)
\]


Given a validation set $T$ , let the CV-fitted model be 
\[
\hat{g}_{\hat{\lambda}}=\arg\min_{\lambda}\|y-\hat{g}_{\lambda}\|_{V}^{2}
\]


We will suppose $I(g^{*})>0$.


\subsection*{Assumptions}

Suppose we have sub-Gaussian errors $\epsilon$ for constants $K$
and $\sigma_{0}^{2}$:
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Suppose $v>2\alpha/(2+\alpha)$. 

Suppose that the entropy of the class $\mathcal{G}'$ is 
\begin{eqnarray*}
H\left(\delta,\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} ,P_{n}\right) & \le & \tilde{A}\delta^{-\alpha}
\end{eqnarray*}
 

Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by $\|\hat{g}_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}\hat{g}_{\lambda}(x_{i})$.
See Lemma 1 below for the specific assumption. This assumption includes
Ridge, Lasso, Generalized Lasso, and the Group Lasso.


\subsection*{Result 1: Single $\lambda$, Single Penalty, cross-validation over
$X_{T}=X_{V}$}

For now, we will suppose $P_{n}=\{X_{i}\}_{i=1}^{n}$ are the same
between the validation and training set. 

Also, suppose the penalty normalizes the empirical norm such that:

\[
\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{n}}{I(g)+I(g^{*})}\le R<\infty
\]


Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by its $L_{2}$-norm with some constant $M$ and
$M_{0}$ such that

\[
I^{v}(\hat{g}_{\lambda})\le M\|\hat{g}_{\lambda}\|_{n}^{2}+M_{0}
\]


Then

\[
\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{n}=O_{p}(n^{-1/(2+\alpha)})\left(M^{\frac{\alpha v-2\alpha+2v}{v(v-2)(2+\alpha)}}R^{v/(v-2)}\vee I^{2\alpha/(2+\alpha)}(g^{*})\right)
\]



\subsubsection*{Proof}

Let $\tilde{\lambda}$ be the optimal $\lambda$ under the given assumptions,
as specified by Van de geer. From the definition of $\hat{\lambda}$,
we get the following basic inequality 
\begin{eqnarray*}
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{V}^{2} & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}+2(\epsilon,\hat{g}_{\hat{\lambda}}-\hat{g}_{\tilde{\lambda}})_{V}\\
 & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}+2(\epsilon,\hat{g}_{\hat{\lambda}}-g^{*})_{V}+2(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\\
 & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}+2\left|(\epsilon,\hat{g}_{\hat{\lambda}}-g^{*})_{V}\right|+2\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|
\end{eqnarray*}


By considering the largest term on the RHS, we have following three
cases.

\textbf{Case 1:} $\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}$ is
the largest

Since we have assumed that the validation and training set are equal,
then $\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}$ converges at the optimal
rate $O_{p}(n^{-1/(2+\alpha)})$.

\textbf{Case 2:} $\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|$
is the largest

In this case, since $\epsilon_{V}$ is independent of $\hat{g}_{\tilde{\lambda}}$,
then by Cauchy Schwarz, 
\begin{eqnarray*}
\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right| & \le & \|\epsilon_{V}\|\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}\\
 & \le & O_{p}\left(n^{-1/2}\right)\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}
\end{eqnarray*}


Hence $\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|$
will shrink a bit faster than the optimal rate at a rate of $O_{p}(n^{-(\frac{1}{2+\alpha}+\frac{1}{2})})$.

\textbf{Case 3:} $\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|$
is the largest.

By the assumptions given, Vandegeer (10.6) gives us that 
\[
\sup_{g\in\mathcal{G}}\frac{|(\epsilon,g-g*)_{n}|}{\|g-g*\|_{n}^{1-\alpha/2}(I(g^{*})+I(g))^{\alpha/2}}=O_{p}(n^{-1/2})
\]


Hence 
\[
\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|\le O_{p}(n^{-1/2})\|\hat{g}_{\hat{\lambda}}-g*\|_{n}^{1-\alpha/2}\left(I(g^{*})+I(\hat{g}_{\hat{\lambda}})\right)^{\alpha/2}
\]


If $I(g^{*})\ge I(g_{\hat{\lambda}})$ , then

\[
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{V}\le O_{p}(n^{-1/(2+\alpha)})I(g^{*})^{\alpha/(2+\alpha)}
\]


Otherwise, we have

\[
\|\hat{g}_{\hat{\lambda}}-g*\|_{n}^{1+\alpha/2}\le O_{p}(n^{-1/2})I(\hat{g}_{\hat{\lambda}}){}^{\alpha/2}
\]


By Lemma 1 below, using the assumption that the penalty of $\hat{g}_{\lambda}$
is bounded above by its $L_{2}(P_{n})$ norm, we have that

\[
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\frac{\alpha v-2\alpha+2v}{v(v-2)(2+\alpha)}}R^{v/(v-2)}
\]



\subsection*{Result 2: Single $\lambda$, Single Penalty, cross-validation over
general $X_{T},X_{V}$}

Now suppose that the training and validation set are independently
sampled, so the values $X_{i}$ are not necessarily the same. Suppose
$X$ is bounded s.t. $|X|\le R_{X}$ and the domain of $g\in\mathcal{G}$
is over $(-R_{X},R_{X})$.

We suppose the training and validation sets are both of size $n$.

Suppose the penalty normalizes the empirical norm as follows:

\[
\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{T}}{I(g)+I(g^{*})}\le R<\infty,\mbox{ }\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{V}}{I(g)+I(g^{*})}\le R<\infty
\]


Suppose that 
\begin{eqnarray*}
\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{\infty}}{I(g)+I(g^{*})} & \le & K<\infty
\end{eqnarray*}


Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by its $L_{2}$-norm with constants $M$ and $M_{0}$:

\[
I^{v}(\hat{g}_{\lambda})\le M\left(\|\hat{g}_{\lambda}\|_{T}^{2}+\|\hat{g}_{\lambda}\|_{V}^{2}\right)+M_{0}=M\|\hat{g}_{\lambda}\|_{2n}^{2}+M_{0}
\]


Then for any $\xi>0$,

\[
\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{V}=O_{p}(n^{-1/(2+\alpha+\xi)})I(g^{*})
\]



\paragraph{Proof:}

We follow the same proof structure of going thru the three cases,
modifying the proofs as appropriate:

\textbf{Case 1:} $\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}$ is
the largest

By Lemma 2, we have
\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Hence for any $\xi>0$,
\[
\frac{\left|\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{T}-\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}\right|}{I(g^{*})+I(\hat{g}_{\tilde{\lambda}})}\le O_{p}(n^{-1/(2+\alpha+\xi)})
\]


Therefore
\begin{eqnarray*}
\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V} & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{T}+O_{p}(n^{-1/(2+\alpha+\xi)})\left(I(g^{*})+I(\hat{g}_{\tilde{\lambda}})\right)\\
 & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{T}+O_{p}(n^{-1/(2+\alpha+\xi)})I(g^{*})
\end{eqnarray*}


Hence we can attain a rate that is infinitely close to the optimal
rate.

\textbf{Case 2:} $\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|$
is the largest

The same proof still holds.

\textbf{Case 3:} $\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|$
is the largest.

Again, we have by Van de geer (10.6),
\[
\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|\le O_{p}(n^{-1/2})\|\hat{g}_{\hat{\lambda}}-g*\|_{V}^{1-\alpha/2}(I(g^{*})+I(\hat{g}_{\hat{\lambda}}))^{\alpha/2}
\]


If $I(g^{*})\ge I(g_{\hat{\lambda}})$ is true, then result is clearly
attained.

Otherwise, we have

\[
\|\hat{g}_{\hat{\lambda}}-g*\|_{V}^{1+\alpha/2}\le O_{p}(n^{-1/2})I(\hat{g}_{\hat{\lambda}}){}^{\alpha/2}
\]


By Lemma 1 below, since the penalty is bounded above by the $L_{2}(P_{n})$
norm, it follows that

\[
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{V}\le O_{p}(n^{-1/(2+\alpha)})M^{\frac{\alpha v-2\alpha+2v}{v(v-2)(2+\alpha)}}R^{v/(v-2)}
\]



\subsection*{Result 3: Single $\lambda$, Multiple Penalties, Optimal $\tilde{\lambda}_{T}$
over $X_{T}$}

Consider function classes $\mathcal{G}_{j}$ that are cones. Also,
suppose we have an additive model:

\[
y=\sum_{j=1}^{J}g_{j}^{*}+\epsilon
\]


where $g_{j}^{*}\in\mathcal{G}_{j}$.

We fit the model by least squares with separate penalties for each
function $g_{j}$:
\[
\{\hat{g}_{j}\}_{j=1}^{J}=\arg\min_{g_{j}\in\mathcal{G}_{j}}\|y-\sum_{j=1}^{J}g_{j}\|_{T}^{2}+\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j})
\]


Suppose $v_{j}\ge1$ for all $j$. (This requirement on $v_{j}$ stricter
than Vandegeer Them 10.2)

Suppose for all $j$, there is some $0<\alpha<2$ s.t. for all $\delta>0$,
\[
H\left(\delta,\{g_{j}\in\mathcal{G}_{j}:I(g_{j})\le1\},\|\cdot\|_{T}\right)\le A\delta^{-\alpha}
\]


and that for all $j$

\[
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\|g_{j}-g_{j}^{*}\|_{T}}{I(g_{j})+I(g_{j}^{*})}\le R<\infty
\]


If we choose $\lambda$ s.t. 
\[
\tilde{\lambda}_{T}^{-1}=O_{p}\left(n^{1/(2+\alpha)}\right)\left(J+\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})\right)^{(2-\alpha)/2(2+\alpha)}
\]


then
\[
\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|_{T}=O_{p}\left(\tilde{\lambda}_{T}\right)J^{\alpha/4}\left(\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})\right)^{1/2}
\]


and 
\[
\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le J^{1/2+\alpha/4}\left(J+\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})\right)
\]



\subsubsection*{Proof:}

The basic inequality gives us:

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}(\hat{g}_{j})\le2\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|+\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}\left(g_{j}^{*}\right)
\]


\textbf{Case 1: $\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\le\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}\left(g_{j}^{*}\right)$}

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}(\lambda)\left(\frac{1}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}\left(g_{j}^{*}\right)\right)^{1/2}
\]


\textbf{Case 2: $\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\ge\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}\left(g_{j}^{*}\right)$}

By Lemma 3,

\[
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sum_{j=1}^{J}I(g_{j})+I(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le\tilde{A}J^{1-\alpha}\delta^{-\alpha}
\]


Hence by (10.6) in Vandegeer,

\[
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\left|\left(\epsilon_{T},\sum_{j=1}^{J}g_{j}-g_{j}^{*}\right)\right|}{\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|^{1-\alpha/2}\left(\sum_{j=1}^{J}I(g_{j})+I(g_{j}^{*})\right)^{\alpha/2}}=O_{p}\left(n^{-1/2}\right)J^{1-\alpha}
\]


and the basic inequality becomes 
\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\frac{\lambda^{2}}{J}\sum_{j=1}^{J}I_{j}^{v_{j}}(\hat{g}_{j})\le O_{p}\left(n^{-1/2}\right)J^{1-\alpha}\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha/2}\left(\sum_{j=1}^{J}I(\hat{g}_{j})+I(g_{j}^{*})\right)^{\alpha/2}
\]


\textbf{Case 2a:} Suppose $\sum_{j=1}^{J}I(\hat{g}_{j})\le\sum I(g_{j}^{*})$. 

Then 
\begin{eqnarray*}
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T} & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)J^{\frac{2(1-\alpha)}{\alpha+2}}\left(\sum_{j=1}^{J}I(g_{j}^{*})\right)^{\alpha/(2+\alpha)}
\end{eqnarray*}


\textbf{Case 2b:} Suppose $\sum_{j=1}^{J}I(\hat{g}_{j})\ge\sum I(g_{j}^{*})$. 

Then 
\begin{eqnarray*}
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1+\alpha/2} & \le & O_{p}\left(n^{-1/2}\right)\left(\sum_{j=1}^{J}I(\hat{g}_{j})\right)^{\alpha/2}
\end{eqnarray*}


By assuming $v_{j}\ge1$, we must have $I_{j}(\hat{g}_{j})\le I_{j}^{v_{j}}(\hat{g}_{j})\vee1$.
Hence

\begin{eqnarray*}
\sum_{j=1}^{J}I_{j}(\hat{g}_{j}) & \le & J+\sum_{j=1}^{J}I_{j}^{v_{j}}(\hat{g}_{j})\\
 & \le & J+O_{p}\left(n^{-1/2}\right)J^{2-\alpha}\lambda^{-2}\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha/2}\left(\sum_{j=1}^{J}I(\hat{g}_{j})\right)^{\alpha/2}
\end{eqnarray*}


\textbf{Case 2ba:} If $J\le O_{p}\left(n^{-1/2}\right)J^{2-\alpha}\lambda^{-2}\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha/2}\left(\sum_{j=1}^{J}I(\hat{g}_{j})\right)^{\alpha/2}$,
then

\[
\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le O_{p}\left(n^{-1/(2-\alpha)}\right)J^{1/2}\lambda^{-4/(2-\alpha)}\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}
\]


which implies

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}\left(n^{-1/(2-\alpha)}\right)J^{\alpha/4}\lambda^{-2\alpha/(2-\alpha)}
\]


and 
\[
\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le J^{1/2+\alpha/4}\left(J+\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})\right)
\]


\textbf{Case 2bb:} If $J\ge O_{p}\left(n^{-1/2}\right)J^{2-\alpha}\lambda^{-2}\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha/2}\left(\sum_{j=1}^{J}I(\hat{g}_{j})\right)^{\alpha/2}$,
then
\[
\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le J\implies\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}\left(n^{-1/(2+\alpha)}\right)J^{\alpha/(2+\alpha)}
\]



\subsection*{Result 4: Single $\lambda$, Multiple Penalties, cross-validation
over general $X_{T},X_{V}$}

Now suppose that the training and validation set are independently
sampled, so the values $X_{i}$ are not necessarily the same. Suppose
$X$ is bounded s.t. $|X|\le R_{X}$ and the domain of $g\in\mathcal{G}$
is over $(-R_{X},R_{X})$.

We suppose the training and validation sets are both of size $n$.

Suppose the penalty normalizes the empirical norm as follows:

\[
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\|g_{j}-g_{j}^{*}\|_{T}}{I(g_{j})+I(g_{j}^{*})}\le R<\infty,\mbox{ }\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\|g_{j}-g_{j}^{*}\|_{V}}{I(g_{j})+I(g_{j}^{*})}\le R<\infty
\]


We suppose the same entropy conditions as Result 3. Furthermore, suppose
that 
\begin{eqnarray*}
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\|g_{j}-g_{j}^{*}\|_{\infty}}{I(g_{j})+I(g_{j}^{*})} & \le & K<\infty
\end{eqnarray*}


Suppose there exist constants $M$ and $M_{0}$ s.t. for all $\lambda\in\Lambda$
and all $j$, $I_{j}^{v_{j}}(\hat{g}_{\lambda,j})$ is upper bounded
by its $L_{2}$-norm :

\[
I_{j}^{v_{j}}(\hat{g}_{\lambda,j})\le M\left(\|\hat{g}_{\lambda,j}\|_{T}^{2}+\|\hat{g}_{\lambda,j}\|_{V}^{2}\right)+M_{0}=M\|\hat{g}_{\lambda,j}\|_{2n}^{2}+M_{0}
\]


Then for any $\xi>0$,

\[
\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-g_{j}^{*}\|_{V}=O_{p}(n^{-1/(2+\alpha+\xi)})\left(\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})\right)
\]



\subsubsection*{Proof:}

The proof is very similar to Result 2.

\textbf{Case 1:} $\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\|_{V}^{2}$
is the largest

By Lemma 2, we have
\[
Pr\left(\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\left|\|\sum_{j=1}^{J}g_{j}^{*}-g_{j}\|_{P_{n}}-\|\sum_{j=1}^{J}g_{j}^{*}-g_{j}\|_{P_{n''}}\right|}{\sum_{j=1}^{J}I_{j}(g_{j}^{*})+I_{j}(g_{j})}\ge6\delta\right)\le2\exp\left(2\tilde{A}J^{1-\alpha}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Hence for any $\xi>0$,
\[
\frac{\left|\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\|_{T}-\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\|_{V}\right|}{\sum_{j=1}^{J}I_{j}(g_{j}^{*})+I_{j}(\hat{g}_{\tilde{\lambda},j})}\le O_{p}(n^{-1/(2+\alpha+\xi)})
\]


Therefore
\begin{eqnarray*}
\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\|_{V} & \le & \|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\|_{T}+O_{p}(n^{-1/(2+\alpha+\xi)})\left(\sum_{j=1}^{J}I(g_{j}^{*})+I(\hat{g}_{\tilde{\lambda},j})\right)\\
 & \le & O_{p}(n^{-1/(2+\alpha+\xi)})J^{1/2+\alpha/4}\left(J+\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})\right)
\end{eqnarray*}


Hence we can attain a rate that is infinitely close to the optimal
rate.

\textbf{Case 2:} $\left|\left(\epsilon_{V},\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right)\right|$
is the largest

Since $\epsilon_{V}$ is independent of $\left\{ \hat{g}_{\tilde{\lambda},j}\right\} $,
then this term shrinks at the rate of $O_{p}(n^{-1/2-1/(2+\alpha_{max})})$.
(So the rate is faster than the optimal rate.)

\textbf{Case 3:} $\left|\left(\epsilon_{V},\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right)\right|$
is the largest.

Again, we have by Vandegeer (10.6),
\[
\left|\left(\epsilon_{V},\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right)\right|\le O_{p}(n^{-1/2})\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\|_{V}^{1-\alpha/2}\left(\sum_{j=1}^{J}I(g_{j}^{*})+I(\hat{g}_{\hat{\lambda},j})\right)^{\alpha/2}
\]


If $\sum_{j=1}^{J}I(g_{j}^{*})\ge\sum_{j=1}^{J}I(\hat{g}_{\hat{\lambda},j})$
is true, then result is clearly attained.

Otherwise, we have

\[
\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\|_{V}^{1+\alpha/2}\le O_{p}(n^{-1/2})\left(\sum_{j=1}^{J}I(\hat{g}_{\hat{\lambda},j})\right)^{\alpha/2}
\]


By the assumption that the penalty is bounded by the $L_{2}(P_{2n})$
norm,

\begin{eqnarray*}
\sum_{j=1}^{J}I(\hat{g}_{\hat{\lambda},j}) & \le & \sum_{j=1}^{J}\left(M\|\hat{g}_{\hat{\lambda},j}\|_{2n}^{2}+M_{0}\right)^{1/v_{j}}\\
 & \le & \sum_{j=1}^{J}\left(M\left(\|g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\|_{2n}+\|g_{j}^{*}\|_{2n}\right)^{2}+M_{0}\right)^{1/v_{j}}
\end{eqnarray*}


By Lemma 1a, $\|g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\|_{2n}$ is bounded
and by assumption $\|g_{j}^{*}\|_{2n}$ is also bounded. Hence

Hence for some constant $c$ dependent on $R,\|g_{j}^{*}\|_{2n},M,j,v_{j}$,
we have
\[
\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\|_{V}\le O_{p}(n^{-1/(2+\alpha)})c
\]



\subsection*{Result 5: Multiple $\lambda$, Multiple Penalties, Optimal $\lambda$
on $X_{T}$}

Consider an additive model:

\[
y=\sum_{j=1}^{J}g_{j}^{*}+\epsilon
\]


We fit the model by least squares with separate penalties and separate
$\lambda$ for each function $g_{j}$:
\[
\{\hat{g}_{j}\}_{j=1}^{J}=\arg\min_{g_{j}\in\mathcal{G}_{j}}\|y-\sum_{j=1}^{J}g_{j}\|_{T}^{2}+\frac{1}{J}\sum_{j=1}^{J}\lambda_{j}^{2}I_{j}^{v_{j}}(g_{j})
\]


Suppose $v_{j}>\frac{2\alpha_{j}}{2+\alpha_{j}}$ for all $j$.

Suppose for all $j$, there is some $0<\alpha_{j}<2$ s.t. for all
$\delta>0$, 
\[
H\left(\delta,\left\{ \frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le\frac{A}{J}\delta^{-\alpha_{j}}
\]


and for all $j$, 
\[
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\|g_{j}-g_{j}^{*}\|_{T}}{I(g_{j})+I(g_{j}^{*})}\le R<\infty
\]


If we choose $\lambda$ s.t. 
\[
\tilde{\lambda}_{j}^{-1}=???
\]


then
\[
\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|_{T}=???
\]


and 
\[
\sum_{j=1}^{J}I^{v_{j}}(\hat{g}_{\lambda,j})=???
\]



\subsubsection*{Proof:}


\subsection*{Lemmas}


\subsubsection*{Lemma 1: }

Suppose for all $\lambda\in\Lambda$, the penalty function $I^{v}(g_{\lambda})$
is upper-bounded by $\|g_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}g_{\lambda}^{2}(x_{i})$
with constants $M_{0}$ and $M$:

\[
I^{v}(g_{\lambda})\le M\|g_{\lambda}\|_{n}^{2}+M_{0}
\]


and 
\[
\frac{\|g_{\lambda}-g^{*}\|_{n}}{I(g_{\lambda})+I(g^{*})}\le R
\]


Then (lemma 1a) 
\[
\sup_{\lambda\in\Lambda}\|g_{\lambda}-g^{*}\|_{n}\le O_{p}(R^{v/(v-2)})M^{1/(v-2)}
\]


Furthermore, if there is some function $g^{*}\in\mathcal{G}$ such
that 
\[
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2}\le O_{p}(n^{-1/2})I^{\alpha/2}(g_{\lambda})
\]
then for sufficiently large $n$, (lemma 1b) 
\[
\|g^{*}-g_{\lambda}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\frac{\alpha v-2\alpha+2v}{v(v-2)(2+\alpha)}}
\]


\textbf{Proof:}

First we show that $\sup_{\lambda}\|g_{\lambda}-g^{*}\|_{n}$ is bounded
and does not grow with $n$. For any $\lambda$, we have

\begin{eqnarray*}
\|g_{\lambda}-g^{*}\|_{n} & \le & R\left(I(g_{\lambda})+I(g^{*})\right)
\end{eqnarray*}


Clearly if $I(g^{*})\ge I(g_{\lambda})$, we're done. Otherwise,
\[
\|g_{\lambda}-g^{*}\|_{n}\le2RI(g_{\lambda})\le2R\left(M\|g_{\lambda}\|_{n}^{2}+M_{0}\right)^{1/v}
\]


If $M\|g_{\lambda}\|_{n}^{2}\le M_{0}$, we're done. Otherwise,
\begin{eqnarray*}
\|g_{\lambda}-g^{*}\|_{n} & \le & O_{p}(R)M^{1/v}\|g_{\lambda}\|_{n}^{2/v}\\
 & \le & O_{p}(R)M^{1/v}\left(\|g_{\lambda}-g^{*}\|_{n}+\|g^{*}\|_{n}\right)^{2/v}
\end{eqnarray*}


Again, if $\|g_{\lambda}-g^{*}\|_{n}\le\|g^{*}\|_{n}$, we're done.
Otherwise,
\[
\|g_{\lambda}-g^{*}\|_{n}\le O_{p}(R^{v/(v-2)})M^{1/(v-2)}
\]


So we've shown that $\sup_{\lambda}\|g_{\lambda}-g^{*}\|_{n}$ is
bounded (lemma 1a).

Now to prove lemma 1b, note that from the assumptions, we have

\begin{eqnarray*}
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(M\|g_{\lambda}\|_{n}^{2}+M_{0}\right)^{\alpha/2v}
\end{eqnarray*}


If $M_{0}>\|g_{\lambda}\|_{n}^{2}$, we're done. Otherwise,

\begin{eqnarray*}
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\|g_{\lambda}\|_{n}^{\alpha/v}\\
 & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\left(\|g_{\lambda}-g^{*}\|_{n}+\|g^{*}\|_{n}\right)^{\alpha/v}
\end{eqnarray*}


Since we showed that $\|g_{\lambda}-g^{*}\|_{n}$ is bounded, then
\[
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2}\le O_{p}(n^{-1/2})M^{\alpha/2v+1/(v-2)}R^{v/(v-2)}
\]


Hence

\[
\|g^{*}-g_{\lambda}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\frac{\alpha v-2\alpha+2v}{v(v-2)(2+\alpha)}}R^{v/(v-2)}
\]


I believe we can often provide a good estimate of $M$ for the entire
class $\mathcal{G}$, which means that we can always estimate the
sample size needed to ensure this case never occurs. That is, I believe
we can often estimate $M$ s.t. 
\[
I^{v}(g)\le M\|g\|_{n}^{2}+M_{0}\forall g\in\mathcal{G}
\]



\subsubsection*{Lemma 2:}

Let $P_{n'}$ and $P_{n''}$ be empirical distributions over $\{X_{i}'\}_{i=1}^{n},\{X_{i}''\}_{i=1}^{n}$.
Let $P_{2n}=\frac{1}{2}\left(P_{n'}+P_{n''}\right)$. Suppose $X$
is bounded s.t. $|X|<R_{X}$.

Let $\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} $.
Suppose $g$ is defined over the domain over $X$ (and zero otherwise).
Suppose 
\[
\sup_{f\in\mathcal{G}'}\|f\|_{P_{2n}}\le R<\infty,\mbox{ }\sup_{f\in\mathcal{G}'}\|f\|_{\infty}\le K<\infty
\]


and 
\[
H\left(\delta,\mathcal{G}',P_{n'}\right)\le\tilde{A}\delta^{-\alpha},\mbox{ }H\left(\delta,\mathcal{G}',P_{n''}\right)\le\tilde{A}\delta^{-\alpha}
\]


Then 

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n'}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]



\paragraph{Proof:}

The proof is very similar to that in Pollard 1984 (page 32), so some
details below are omitted.

First note that for any function $f$ and $h$, we have 
\[
\|f\|_{P_{n'}}-\|h\|_{P_{n'}}\le\|f-h\|_{P_{n'}}\le\sqrt{2}\|f-h\|_{P_{2n}}
\]


Similarly for $P_{n''}$. 

Let $\{h_{j}\}_{j=1}^{N}$ be the $\sqrt{2}\delta$-cover for $\mathcal{G}'$
(where $N=N(\sqrt{2}\delta,\mathcal{G}',P_{2n})$). Let $h_{j}$ be
the closest function (in terms of $\|\cdot\|_{P_{2n}}$) to some $f\in\mathcal{G}'$.
Then 
\begin{eqnarray*}
\|f\|_{P_{n'}}-\|f\|_{P_{n''}} & \le & \|f-h_{j}\|_{P_{n'}}+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|+\|f-h_{j}\|_{P_{n''}}\\
 & \le & 4\delta+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|
\end{eqnarray*}


Therefore for $f=\frac{g^{*}-g}{I(g^{*})+I(g)}$, we have
\begin{eqnarray*}
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right) & \le & Pr\left(\sup_{j\in1:N}\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\\
 & \le & N\max_{j\in1:N}Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)
\end{eqnarray*}


Now note that 
\begin{eqnarray*}
\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right| & = & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\|h_{j}\|_{P_{n'}}+\|h_{j}\|_{P_{n''}}}\\
 & \le & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\sqrt{2}\|h_{j}\|_{P_{2n}}}
\end{eqnarray*}


By Hoeffding's inequality, 
\begin{eqnarray*}
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right) & \le & Pr\left(\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|\ge2\sqrt{2}\delta\|h_{j}\|_{P_{2n}}\right)\\
 & = & Pr\left(\left|\sum_{i=1}^{n}W_{i}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)\right|\ge2\sqrt{2}n\delta\|h_{j}\|_{P_{2n}}\right)\\
 & \le & 2\exp\left(-\frac{16\delta^{2}n^{2}\|h_{j}\|_{P_{2n}}^{2}}{4\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2}}\right)
\end{eqnarray*}


Since $\|h_{j}\|_{\infty}<K$, then

\begin{eqnarray*}
\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2} & \le & \sum_{i=1}^{n}h_{j}^{4}(x_{i}')+h_{j}^{4}(x_{i}'')\\
 & \le & nK^{2}\|h_{j}\|_{P_{2n}}^{2}
\end{eqnarray*}


Hence 
\[
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\le2\exp\left(-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Since (Pollard and Vandegeer say that) 
\[
N(\sqrt{2}\delta,\mathcal{G}',P_{2n})\le N(\delta,\mathcal{G}',P_{n''})+N(\delta,\mathcal{G}',P_{n''})
\]


then

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Using shorthand, we can write that for any $\xi>0$, 
\[
\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}=O_{p}(n^{-1/(2+\alpha+\xi)})
\]



\subsubsection*{Lemma 3:}

Suppose the function classes $\mathcal{F}_{j}$ is a cone and $I_{j}:\mathcal{F}_{j}\mapsto[0,\infty)$
is a psuedonorm. Furthermore, suppose 
\[
H\left(\delta,\{f_{j}\in\mathcal{F}_{j}:I_{j}(f_{j})\le1\},\|\cdot\|_{n}\right)\le A_{j}\delta^{-\alpha_{j}}
\]
Then if $f_{j}^{*}\in\mathcal{F}_{j}$, then

\begin{eqnarray*}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}:f_{j}\in\mathcal{F}_{j},I_{j}(f_{j})+I_{j}(f_{j}^{*})>0\right\} ,\|\cdot\|_{n}\right) & \le & 2\sum_{j=1}^{J}A_{j}\left(\frac{\delta}{2J}\right)^{-\alpha_{j}}
\end{eqnarray*}



\paragraph*{Proof:}

Let $\tilde{f}_{j}=\frac{f_{j}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}$.
Then $\tilde{f}_{j}\in\mathcal{F}_{j}$ and $I_{j}(\tilde{f}_{j})\le1$.
Let $h_{(j)}$ be the closest function to $\tilde{f}_{j}$ in the
$\delta$ cover of $\mathcal{F}_{j}$. Similarly, let $h_{(j)}^{*}$
be the closest function to $\tilde{f}_{j}^{*}$ in the $\delta$ cover
of $\mathcal{F}_{j}$. Then 
\begin{eqnarray*}
\left\Vert \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-\left(\sum_{j=1}^{J}h_{(j)}-h_{(j)}^{*}\right)\right\Vert  & \le & \sum_{j=1}^{J}\left\Vert \frac{f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-\left(h_{(j)}-h_{(j)}^{*}\right)\right\Vert \\
 & \le & \sum_{j=1}^{J}\left\Vert \frac{f_{j}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-h_{(j)}\right\Vert +\left\Vert \frac{f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-h_{(j)}^{*}\right\Vert \\
 & \le & 2J\delta
\end{eqnarray*}


Hence

\[
H\left(2J\delta,\left\{ \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}:f_{j}\in\mathcal{F}_{j},I_{j}(f_{j})+I_{j}(f_{j}^{*})>0\right\} ,\|\cdot\|_{n}\right)\le2\sum_{j=1}^{J}A_{j}\delta^{-\alpha_{j}}
\]



\subsubsection*{Example 1: Sobelov norm (NOT DONE)}

Consider the functions 
\[
\mathcal{G}=\left\{ g:[0,1]\mapsto\mathbb{R}:\int_{0}^{1}g^{(m)}(z)^{2}dz<\infty\right\} 
\]


Suppose $x_{i}$ are all unique. Then the Sobelov norm for the class
$\{\hat{g}_{\lambda}\in\mathcal{G}:\lambda\in\Lambda\}$ is bounded
above by its $L_{2}(P_{n})$ norm. 

\[
I^{2}(\hat{g}_{\lambda})=\int_{0}^{1}\left(\hat{g}_{\lambda}^{(m)}(z)\right)^{2}dz\le2\|\hat{g}_{\lambda}\|_{n}^{2}+4I^{2}(\tilde{g})+4\|y\|_{n}^{2}\mbox{ }\forall\lambda\in\Lambda
\]


PROBLEM: as defined, it is possible that $I^{2}(\tilde{g})$ grows
with $n$, which is not okay!

\textbf{Proof:}

Let $\tilde{g}$ satisfy $\tilde{g}(x_{i})=y_{i}$ and have the smallest
value for $\int_{0}^{1}\left(\tilde{g}^{(m)}(z)\right)^{2}dz$. This
function $\tilde{g}$ should always exist. 

\textbf{Case 1:} $\lambda\le1/2$

By definition of $\hat{g}_{\lambda}$

\[
\|y-\hat{g}_{\lambda}\|_{n}^{2}+\lambda^{2}I^{2}(\hat{g}_{\lambda})\le\|y-(\tilde{g}-\lambda\hat{g}_{\lambda})\|_{n}^{2}+\lambda^{2}I^{2}(\tilde{g}-\lambda\hat{g}_{\lambda})
\]


Note that 
\begin{eqnarray*}
I^{2}(\tilde{g}-\lambda\hat{g}_{\lambda}) & = & \int_{0}^{1}\left(\tilde{g}^{(m)}-\lambda\hat{g}_{\lambda}^{(m)}\right)^{2}dz\\
 & = & 2\int_{0}^{1}\max\left(\left|\tilde{g}^{(m)}\right|^{2},\left|\lambda\hat{g}_{\lambda}^{(m)}\right|^{2}\right)dz\\
 & = & 2\left(\int_{0}^{1}\left|\tilde{g}^{(m)}\right|^{2}dz+\int_{0}^{1}\left|\lambda\hat{g}_{\lambda}^{(m)}\right|^{2}dz\right)
\end{eqnarray*}


Hence

\[
\lambda^{2}I^{2}(\hat{g}_{\lambda})\le\lambda^{2}\|\hat{g}_{\lambda}\|_{n}^{2}+2\lambda^{2}I^{2}(\tilde{g})+2\lambda^{4}I^{2}(\hat{g}_{\lambda})
\]


The following ineq follows, where the RHS is maximized when $\lambda=1/2$

\[
I^{2}(\hat{g}_{\lambda})\le\frac{\lambda^{2}}{\lambda^{2}-2\lambda^{4}}\left(\|\hat{g}_{\lambda}\|_{n}^{2}+2I^{2}(\tilde{g})\right)\le2\|\hat{g}_{\lambda}\|_{n}^{2}+4I^{2}(\tilde{g})
\]


\textbf{Case 2:} $\lambda>1/2$

By definition of $\hat{g}_{\lambda}$

\[
\|y-\hat{g}_{\lambda}\|_{n}^{2}+\lambda^{2}I^{2}(\hat{g}_{\lambda})\le\|y\|_{n}^{2}
\]


The RHS is maximized when $\lambda=1/2$, so

\[
I^{2}(\hat{g}_{\lambda})\le4\|y\|_{n}^{2}
\]


Hence we have an upper bound for the Sobelov norm

\[
I^{2}(\hat{g}_{\lambda})\le2\|\hat{g}_{\lambda}\|_{n}^{2}+4I^{2}(\tilde{g})+4\|y\|_{n}^{2}
\]



\subsubsection*{Appendix}


\paragraph{A cute lemma I found but never used: }

Supposing that $I^{v}(\hat{g}_{\lambda})$ is continuous in $\lambda$,
then given training data $T$, 
\begin{eqnarray*}
\frac{\partial}{\partial\lambda}L_{T}(\hat{g}_{\lambda},\lambda) & = & 2\lambda I^{v}(\hat{g}_{\lambda})
\end{eqnarray*}


Also, $L_{T}$ is convex in $\lambda$.

\textbf{Proof:}

By definition, 
\[
L_{T}(\hat{g}_{\lambda},\lambda)=\|y-\hat{g}_{\lambda}\|_{T}^{2}+\lambda^{2}I^{v}(\hat{g}_{\lambda})\le\|y-\hat{g}_{\lambda'}\|_{T}^{2}+\lambda^{2}I^{v}(\hat{g}_{\lambda'})=L_{T}(\hat{g}_{\lambda'},\lambda)
\]


Then we can provide upper and lower bounds for $L_{T}(\hat{g}_{\lambda_{2}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{1}},\lambda_{1})$:
\begin{eqnarray*}
L_{T}(\hat{g}_{\lambda_{2}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{1}},\lambda_{1}) & \le & L_{T}(\hat{g}_{\lambda_{1}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{1}},\lambda_{1})\\
 & = & \|y-\hat{g}_{\lambda_{1}}\|_{T}^{2}+\lambda_{2}^{2}I^{v}(\hat{g}_{\lambda_{1}})-\|y-\hat{g}_{\lambda_{1}}\|_{T}^{2}-\lambda_{1}^{2}I^{v}(\hat{g}_{\lambda_{1}})\\
 & = & (\lambda_{2}^{2}-\lambda_{1}^{2})I^{v}(\hat{g}_{\lambda_{1}})
\end{eqnarray*}


\begin{eqnarray*}
L_{T}(\hat{g}_{\lambda_{2}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{1}},\lambda_{1}) & \ge & L_{T}(\hat{g}_{\lambda_{2}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{2}},\lambda_{1})\\
 & = & \|y-\hat{g}_{\lambda_{2}}\|_{T}^{2}+\lambda_{2}^{2}I^{v}(\hat{g}_{\lambda_{2}})-\|y-\hat{g}_{\lambda_{2}}\|_{T}^{2}-\lambda_{1}^{2}I^{v}(\hat{g}_{\lambda_{2}})\\
 & = & (\lambda_{2}^{2}-\lambda_{1}^{2})I^{v}(\hat{g}_{\lambda_{2}})
\end{eqnarray*}


So suppose WLOG $\lambda_{2}>\lambda_{1}$:
\[
(\lambda_{2}+\lambda_{1})I^{v}(\hat{g}_{\lambda_{2}})\le\frac{L_{T}(\hat{g}_{\lambda_{2}},\lambda_{2})-L_{T}(\hat{g}_{\lambda_{1}},\lambda_{1})}{\lambda_{2}-\lambda_{1}}\le(\lambda_{2}+\lambda_{1})I^{v}(\hat{g}_{\lambda_{1}})
\]


So as $\lambda_{1}\rightarrow\lambda_{2}=\lambda$, we have by the
sandwich theorem,

\[
\frac{\partial}{\partial\lambda}L_{T}(\hat{g}_{\lambda},\lambda)=2\lambda I^{v}(\hat{g}_{\lambda})
\]


Furthermore, given training data $T$ 
\[
\frac{\partial}{\partial\lambda}L_{T}(\hat{g}_{\lambda},\lambda)=\frac{\partial}{\partial\lambda}\|y-\hat{g}_{\lambda}\|_{T}^{2}+2\lambda I^{v}(\hat{g}_{\lambda})+\lambda^{2}\frac{\partial}{\partial\lambda}I^{v}(\hat{g}_{\lambda})
\]


then, combining this with the lemma, we have that 
\[
\frac{\partial}{\partial\lambda}\|y-\hat{g}_{\lambda}\|_{T}^{2}=-\lambda^{2}\frac{\partial}{\partial\lambda}I^{v}(\hat{g}_{\lambda})
\]


Finally, to see that $L_{T}$ is convex in $\lambda$, note that 
\[
\frac{\partial^{2}}{\partial\lambda^{2}}L_{T}(\hat{g}_{\lambda},\lambda)=2I^{v}(\hat{g}_{\lambda})+2\lambda vI^{v-1}(\hat{g}_{\lambda})\frac{\partial}{\partial\lambda}I(\hat{g}_{\lambda})>0
\]


since $\frac{\partial}{\partial\lambda}I(\hat{g}_{\lambda})>0$.


\section*{OLD}


\subsubsection*{Lemma 3:}

Suppose the function class $\mathcal{F}$ is bounded s.t. $\sup_{f\in\mathcal{F}}\|f\|_{n}\le R<\infty$.
Let 
\[
\tilde{\mathcal{F}}=\left\{ \gamma f\mbox{ }:\mbox{ }f\in\mathcal{F},\gamma\in(0,1]\right\} 
\]


\[
H\left(\delta(1+R+\delta),\tilde{\mathcal{F}},\|\cdot\|_{n}\right)\le\log(1+\lfloor\frac{1}{\delta}\rfloor)+H\left(\delta,\mathcal{F},\|\cdot\|_{n}\right)
\]



\paragraph*{Proof:}

Let $\{h_{i}\}_{i=1}^{N}$ be the $\delta$-cover for $\mathcal{F}$.
Consider any $f\in\mathcal{F}$ and let $h_{(f)}$ be the closest
function in $\delta$-cover for $\mathcal{F}$. Choose $j\in\mathbb{Z}^{+}$
such that $|\gamma-\delta j|<\delta$.

\begin{eqnarray*}
\|\gamma f-\delta jh_{(f)}\|_{n} & \le & \|\gamma f-\gamma h_{(f)}\|_{n}+\|\gamma h_{(f)}-\delta jh_{(f)}\|_{n}\\
 & \le & \gamma\|f-h_{(f)}\|_{n}+|\gamma-\delta j|\|h_{(f)}\|_{n}\\
 & \le & \gamma\delta+\delta\left(\|f-h_{(f)}\|_{n}+\|f\|_{n}\right)\\
 & \le & \gamma\delta+\delta\left(\delta+R\right)\\
 & \le & \delta\left(1+R+\delta\right)
\end{eqnarray*}


Hence we have found that the following $N(1+\lfloor\frac{1}{\delta}\rfloor)$
functions form a $\delta(1+R+\delta)$-cover for $\tilde{\mathcal{F}}$:
\[
\{h_{i}\}_{i=1}^{N}\cup\left\{ j\delta h_{i}\mbox{ }:\mbox{ }j\in1:\lfloor\frac{1}{\delta}\rfloor,i\in1:N\right\} 
\]



\subsubsection*{Lemma 4:}

Define function classes $\{\mathcal{F}_{j}\}_{j=1}^{J}$ and 
\[
\tilde{\mathcal{F}}=\left\{ \sum_{j=1}^{J}f_{j}\mbox{ }:\mbox{ }f_{j}\in\mathcal{F}_{j}\right\} 
\]


Then

\[
H\left(J\delta,\tilde{\mathcal{F}},\|\cdot\|_{n}\right)\le\sum_{j=1}^{J}H\left(\delta,\mathcal{F}_{j},\|\cdot\|_{n}\right)
\]



\paragraph*{Proof:}

For every $j=1:J$, consider any $f_{j}\in\mathcal{F}_{j}$ and let
$h_{(j)}$ be the closest function in the $\delta$-cover for $\mathcal{F}_{j}$.
\begin{eqnarray*}
\|\sum_{j=1}^{J}f_{j}-\sum_{j=1}^{J}h_{(j)}\| & \le & \sum_{j=1}^{J}\|f_{j}-h_{(j)}\|\le J\delta
\end{eqnarray*}


Hence $\exp\left(\sum_{j=1}^{J}H\left(\delta,\mathcal{F}_{j},\|\cdot\|_{n}\right)\right)$
functions form a $J\delta$-cover for $\tilde{\mathcal{F}}$.


\subsubsection*{Lemma 5:}

Suppose for all $j=1,...,J$, there is some $\alpha_{j}>0$ and $A_{j}>0$
s.t. the following entropy bound holds for all $\delta>0$

\[
H\left(\delta,\left\{ \frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le A\delta^{-\alpha_{j}}
\]


Then for sufficiently small$\delta>0$, we have

\[
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sup_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le2JA\left(\frac{\delta}{2J\left(1+R\right)}\right)^{-\alpha_{max}}
\]


where $\alpha_{max}=\max_{j\in1:J}\alpha_{j}$.


\paragraph*{Proof:}

By Lemma 3,
\[
H\left(\delta(1+R+\delta),\left\{ \gamma\frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}\mbox{ }:\mbox{ }g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0,\gamma\in(0,1]\right\} ,\|\cdot\|_{T}\right)\le\log(1+\lfloor\frac{1}{\delta}\rfloor)+A\delta^{-\alpha_{j}}
\]


Note that 
\[
\frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sup_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)}=\sum_{j=1}^{J}\left(\frac{I(g_{j})+I(g_{j}^{*})}{\sup_{\ell\in1:J}I(g_{\ell})+I(g_{\ell}^{*})}\right)\frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}
\]


By Lemma 4,

\[
H\left(J\delta(1+R+\delta),\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sup_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)}\mbox{ }:\mbox{}g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le J\log(1+\lfloor\frac{1}{\delta}\rfloor)+JA\delta^{-\alpha_{j}}
\]


Hence for sufficiently small $\delta$, 
\[
H\left(J\delta(1+R+\delta),\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sup_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)}\mbox{ }:\mbox{}g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le2JA\delta^{-\alpha_{max}}
\]


Rearranging, we get 
\begin{eqnarray*}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sup_{j\in1:J}\left(I(g_{j})+I(g_{j}^{*})\right)}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right) & \le & 2AJ\left(\sqrt{\left(\frac{1+R}{2}\right)^{2}+\frac{\delta}{J}}-\frac{1+R}{2}\right)^{-\alpha_{max}}\\
 & \le & 2AJ\left(\frac{\delta}{2J\left(1+R\right)}\right)^{-\alpha_{max}}
\end{eqnarray*}


(Used the fact that for $b>0$ small enough, $\sqrt{a^{2}+b}-a\ge\sqrt{(a+\frac{b}{4a})^{2}}-a=\frac{b}{4a}$)


\subsubsection*{Lemma 5b:}

Suppose for all $j=1,...,J$, there is some $\alpha_{j}>0$ and $A_{j}>0$
s.t. the following entropy bound holds for all $\delta>0$

\[
H\left(\delta,\left\{ \frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|\right)\le A\delta^{-\alpha_{j}}
\]


Then for sufficiently small$\delta>0$, we have

\[
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sum_{j=1}^{J}I(g_{j})+I(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|\right)\le2JA\left(STUFF\right)^{-\alpha_{max}}
\]


where $\alpha_{max}=\max_{j\in1:J}\alpha_{j}$.


\paragraph*{Proof:}

Note that

\[
\frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sum_{j=1}^{J}I(g_{j})+I(g_{j}^{*})}=\sum_{j=1}^{J}\left(\frac{I(g_{j})+I(g_{j}^{*})}{\sum_{j=1}^{J}I(g_{\ell})+I(g_{\ell}^{*})}\right)\frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}
\]


So we can express
\[
\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sum_{j=1}^{J}I(g_{j})+I(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} \subseteq\left\{ \sum_{j=1}^{J}\gamma_{j}\frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0,\sum_{j=1}^{J}\gamma_{j}=1\right\} 
\]


Let $\mathcal{H}_{j}$ be the set of functions that form a $\delta$-cover
for $\left\{ \frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} $.
Consider the set of functions 
\[
\left\{ \sum_{j=1}^{J}\delta k_{j}h_{j}\mbox{ }:\mbox{ }h_{j}\in\mathcal{H}_{j},1-\frac{1}{\delta}\le\delta\mbox{\ensuremath{\sum}}_{j=1}^{J}k_{j}\le1,k_{j}\in1:\lfloor\frac{1}{\delta}\rfloor\right\} 
\]


Let $|\delta k_{j}-\gamma_{i}|<\delta/2$. Then 
\begin{eqnarray*}
\left\Vert \sum_{j=1}^{J}\gamma_{j}\frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}-\sum_{j=1}^{J}\delta k_{j}h_{j}\right\Vert  & \le & \sum_{j=1}^{J}\left\Vert \gamma_{j}\frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}-\delta k_{j}h_{j}\right\Vert \\
 & \le & \sum_{j=1}^{J}\left\Vert \gamma_{j}\frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}-\gamma_{i}h_{j}\right\Vert +|\delta k_{j}-\gamma_{i}|\|h_{j}\|\\
 & \le & \sum_{j=1}^{J}\left(\gamma_{j}\delta+\frac{\delta}{2}\left(\left\Vert \frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}-h_{j}\right\Vert +\left\Vert \frac{g_{j}-g_{j}^{*}}{I(g_{j})+I(g_{j}^{*})}\right\Vert \right)\right)\\
 & \le & \delta(1+JR+J\delta)
\end{eqnarray*}


Hence these $\left(\Pi_{j=1}^{J}N_{j}\right){\lfloor\frac{1}{\delta}\rfloor+J-1 \choose J-1}$
functions form a $\delta(1+JR+J\delta)$ cover. Hence the entropy
is 

\[
H\left(\delta(1+JR+J\delta),\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sum_{j=1}^{J}I(g_{j})+I(g_{j}^{*})}\mbox{ }:\mbox{}g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|\right)\le(J-1)\log(1+J+\lfloor\frac{1}{\delta}\rfloor)+A\sum_{j=1}^{J}\delta^{-\alpha_{j}}
\]


Note: 
\[
{\lfloor\frac{1}{\delta}\rfloor+J-1 \choose J-1}\le\left(\lfloor\frac{1}{\delta}\rfloor+J-1\right)^{J-1}
\]


Hence for sufficiently small $\delta$, 
\[
H\left(\delta(1+JR+J\delta),\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sum_{j=1}^{J}I(g_{j})+I(g_{j}^{*})}\mbox{ }:\mbox{}g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|\right)\le2JA\delta^{-\alpha_{max}}
\]


Rearranging, we get 
\begin{eqnarray*}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sum_{j=1}^{J}I(g_{j})+I(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},I(g_{j})+I(g_{j}^{*})>0\right\} ,\|\cdot\|\right) & \le & 2AJ\left(\frac{-JR+1+\sqrt{(JR+1)^{2}+4\delta J}}{2J}\right)^{-\alpha_{max}}\\
 & \le & 2AJ\left(\frac{\sqrt{2}\delta J^{3/2}}{1+JR}\right)^{-\alpha_{max}}
\end{eqnarray*}


(Used the fact that for $b>0$ small enough, $\sqrt{a^{2}+b}-a\ge\sqrt{(a+\frac{b}{4a})^{2}}-a=\frac{b}{4a}$)


\subsubsection*{Lemma 6:}

Suppose $\epsilon_{i}$ are sub-gaussian errors and for the function
class $\mathcal{F}$, we have that for some $0<\alpha<2$, $A'>0$,
and $J>0$

\[
H\left(\delta,\mathcal{F},\|\cdot\|_{T}\right)\le A'J^{\tau}\delta^{-\alpha}\mbox{ }\forall\delta>0
\]


Then for $T=2C_{1}CA'^{1/2}J^{\tau/2}2^{1-\alpha/2}$

\[
Pr\left(\sup_{f\in\mathcal{F}}\frac{\left|\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\epsilon_{i}f(z_{i})\right|}{\|f\|_{n}^{1-\alpha/2}}\ge T\right)\le c\exp(-T^{2}/c^{2})
\]



\paragraph*{Proof:}

Follow proof for Lemma 8.4 in Vandegeer, but with $A=A'J^{-\alpha}$.
Note that we then have $A_{0}=A'^{1/2}J^{\tau/2}$. We then get 
\[
Pr\left(\sup_{f\in\mathcal{F}}\frac{\left|\frac{1}{\sqrt{n}}\sum_{i=1}^{n}\epsilon_{i}f(z_{i})\right|}{\|f\|_{n}^{1-\alpha/2}}\ge2C_{1}CA'^{1/2}J^{\tau/2}2^{1-\alpha/2}\right)\le c\exp(-T^{2}/c^{2})
\]


Note that we can write via shorthand that
\[
\sup_{f\in\mathcal{F}}\frac{\left|\frac{1}{n}\sum_{i=1}^{n}\epsilon_{i}f(z_{i})\right|}{\|f\|_{n}^{1-\alpha/2}}=O_{p}(J^{\tau/2}n^{-1/2})
\]

\end{document}
