%% LyX 2.1.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{geometry}
\geometry{verbose,tmargin=3cm,bmargin=3cm,lmargin=3cm,rmargin=3cm}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{babel}
\begin{document}

\section{Simple model}


\subsection*{Definitions}

We find the best model for $y$ over function class $\mathcal{G}$.
Presume $g^{*}\in\mathcal{G}$ is the true model and 
\[
y=g^{*}(X)+\epsilon
\]


where $\epsilon$ are sub-Gaussian errors for constants $K$ and $\sigma_{0}^{2}$
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Given a training set $T$ , We define the fitted models 
\[
\hat{g}_{\lambda}=\|y-g\|_{T}^{2}+\lambda^{2}I^{v}(g)
\]


Given a validation set $V$ , let the CV-fitted model be 
\[
\hat{g}_{\hat{\lambda}}=\arg\min_{\lambda}\|y-\hat{g}_{\lambda}\|_{V}^{2}
\]


We will suppose $I(g^{*})>0$.


\subsection*{Assumptions}

Suppose the entropy of the class $\mathcal{G}'$ is 
\begin{eqnarray}
H\left(\delta,\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} ,P_{T}\right) & \le & \tilde{A}\delta^{-\alpha}
\end{eqnarray}


Suppose $v>2\alpha/(2+\alpha)$. 

Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by $\|\hat{g}_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}\hat{g}_{\lambda}(x_{i})$.
See Lemma 1 below for the specific assumption. This assumption includes
Ridge, Lasso, Generalized Lasso, and the Group Lasso.


\subsection*{Result 1: Single $\lambda$, Single Penalty, cross-validation over
$X_{T}=X_{V}$}

Suppose $P_{T}=P_{V}=P_{n}=\{X_{i}\}_{i=1}^{n}$ are the same between
the validation and training set. 

Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by its $L_{2}$-norm with some constant $M$ and
$M_{0}$ such that

\[
I^{v}(\hat{g}_{\lambda})\le M\|\hat{g}_{\lambda}\|_{n}^{2}+M_{0}
\]


Then

\[
\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{n}=O_{p}(n^{-1/(2+\alpha)})\left(M^{\alpha/v(2+\alpha)}\|g^{*}\|_{n}^{\alpha/2v(2+\alpha)}\vee I^{2\alpha/(2+\alpha)}(g^{*})\right)
\]



\subsubsection*{Proof}

Let $\tilde{\lambda}$ be the optimal $\lambda$ under the given assumptions,
as specified by Van de geer. From the definition of $\hat{\lambda}$,
we get the following basic inequality 
\begin{eqnarray*}
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{V}^{2} & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}+2(\epsilon,\hat{g}_{\hat{\lambda}}-\hat{g}_{\tilde{\lambda}})_{V}\\
 & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}+2(\epsilon,\hat{g}_{\hat{\lambda}}-g^{*})_{V}+2(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\\
 & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}+2\left|(\epsilon,\hat{g}_{\hat{\lambda}}-g^{*})_{V}\right|+2\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|
\end{eqnarray*}


By considering the largest term on the RHS, we have following three
cases.

\textbf{Case 1:} $\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}$ is
the largest

Since we have assumed that the validation and training set are equal,
then $\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}$ converges at the optimal
rate $O_{p}(n^{-1/(2+\alpha)})$.

\textbf{Case 2:} $\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|$
is the largest

In this case, since $\epsilon_{V}$ is independent of $\hat{g}_{\tilde{\lambda}}$,
then by Cauchy Schwarz, 
\begin{eqnarray*}
\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right| & \le & \|\epsilon_{V}\|\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}\\
 & \le & O_{p}\left(n^{-1/2}\right)\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}
\end{eqnarray*}


Hence $\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|$
will shrink a bit faster than the optimal rate at a rate of $O_{p}(n^{-(\frac{1}{2+\alpha}+\frac{1}{2})})$.

\textbf{Case 3:} $\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|$
is the largest.

By the assumptions given, Vandegeer (10.6) gives us that 
\[
\sup_{g\in\mathcal{G}}\frac{|(\epsilon,g-g*)_{n}|}{\|g-g*\|_{n}^{1-\alpha/2}(I(g^{*})+I(g))^{\alpha/2}}=O_{p}(n^{-1/2})
\]


Hence 
\[
\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|\le O_{p}(n^{-1/2})\|\hat{g}_{\hat{\lambda}}-g*\|_{n}^{1-\alpha/2}\left(I(g^{*})+I(\hat{g}_{\hat{\lambda}})\right)^{\alpha/2}
\]


If $I(g^{*})\ge I(g_{\hat{\lambda}})$ , then

\[
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{V}\le O_{p}(n^{-1/(2+\alpha)})I(g^{*})^{\alpha/(2+\alpha)}
\]


Otherwise, we have

\[
\|\hat{g}_{\hat{\lambda}}-g*\|_{n}^{1+\alpha/2}\le O_{p}(n^{-1/2})I(\hat{g}_{\hat{\lambda}}){}^{\alpha/2}
\]


By Lemma 1 below, using the assumption that the penalty of $\hat{g}_{\lambda}$
is bounded above by its $L_{2}(P_{n})$ norm, we have that

\[
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha v/(2+\alpha)}\|g^{*}\|_{n}^{2\alpha/v(2+\alpha)}
\]


(Note: Here we've assumed function $I$ is from the optimization criterion,
but that is not necessary!)


\subsection*{Result 2: Single $\lambda$, Single Penalty, cross-validation over
general $X_{T},X_{V}$}

Suppose that the training and validation set are independently sampled,
so $X_{i}$ are not necessarily the same. Suppose $X$ is bounded
s.t. $|X|\le R_{X}$ and the domain of $g\in\mathcal{G}$ is over
$(-R_{X},R_{X})$.

We suppose the training and validation sets are both of size $n$.

Suppose the entropy bound (1) for both training set $P_{T}$ and validation
set $P_{V}$.

Suppose that 
\begin{eqnarray*}
\sup_{g\in\mathcal{G}}\frac{\|g-g^{*}\|_{\infty}}{I(g)+I(g^{*})} & \le & K<\infty
\end{eqnarray*}


Suppose for all $\lambda\in\Lambda$, $I^{v}(\hat{g}_{\lambda})$
is upper bounded by its $L_{2}$-norm with constants $M$ and $M_{0}$:

\[
I^{v}(\hat{g}_{\lambda})\le M\|\hat{g}_{\lambda}\|_{V}^{2}+M_{0}
\]


Then

\[
\|\hat{g}_{\hat{\lambda}}-g^{*}\|_{V}=O_{p}(n^{-1/(2+\alpha)})\left(I(g^{*})\vee I^{(4a-4v+a^{2}v)/2a(2+a)}(g^{*})\vee M^{\alpha v/(2+\alpha)}\|g^{*}\|_{V}^{2\alpha/v(2+\alpha)}\right)
\]



\paragraph{Proof:}

We follow the same proof structure of going thru the three cases,
modifying the proofs as appropriate:

\textbf{Case 1:} $\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}^{2}$ is
the largest

By Lemma 2, we have
\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Hence for any $\xi>0$,
\[
\frac{\left|\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{T}-\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V}\right|}{I(g^{*})+I(\hat{g}_{\tilde{\lambda}})}\le O_{p}(n^{-1/(2+\alpha)})
\]


Therefore
\begin{eqnarray*}
\|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{V} & \le & \|g^{*}-\hat{g}_{\tilde{\lambda}}\|_{T}+O_{p}(n^{-1/(2+\alpha)})\left(I(g^{*})+I(\hat{g}_{\tilde{\lambda}})\right)\\
 & \le & O_{p}(\tilde{\lambda})I^{v/2}(g^{*})+O_{p}(n^{-1/(2+\alpha)})I(g^{*})
\end{eqnarray*}


where 
\[
\tilde{\lambda}^{-1}=O_{p}(n^{-1/(2+\alpha)})I^{(2v-2\alpha+\alpha v)/2(2+\alpha)}(g^{*})
\]


\textbf{Case 2:} $\left|(\epsilon,g^{*}-\hat{g}_{\tilde{\lambda}})_{V}\right|$
is the largest

Same proof still holds.

\textbf{Case 3:} $\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|$
is the largest.

Again, by Vandegeer (10.6),
\[
\left|(\epsilon,g^{*}-\hat{g}_{\hat{\lambda}})_{V}\right|\le O_{p}(n^{-1/2})\|\hat{g}_{\hat{\lambda}}-g*\|_{V}^{1-\alpha/2}(I(g^{*})+I(\hat{g}_{\hat{\lambda}}))^{\alpha/2}
\]


\textbf{Case 3a:} $I(g^{*})\ge I(g_{\hat{\lambda}})$ 

The result is clearly attained.

\textbf{Case 3b: }$I(g^{*})\le I(g_{\hat{\lambda}})$

By Lemma 1 below, since the penalty is bounded above by the $L_{2}(P_{V})$
norm, it follows that

\[
\|g^{*}-\hat{g}_{\hat{\lambda}}\|_{V}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha v/(2+\alpha)}\|g^{*}\|_{V}^{2\alpha/v(2+\alpha)}
\]


\pagebreak{}


\section{General Additive Model}


\subsection*{Definitions}

We find the best model for $y$ over function classes $\mathcal{G}_{j}$.
Suppose we observe:

\[
y=\sum_{j=1}^{J}g_{j}^{*}+\epsilon
\]


where $g_{j}^{*}\in\mathcal{G}_{j}$ are the true functions. $\epsilon$
are sub-Gaussian errors for constants $K$ and $\sigma_{0}^{2}$
\[
\max_{i=1:n}K^{2}\left(E\left[\exp(|\epsilon_{i}|^{2}K^{2})-1\right]\right)\le\sigma_{0}^{2}
\]


Given a training set $T$ , we fit models by least squares with multiple
penalties 
\[
\{\hat{g}_{\lambda,j}\}_{j=1}^{J}=\arg\min_{g_{j}\in\mathcal{G}_{j}}\|y-\sum_{j=1}^{J}g_{j}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}(g_{j})
\]


Given a validation set $V$ , let the CV-fitted model be 
\[
\{\hat{g}_{\hat{\lambda},j}\}_{j=1}^{J}=\arg\min_{\lambda}\|y-\sum_{j=1}^{J}\hat{g}_{\lambda,j}\|_{V}^{2}
\]


\textbf{Reasonable assumption:}
\begin{itemize}
\item The entropy bound (2) in result 3 comes from the assumptions in Lemma
3. The $\alpha$ below is $\alpha=\max_{j=1:J}\{\alpha_{j}\}$, so
convergence is only as fast as fitting the highest-entropy function
class. The constant $A$ must be appropriately inflated such that
the entropy bound holds for all $\delta\in(0,R]$.
\end{itemize}
\textbf{Special assumptions:}
\begin{itemize}
\item I assume exponents $v_{k}$ in the optimization criteria are greater
than one, whereas Vandegeer Thrm 10.2 only assumes $v>2\alpha/(2+\alpha)$.
Without this assumption, I wasn't able to form inequalities between
$\sum_{j=1}^{J}I_{j}(g_{j})\le O_{p}(1)+\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j})$.
To remove this assumption, we need something else in the denominator
of the entropy bound. (Currently, I use $\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})$).
\item In Result 1 and 2, I bounded $\left|(\epsilon_{V},g^{*}-\hat{g}_{\hat{\lambda}})\right|$
by assuming the penalty function $I^{v}(g)$ was upper bounded by
$\|g\|_{n}^{2}$. However, that isn't enough for the case of additive
penalties. I've assumed that there is some function $\tilde{I}:\left\{ \sum_{j=1}^{J}g_{j}:g_{j}\in\mathcal{G}_{j}\right\} \mapsto\mathbb{R}$
such $I^{v}(\sum_{j=1}^{J}g_{j})$ is upper bounded by $\|\sum_{j=1}^{J}g_{j}\|_{n}^{2}$
AND it gives the same entropy bound.
\end{itemize}

\subsection*{Result 3: Additive Model with multiple penalties, Single oracle $\lambda$
over $X_{T}$}

Suppose there is some $0<\alpha<2$ s.t. for all $\delta\in(0,R]$,
\begin{equation}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})}:g_{j}\in\mathcal{G}_{j},\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})>0\right\} ,\|\cdot\|_{T}\right)\le A\delta^{-\alpha}
\end{equation}


\textbf{Special assumption: }Suppose $v_{k}\ge1$ for all $k$. 

If $\lambda$ is chosen s.t. 
\[
\tilde{\lambda}_{T}^{-1}=O_{p}\left(n^{1/(2+\alpha)}\right)\left(J+\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{(2-\alpha)/2(2+\alpha)}
\]


then
\[
\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|_{T}=O_{p}\left(\tilde{\lambda}_{T}\right)\left(\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})\right)^{1/2}
\]


and 
\[
\sum_{k=1}^{K}I_{k}(\{\hat{g}_{j}\})\le K+\sum_{j=1}^{J}I_{j}(g_{j}^{*})
\]



\subsubsection*{Proof:}

The basic inequality gives us:

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}^{v_{j}}(\hat{g}_{j})\le2\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|+\lambda^{2}\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})
\]


\textbf{Case 1: $\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\le\lambda^{2}\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})$}

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}(\lambda)\left(\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})\right)^{1/2}
\]


\textbf{Case 2: $\left|\left(\epsilon_{T},\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\right)\right|\ge\lambda^{2}\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})$}

By Vandegeer (10.6), the basic inequality becomes 
\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{2}+\lambda^{2}\sum_{j=1}^{J}I_{j}^{v_{j}}(\hat{g}_{j})\le O_{p}\left(n^{-1/2}\right)\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha/2}\left(\sum_{j=1}^{J}I_{j}(\hat{g}_{j})+I_{j}(g_{j}^{*})\right)^{\alpha/2}
\]


\textbf{Case 2a:} $\sum_{j=1}^{J}I_{j}(g_{j})\le\sum_{j=1}^{J}I_{j}(g_{j}^{*})$ 

Then 
\begin{eqnarray*}
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T} & \le & O_{p}\left(n^{-1/(2+\alpha)}\right)\left(\sum_{j=1}^{J}I_{j}(g_{j}^{*})\right)^{\alpha/(2+\alpha)}
\end{eqnarray*}


\textbf{Case 2b:} $\sum_{j=1}^{J}I_{j}(g_{j})\ge\sum_{j=1}^{J}I_{j}(g_{j}^{*})$ 

First note that for exponent $v\ge1$, we must have $x\le x^{v}+1$.
So by assuming $v_{j}\ge1$,

\begin{eqnarray*}
\sum_{j=1}^{J}I_{j}(\hat{g}_{j}) & \le & J+\sum_{j=1}^{J}I_{j}^{v_{j}}(\hat{g}_{j})\\
 & \le & J+O_{p}\left(n^{-1/2}\right)\lambda^{-2}\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}^{1-\alpha/2}\left(\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\right)^{\alpha/2}
\end{eqnarray*}


\textbf{Case 2b part a:} 2nd term on the RHS in the inequality above
is bigger

Then

\[
\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le O_{p}\left(n^{-1/(2-\alpha)}\right)\lambda^{-4/(2-\alpha)}\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}
\]


which implies

\[
\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}\left(n^{-1/(2-\alpha)}\right)\lambda^{-2\alpha/(2-\alpha)}
\]


and 
\[
\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le J+\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})
\]


\textbf{Case 2b part b:} 1st term on the RHS in the inequality above
is bigger

Then
\[
\sum_{j=1}^{J}I_{j}(\hat{g}_{j})\le2J\implies\|\sum_{j=1}^{J}\hat{g}_{j}-g_{j}^{*}\|_{T}\le O_{p}\left(n^{-1/(2+\alpha)}\right)
\]



\subsection*{Result 4: Additive Model with multiple penalties, Single cross-validation
$\lambda$ over general $X_{T},X_{V}$}

Suppose that the training and validation set are independently sampled,
so the values $X_{i}$ are not necessarily the same. Suppose the training
and validation sets are both of size $n$. Suppose $X$ is bounded
s.t. $|X|\le R_{X}$ and the domain of $g\in\mathcal{G}$ is over
$(-R_{X},R_{X})$.

In addition to the assumptions in Result 3, assume the following:

Suppose the same entropy bound (2) for both the training set $P_{T}$
and validation set $P_{V}$.

Suppose the infinity norm is also bounded 
\begin{eqnarray*}
\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\|\sum_{j=1}^{J}g_{j}-g_{j}^{*}\|_{\infty}}{\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})} & \le & K<\infty
\end{eqnarray*}


\textbf{Special Assumption}: Suppose there is some function $\tilde{I}:\left\{ \sum_{j=1}^{J}g_{j}:g_{j}\in\mathcal{G}_{j}\right\} \mapsto\mathbb{R}$
such 

\[
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}g_{j}-g_{j}^{*}}{\tilde{I}\left(\sum_{j=1}^{J}g_{j}\right)+\tilde{I}\left(\sum_{j=1}^{J}g_{j}^{*}\right)}:g_{j}\in\mathcal{G}_{j},\tilde{I}\left(\sum_{j=1}^{J}g_{j}\right)+\tilde{I}\left(\sum_{j=1}^{J}g_{j}^{*}\right)>0\right\} ,\|\cdot\|_{V}\right)\le\tilde{A}\delta^{-\alpha}
\]


Furthermore, suppose there exist constants $M$,$M_{0}$, and $w>2\alpha/(2+\alpha)$
s.t. for all $\lambda\in\Lambda$ 

\[
\tilde{I}^{w}\left(\sum_{j=1}^{J}\hat{g}_{\lambda,j}\right)\le M\|\sum_{j=1}^{J}\hat{g}_{\lambda,j}\|_{V}^{2}+M_{0}
\]


Then

\[
\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}-g_{j}^{*}\|_{V}=O_{p}(n^{-1/(2+\alpha)})\left[\left(K+\sum_{k=1}^{K}I_{k}^{v_{k}}(\{g_{j}^{*}\})\right)\vee\left(M^{\alpha w/(2+\alpha)}\|\sum_{j=1}^{J}g_{j}^{*}\|_{V}^{2\alpha/w(2+\alpha)}\right)\right]
\]



\subsubsection*{Proof:}

The proof is very similar to Result 2.

\textbf{Case 1:} $\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\|_{V}^{2}$
is the largest

By Lemma 2, we have
\[
Pr\left(\sup_{g_{j}\in\mathcal{G}_{j}}\frac{\left|\|\sum_{j=1}^{J}g_{j}^{*}-g_{j}\|_{P_{n}}-\|\sum_{j=1}^{J}g_{j}^{*}-g_{j}\|_{P_{n''}}\right|}{\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})}\ge6\delta\right)\le2\exp\left(2A\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Hence
\[
\frac{\left|\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\|_{T}-\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\|_{V}\right|}{\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})}\le O_{p}(n^{-1/(2+\alpha)})
\]


Therefore
\begin{eqnarray*}
\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\|_{V} & \le & \|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\|_{T}+O_{p}(n^{-1/(2+\alpha)})\left(\sum_{j=1}^{J}I_{j}(g_{j})+I_{j}(g_{j}^{*})\right)\\
 & \le & O_{p}\left(\tilde{\lambda}_{T}\right)\left(\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})\right)^{1/2}+O_{p}(n^{-1/(2+\alpha)})\left(J+\sum_{j=1}^{J}I_{j}^{v_{j}}(g_{j}^{*})\right)
\end{eqnarray*}


\textbf{Case 2:} $\left|\left(\epsilon_{V},\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\tilde{\lambda},j}\right)\right|$
is the largest

Since $\epsilon_{V}$ is independent of $\left\{ \hat{g}_{\tilde{\lambda},j}\right\} $,
then this term shrinks at the rate of $O_{p}(n^{-1/2-1/(2+\alpha)})$.
(So the rate is faster than the optimal rate.)

\textbf{Case 3:} $\left|\left(\epsilon_{V},\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right)\right|$
is the largest.

By our \textbf{special assumption}, we can again apply Vandegeer (10.6),
\[
\left|\left(\epsilon_{V},\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\right)\right|\le O_{p}(n^{-1/2})\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\|_{V}^{1-\alpha/2}\left(\tilde{I}\left(\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}\right)+\tilde{I}\left(\sum_{j=1}^{J}g_{j}^{*}\right)\right)^{\alpha/2}
\]


\textbf{Case 3a:}$\tilde{I}(\{\hat{g}_{\hat{\lambda},j}\})\le\tilde{I}(\{g_{j}^{*}\})$ 

The result is clearly attained.

\textbf{Case 3b:}$\tilde{I}(\{\hat{g}_{\hat{\lambda},j}\})>\tilde{I}(\{g_{j}^{*}\})$ 

By the assumption that $\tilde{I}^{w}(\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j})$
is bounded above by $\|\sum_{j=1}^{J}\hat{g}_{\hat{\lambda},j}\|_{V}^{2}$,
Lemma 1 gives us
\begin{eqnarray*}
\|\sum_{j=1}^{J}g_{j}^{*}-\hat{g}_{\hat{\lambda},j}\|_{V} & \le & O_{p}(n^{-1/(2+\alpha)})M^{\alpha w/(2+\alpha)}\|\sum_{j=1}^{J}g_{j}^{*}\|_{V}^{2\alpha/w(2+\alpha)}
\end{eqnarray*}


\pagebreak{}


\subsection*{Lemmas}


\subsubsection*{Lemma 1: }

Suppose for all $\lambda\in\Lambda$, the penalty function $I^{v}(g_{\lambda})$
is upper-bounded by $\|g_{\lambda}\|_{n}^{2}=\frac{1}{n}\sum_{i=1}^{n}g_{\lambda}^{2}(x_{i})$
with constants $M_{0}$ and $M$:

\[
I^{v}(g_{\lambda})\le M\|g_{\lambda}\|_{n}^{2}+M_{0}
\]


Suppose there is some function $g^{*}\in\mathcal{G}$ such that
\[
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2}\le O_{p}(n^{-1/2})I^{\alpha/2}(g_{\lambda})
\]


Then

\[
\|g^{*}-g_{\lambda}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha v/(2+\alpha)}\|g^{*}\|_{n}^{2\alpha/v(2+\alpha)}
\]


\textbf{Proof:}

From the assumptions, we have

\begin{eqnarray*}
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})\left(M\|g_{\lambda}\|_{n}^{2}+M_{0}\right)^{\alpha/2v}
\end{eqnarray*}


If $M_{0}>\|g_{\lambda}\|_{n}^{2}$, we're done. Otherwise,

\begin{eqnarray*}
\|g^{*}-g_{\lambda}\|_{n}^{1+\alpha/2} & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\|g_{\lambda}\|_{n}^{\alpha/v}\\
 & \le & O_{p}(n^{-1/2})M^{\alpha/2v}\left(\|g_{\lambda}-g^{*}\|_{n}+\|g^{*}\|_{n}\right)^{\alpha/v}
\end{eqnarray*}


\textbf{Case 1:} $\|g_{\lambda}-g^{*}\|_{n}\ge\|g^{*}\|_{n}$

Then

\[
\|g^{*}-g_{\lambda}\|_{n}\le O_{p}(n^{-v/(2v+\alpha v-2\alpha)})M^{\alpha v^{2}/(2v+\alpha v-2\alpha)}
\]


Note that $\sup_{v}-\frac{v}{2v+\alpha v-2\alpha}=-\frac{1}{2+\alpha}$,
so this rate is faster than $O_{p}(n^{-\frac{1}{2+\alpha}})$.

\textbf{Case 2:} $\|g_{\lambda}-g^{*}\|_{n}\le\|g^{*}\|_{n}$

Then

\[
\|g^{*}-g_{\lambda}\|_{n}\le O_{p}(n^{-1/(2+\alpha)})M^{\alpha v/(2+\alpha)}\|g^{*}\|_{n}^{2\alpha/v(2+\alpha)}
\]


I believe we can often provide a good estimate of $M$ for the entire
class $\mathcal{G}$, which means that we can always estimate the
sample size needed to ensure this case never occurs. That is, I believe
we can often estimate $M$ s.t. 
\[
I^{v}(g)\le M\|g\|_{n}^{2}+M_{0}\forall g\in\mathcal{G}
\]



\subsubsection*{Lemma 2:}

Let $P_{n'}$ and $P_{n''}$ be empirical distributions over $\{X_{i}'\}_{i=1}^{n},\{X_{i}''\}_{i=1}^{n}$.
Let $P_{2n}=\frac{1}{2}\left(P_{n'}+P_{n''}\right)$. Suppose $X$
is bounded s.t. $|X|<R_{X}$.

Let $\mathcal{G}'=\left\{ \frac{g-g^{*}}{I(g)+I(g^{*})}:g\in\mathcal{G},I(g)+I(g^{*})>0\right\} $.
Suppose $g$ is defined over the domain over $X$ (and zero otherwise).
Suppose 
\[
\sup_{f\in\mathcal{G}'}\|f\|_{P_{2n}}\le R<\infty,\mbox{ }\sup_{f\in\mathcal{G}'}\|f\|_{\infty}\le K<\infty
\]


and 
\[
H\left(\delta,\mathcal{G}',P_{n'}\right)\le\tilde{A}\delta^{-\alpha},\mbox{ }H\left(\delta,\mathcal{G}',P_{n''}\right)\le\tilde{A}\delta^{-\alpha}
\]


Then 

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n'}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]



\paragraph{Proof:}

The proof is very similar to that in Pollard 1984 (page 32), so some
details below are omitted.

First note that for any function $f$ and $h$, we have 
\[
\|f\|_{P_{n'}}-\|h\|_{P_{n'}}\le\|f-h\|_{P_{n'}}\le\sqrt{2}\|f-h\|_{P_{2n}}
\]


Similarly for $P_{n''}$. 

Let $\{h_{j}\}_{j=1}^{N}$ be the $\sqrt{2}\delta$-cover for $\mathcal{G}'$
(where $N=N(\sqrt{2}\delta,\mathcal{G}',P_{2n})$). Let $h_{j}$ be
the closest function (in terms of $\|\cdot\|_{P_{2n}}$) to some $f\in\mathcal{G}'$.
Then 
\begin{eqnarray*}
\|f\|_{P_{n'}}-\|f\|_{P_{n''}} & \le & \|f-h_{j}\|_{P_{n'}}+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|+\|f-h_{j}\|_{P_{n''}}\\
 & \le & 4\delta+\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|
\end{eqnarray*}


Therefore for $f=\frac{g^{*}-g}{I(g^{*})+I(g)}$, we have
\begin{eqnarray*}
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right) & \le & Pr\left(\sup_{j\in1:N}\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\\
 & \le & N\max_{j\in1:N}Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)
\end{eqnarray*}


Now note that 
\begin{eqnarray*}
\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right| & = & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\|h_{j}\|_{P_{n'}}+\|h_{j}\|_{P_{n''}}}\\
 & \le & \frac{\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|}{\sqrt{2}\|h_{j}\|_{P_{2n}}}
\end{eqnarray*}


By Hoeffding's inequality, 
\begin{eqnarray*}
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right) & \le & Pr\left(\left|\|h_{j}\|_{P_{n'}}^{2}-\|h_{j}\|_{P_{n''}}^{2}\right|\ge2\sqrt{2}\delta\|h_{j}\|_{P_{2n}}\right)\\
 & = & Pr\left(\left|\sum_{i=1}^{n}W_{i}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)\right|\ge2\sqrt{2}n\delta\|h_{j}\|_{P_{2n}}\right)\\
 & \le & 2\exp\left(-\frac{16\delta^{2}n^{2}\|h_{j}\|_{P_{2n}}^{2}}{4\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2}}\right)
\end{eqnarray*}


Since $\|h_{j}\|_{\infty}<K$, then

\begin{eqnarray*}
\sum_{i=1}^{n}\left(h_{j}^{2}(x_{i}')-h_{j}^{2}(x_{i}'')\right)^{2} & \le & \sum_{i=1}^{n}h_{j}^{4}(x_{i}')+h_{j}^{4}(x_{i}'')\\
 & \le & nK^{2}\|h_{j}\|_{P_{2n}}^{2}
\end{eqnarray*}


Hence 
\[
Pr\left(\left|\|h_{j}\|_{P_{n'}}-\|h_{j}\|_{P_{n''}}\right|\ge2\delta\right)\le2\exp\left(-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Since (Pollard and Vandegeer say that) 
\[
N(\sqrt{2}\delta,\mathcal{G}',P_{2n})\le N(\delta,\mathcal{G}',P_{n''})+N(\delta,\mathcal{G}',P_{n''})
\]


then

\[
Pr\left(\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}\ge6\delta\right)\le2\exp\left(2\tilde{A}\delta^{-\alpha}-\frac{4\delta^{2}n}{K^{2}}\right)
\]


Using shorthand, we can write 
\[
\sup_{g\in\mathcal{G}}\frac{\left|\|g^{*}-g\|_{P_{n}}-\|g^{*}-g\|_{P_{n''}}\right|}{I(g^{*})+I(g)}=O_{p}(n^{-1/(2+\alpha)})
\]



\subsubsection*{Lemma 3:}

Suppose the function classes $\mathcal{F}_{j}$ is a cone and $I_{j}:\mathcal{F}_{j}\mapsto[0,\infty)$
is a psuedonorm. Furthermore, suppose 
\[
H\left(\delta,\{f_{j}\in\mathcal{F}_{j}:I_{j}(f_{j})\le1\},\|\cdot\|_{n}\right)\le A_{j}\delta^{-\alpha_{j}}
\]
Then if $f_{j}^{*}\in\mathcal{F}_{j}$, then

\begin{eqnarray*}
H\left(\delta,\left\{ \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}:f_{j}\in\mathcal{F}_{j},I_{j}(f_{j})+I_{j}(f_{j}^{*})>0\right\} ,\|\cdot\|_{n}\right) & \le & 2\sum_{j=1}^{J}A_{j}\left(\frac{\delta}{2J}\right)^{-\alpha_{j}}
\end{eqnarray*}



\paragraph*{Proof:}

Let $\tilde{f}_{j}=\frac{f_{j}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}$.
Then $\tilde{f}_{j}\in\mathcal{F}_{j}$ and $I_{j}(\tilde{f}_{j})\le1$.
Let $h_{(j)}$ be the closest function to $\tilde{f}_{j}$ in the
$\delta$ cover of $\mathcal{F}_{j}$. Similarly, let $h_{(j)}^{*}$
be the closest function to $\tilde{f}_{j}^{*}$ in the $\delta$ cover
of $\mathcal{F}_{j}$. Then 
\begin{eqnarray*}
\left\Vert \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-\left(\sum_{j=1}^{J}h_{(j)}-h_{(j)}^{*}\right)\right\Vert  & \le & \sum_{j=1}^{J}\left\Vert \frac{f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-\left(h_{(j)}-h_{(j)}^{*}\right)\right\Vert \\
 & \le & \sum_{j=1}^{J}\left\Vert \frac{f_{j}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-h_{(j)}\right\Vert +\left\Vert \frac{f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}-h_{(j)}^{*}\right\Vert \\
 & \le & 2J\delta
\end{eqnarray*}


Hence

\[
H\left(2J\delta,\left\{ \frac{\sum_{j=1}^{J}f_{j}-f_{j}^{*}}{\sum_{j=1}^{J}I_{j}(f_{j})+I_{j}(f_{j}^{*})}:f_{j}\in\mathcal{F}_{j},I_{j}(f_{j})+I_{j}(f_{j}^{*})>0\right\} ,\|\cdot\|_{n}\right)\le2\sum_{j=1}^{J}A_{j}\delta^{-\alpha_{j}}
\]

\end{document}
